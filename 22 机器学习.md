#### 总结

1. GBDT(Gradient Boosting Decision Tree), 梯度提升树
2. 决策树是二叉树吗
   1. 不是. 二叉树: 一个节点有有个子分支
   2. 决策树: 一个节点可以有多个分支.



黑马 机器学习课件 html 版本

## 机器学习引言

### 1 sklearn与特征工程

1. Scikit-learn与特征工程
   1. 数据决定 上限， 算法逼近上限
   
   2. Scikit-learn
      1. Python 机器学习工具. 所有人都适用，不同的上下文中重用
         1. 基于NumPy、SciPy和matplotlib构建
         2. 目前稳定版本0.18
      2. 支持机器学习算法: 分类，回归，降维, 聚类
      3. Scikit-learn模块
         1. 特征提取 模块（extracting features）
         2. 数据处理 模块（processing data）
         3. 模型评估 模块（evaluating models）
      
   3. Scikit-learn安装步骤
      1. 创建 基于 Python3 虚拟环境 `ml3`
         
         `mkvirtualenv -p /usr/local/bin/python3.6 ml3`
         
      2. ubuntu 虚拟环境 命令
         
         `pip3 install Scikit-learn`
         
      3. 导入命令 使用
         
         `import sklearn`
   
2. 特征工程
   1. 原始特征数据集 可能太大，或 信息冗余
   2. 方法 : 选择特征的子集，或构建 新的特征集
      1. 减少功能 促进算法的学习
      2. 提高 泛化能力和可解释性
   3. 更好的特征意味着: 更强的鲁棒性, 简单模型, 更好的结果
   
3. 特征工程 之 特征处理
   1. 特征预处理 : 专业技巧
      1. 对一个特征
         1. 归一化
         2. 标准化
         3. 缺失值
      2. 对多个特征
         1. 降维: 如 PCA

4. 特征工程的步骤: 
   1. 特征抽取 : 文本和图像格式 转换为机器学习的 `数字格式`(特征的数字化)
   2. 特征处理 : 处理已有的数据 达到 数据标准
   3. 特征选择 : 在已有的特征中选择 更好的特征

#### 1.1 数据的来源与类型

1. 来源: 1.已有的数据库; 2. 爬虫工程师 爬取
2. 数据的类型
   1. 机器学习数据分类
      1. 标称型  : 标称型变量用于 分类
      2. 数值型 :  数值型变量用于回归分析
   2. 数据的 分布特性 分类
      1. 离散型 : 自然数或整数单位
      2. 连续型 : 区间内可以是任意一个数值

#### 1.2 数据的特征抽取

1. 特征抽取: 特征的数字化, (如 分类、文字、图像 的数字化表述)

   2. `sklearn.feature_extraction`   #有很多特征抽取方法
   
2. 分类特征 提取

   1. `sklearn.feature_extraction.DictVectorizer(sparse = True)` 转换器

      1. 特征与类别的映射 `字典` 组成的 `列表` 转换成 `向量` ,  即 映射列表转为向量. 或 映射列表转换为 numpy 数组或 scipy.sparse 矩阵
      2. 参数 sparse 是否转换为 scipy.sparse 矩阵表示，默认开启

   2. 转换器的方法

      1. `fit_transform(X [,y])`   #映射列表X转化为向量,并应用之. y为目标类型

      2. `inverse_transform(X[, dict_type])`   #Numpy数组或scipy.sparse矩阵转换为 映射列表

   3. ```
      from sklearn.feature_extraction import DictVectorizer
      onehot = DictVectorizer() #创建转换器, 如果结果不toarray，请开启sparse=False
      instances = [{'city': '北京','temperature':100},{'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
      X = onehot.fit_transform(instances).toarray()
      onehot.inverse_transform(X) #与fit_transform相反
      ```

3. 文本特征提取（只限于英文）

   1. 应用: 文档分类、垃圾邮件分类和新闻分类

   2. 文本特征抽取类:  特征数值计算类、 词的概率（重要性） 表示

   3. CountVectorize , 特征数值计算类，是一个文本特征提取方法, 只考虑每种词汇在该训练文本中出现的频率.  将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数。

      1. ```
         CountVectorizer(input='content', encoding='utf-8',  decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, 
         token_pattern='(?u)\b\w\w+\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)
         ```

         CountVectorizer类的参数很多，分为三个处理步骤：preprocessing、tokenizing、n-grams generation.

         

      2.  `sklearn.feature_extraction.text.CountVectorizer()`    #转换器

         1. 文本文档的集合转换为 计数矩阵（scipy.sparse matrices）
         2. 数值 `1` 存在，`0` 不存在
         3. 转换器的 方法 `fit_transform(raw_documents,y)`

      3. 词汇词典, 返回 词汇文档矩阵

         1. ```
            from sklearn.feature_extraction.text import CountVectorizer
            content = ["life is short,i like python","life is too long,i dislike python"]
            vectorizer = CountVectorizer()
            
            #特征抽取
            vectorizer.fit_transform(content).toarray()
            ```
            
            ```
            x2 = vectorizer.fit_transform(content).toarray()
            #x2输出:
            array([[0, 1, 1, 1, 0, 1, 1, 0],
                   [1, 1, 1, 0, 1, 1, 0, 1]], dtype=int64)
            ```
            
            
            
         2. `toarray()` 方法转变为numpy的数组

         3. 文档矩阵是稀疏矩阵, 为了解决存储和运算速度, 用 Python的scipy.sparse矩阵结构

      1. TF-IDF : 词的概率 , 词的重要性

         1. 转换器 `sklearn.feature_extraction.text.TfidfVectorizer()`

            1. 根据指定的 公式 将文档中的 词转换为 概率表示

         2. 转换器的 方法 `fit_transform(raw_documents,y)`

            1. 词汇idf，返回 术语文档矩阵

               1. ```
                  from sklearn.feature_extraction.text import TfidfVectorizer
                  content = ["life is short,i like python","life is too long,i dislike python"]
                  vectorizer = TfidfVectorizer(stop_words='english')
                  vectorizer.fit_transform(content).toarray()
                  vectorizer.vocabulary_ #词汇映射字典
                  ```

4. 图像特征提取

   1. 无内容

#### 1.3 数据的特征预处理

 1. 单个特征

     1. 归一化	

         1. 目的: 

             1. 防止某一维或某几维对数据影响过大
             2. 数据统一到一个参考区间, 
             3. 程序 运行更快

         2. min-max公式

             1. $X' = \frac{x-min}{max-min}$
              	1. 通常 原始数据 线性变换 映射到[0,1]

           	3. 处理方法 

                 1. min-max 自定义处理
                 1. 示例: 相亲约会对象数据在MatchData.txt
                
                 1. ```
                           import numpy as np
                           
                           def data_matrix(file_name):
                             """
                             将文本转化为matrix
                             :param file_name: 文件名
                             :return: 数据矩阵
                             """
                             fr = open(file_name)
                             array_lines = fr.readlines()
                             number_lines = len(array_lines)
                             return_mat = zeros((number_lines, 3)) #一行一个人
                             # classLabelVector = []
                             index = 0
                             for line in array_lines:
                               line = line.strip()
                               list_line = line.split('\t')
                               return_mat[index,:] = list_line[0:3]
                               # if(listFromLine[-1].isdigit()):
                               #     classLabelVector.append(int(listFromLine[-1]))
                               # else:
                               #     classLabelVector.append(love_dictionary.get(listFromLine[-1]))
                               # index += 1
                             return return_mat
                        ```
                
                 2. ```
                           #min-max自定义处理
                           def feature_normal(data_set):
                               """
                               特征归一化
                               :param data_set:
                               :return:
                               """
                               # 每列最小值
                               min_vals = data_set.min(0)
                               # 每列最大值
                               max_vals = data_set.max(0)
                               ranges = max_vals - min_vals
                               norm_data = np.zeros(np.shape(data_set))
                               # 得出行数
                               m = data_set.shape[0]
                               # 矩阵相减
                               norm_data = data_set - np.tile(min_vals, (m,1))
                               # 矩阵相除
                               norm_data = norm_data/np.tile(ranges, (m, 1)))
                               return norm_data
                        ```
                
                 	2. min-max的 scikit-learn 处理
                
                   2. scikit-learn.preprocessing中 类 `MinMaxScaler`
                
                      	1. 数据矩阵缩放到[0,1]之间
                
                   2. 示例
                
                        	1. ```
                                X_train = np.array([[ 1., -1.,  2.],
                                    					[ 2.,  0.,  0.],
                                    					[ 0.,  1., -1.]])
                                min_max_scaler = preprocessing.MinMaxScaler()  #转换器
                                X_train_minmax = min_max_scaler.fit_transform(X_train)
                                X_train_minmax
                            ```
                           
                            1. ```
                                      array([[0.5       , 0.        , 1.        ],
                                             [1.        , 0.5       , 0.33333333],
                                             [0.        , 1.        , 0.        ]])
                                   ```
                
                  

     2. 标准化

         1. 常用 `z-score` 标准化，均值 `0`，标准差 `1`

         2. 公式

             	1. $X′=\frac{x−μ}{σ}$
             	 	1.  μ 均值，σ 标准差

         3. 方法

             1. sklearn 提供 `StandardScalar` 类 列标准化

             2. 示例

                 1. ```
                    import numpy as np
                    X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])
                    from sklearn.preprocessing import StandardScaler
                    std = StandardScaler()  #转换器模型
                    X_train_std = std.fit_transform(X_train)
                    X_train_std
                    ```

                     1. ```
                        array([[ 0.        , -1.22474487,  1.33630621],
                               [ 1.22474487,  0.        , -0.26726124],
                               [-1.22474487,  1.22474487, -1.06904497]])
                        ```

     3. 缺失值

         1. 数据集 缺少的值， 编码为 `空白`，`NaN`或其他占位符

         2. 处理方法

             1. 填充缺失值 sklearn.preprocessing 的 `Imputer` 类 数据的填充

                 1. ```
                    class Imputer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)
                        """
                        用于完成缺失值的补充
                    
                        :param param missing_values: integer or "NaN", optional (default="NaN")
                            丢失值的占位符，对于编码为np.nan的缺失值，使用字符串值“NaN”
                    
                        :param strategy: string, optional (default="mean")
                            插补策略
                            如果是“平均值”，则使用沿轴的平均值替换缺失值
                            如果为“中位数”，则使用沿轴的中位数替换缺失值
                            如果“most_frequent”，则使用沿轴最频繁的值替换缺失
                    
                        :param axis: integer, optional (default=0)
                            插补的轴
                            如果axis = 0，则沿列排列
                            如果axis = 1，则沿行排列
                        """
                    ```

             2. 示例

                 1. ```
                    import numpy as np
                    from sklearn.preprocessing import Imputer
                    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
                    imp.fit([[1, 2], [np.nan, 3], [7, 6]])
                    X = [[np.nan, 2], [6, np.nan], [7, 6]]
                    imp.transform(X)
                    ```

                     1. ```
                        array([[4.        , 2.        ],
                               [6.        , 3.66666667],
                               [7.        , 6.        ]])
                        ```

 2. 多个特征

     1. 降维 之 PCA

         1. PCA（Principal component analysis），主成分分析

             	1. 保存数据集中对 `方差` 影响大的 特征
             	2. PCA 易受 特征范围影响， 用PCA前要特征标准化

         2. 转换器 `sklearn.decomposition.PCA`

             1. ```
                class PCA(sklearn.decomposition.base)
                   """
                   主成成分分析
                
                   :param n_components: int, float, None or string
                        参数指定 PCA 降维后的特征维度数目。最常用的做法直接指定降维到的维度数目，此时n_components是一个大于1的整数。
                       也可用默认值，即不输入n_components，此时n_components=min(样本数，特征数)
                
                   :param whiten: bool, optional (default False)
                      判断是否白化。 白化: 对降维后的数据的每个特征 归一化。PCA降维本身一般不需要白化,PCA降维后有后续的数据处理，考虑白化，默认值 False，即不进行白化
                
                   :param svd_solver:
                      选择一个合适的SVD算法来降维,一般来说， 用默认值就够了。
                    """
                ```

             2. 示例

                 1. ```
                    import numpy as np
                    from sklearn.decomposition import PCA
                    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
                    pca = PCA(n_components=2)
                    pca.fit(X)
                    #PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='auto', tol=0.0, whiten=False)
                    pca.explained_variance_ratio_ 
                    ```

                    1. `array([0.99244289, 0.00755711])`

        

#### 1.4 数据的特征选择

1. 特征选择

   1. 降维:
      1. 本质从一个维度空间映射到另一个维度空间
   2. 特征选择 很多方法
      1. Filter(过滤式): VarianceThreshold
         1. 数据本身不变， 维度减少
      2. Embedded(嵌入式)：正则化、决策树
         1. 数据的值，维度 都改变
      3. Wrapper(包裹式)
         1. 一种自动学习的特征选择方法
   3. 特征选择 两个功能
      1. 减少特征数量，降维
         1. 使模型泛化能力更强，减少过拟合
      2. 增强特征和特征值之间的理解

2. sklearn.feature_selection

   1. 原则: 去掉 取值变化小 的特征（删除低方差特征）

   2. VarianceThreshold  : 特征选择 的 基本方法

      1. 性质
         1. 移除所有方差不满足阈值的特征
         2. 默认, 移除所有方差为 `0` 的特征，即 数值完全相同的特征。

   3. 示例: 移除 超过80%的数据都为1或0的特征

      1. ```
         from sklearn.feature_selection import VarianceThreshold
         X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
         sel = VarianceThreshold(threshold=(.8 * (1 - .8)))  #创建转换器
         sel.fit_transform(X)
         ```

         1. ```
            #输出结果
            array([[0, 1],
                   [1, 0],
                   [0, 0],
                   [1, 1],
                   [1, 0],
                   [1, 1]])
            ```

### 2 sklearn数据集与机器学习组成

1. 机器学习组成：模型、策略、优化
   1. 定义
      1. 机器学习=模型+策略+算法    #《统计机器学习》
      2. Learning= Representation+Evalution+Optimization
      3. 机器学习 三部分 ：表示(模型)、评价(策略)、优化(算法)
         1. 表示(模型)：Representation
            1. 主要做的是建模，故称 模型
            2. 模型主要工作是转换：实际问题转化为计算机问题，即 建模
         2. 评价(策略)：Evalution
            1. 目标 : 判断模型的优劣
            2. 评价是一个指标， 表示模型的优劣
            3. 评价的指标 及 评价函数的 设计
         3. 优化：Optimization
            1. 目标: 评价的函数
            2. 找到最好的模型，评价最高的模型
2. 开发机器学习应用程序的步骤
   1. 收集数据
      1. 很多方法收集样本数据
         1. 网络爬虫
         2. RSS反馈
         3. API 得到信息
         4. 设备发送过来的实测数据
   2. 准备输入数据
      1. 处理数据, 数据格式符合要求
   3. 分析输入数据
      1. 没有垃圾数据
      2. 用信任的数据来源
   4. 训练算法
      1. 真正开始机器学习
      2. 若 无监督学习算法， 不存在目标变量值， 不需要训练算法
   5. 测试算法
      1. 实际使用第（4）步机器学习得到的 知识信息, 评估结果的准确率
   6. 使用算法
      1. 转化为应用程序，执行实际任务
      2. 碰到新的数据问题， 需要重复 上述步骤

#### 2.1 Scikit-learn数据集

 1. sklearn.datasets

     	1. `datasets.load_*()`
          	1. 获取 小规模数据集， 在datasets
          	2. `datasets.fetch_*()`
          	1. 获取 大规模数据集，网络下载
          	2. 参数
               	1. data_home #数据集下载的目录，默认 `~/scikit_learn_data/`，修改默认目录要修改环境变量SCIKIT_LEARN_DATA
          	3. `datasets.make_*()`
          	1. 本地数据集
          	4. 小结
          	1. load 和 fetch 返回的数据类型 `datasets.base.Bunch`，本质是一个 dict，键值对通过对象的属性访问
               	1. 主要属性
                    	1. data：特征数组，是 n_samples * n_features 的二维 numpy.ndarray 数组
                    	2. target：标签数组，是 n_samples 的一维 numpy.ndarray 数组
                    	3. DESCR：数据描述
                    	4. feature_names：特征名
                    	5. target_names：标签名
          	2. 数据集目录获取删除
               	1.  `datasets.get_data_home()` 获取数据集目录
                    	1. 返回 scikit 学习数据目录的路径。
                    	2. 一些大的 `数据集装载器` 使用这个文件夹，避免下载数据。
                    	3. 默认情况下，数据目录 为 “scikit_learn_data”文件夹。
                    	4. 通过“SCIKIT_LEARN_DATA”环境变量或 显式的文件夹路径以编程方式设置它
               	2.  `clear_data_home(data_home=None)`删除所有下载数据
                    	1. `sklearn.datasets.clear_data_home(data_home=None)`

 2. 获取小数据集

     1. 分类

         1. `sklearn.datasets.load_iris`

             1. 源码

                1. ```
                   class sklearn.datasets.load_iris(return_X_y=False)
                     """
                     加载并返回虹膜数据集
                   
                     :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False
                   
                     :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）
                     """
                   ```

                2. 示例
                
                      1. ```
                         from sklearn.datasets import load_iris
                         data = load_iris()
                         data.target
                         
                         #形状
                         data.data.shape  #(150, 4)
                         
                         #数据特征名称
                         data.feature_names
                         
                         #数据标签名
                         data.target_names  #array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
                         
                         data.target[[1,10, 100]]  #array([0, 0, 2])
                         ```

         2. `sklearn.datasets.load_digits`

             1. 源码

                 1. ```
                     class sklearn.datasets.load_digits(n_class=10, return_X_y=False)
                     	"""
                     	加载并返回数字数据集
                     	:param n_class: 整数，介于0和10之间，可选（默认= 10，要返回的类的数量
                     	:param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False
                     	:return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）
                     	"""
                     ```
         
             2. 示例
         
            1. ```
                   from sklearn.datasets import load_digits
                  digits = load_digits()
                   digits.data.shape
                   digits.target
                   digits.target_names
                   digits.images
               ```
         
             3. 小结
         
            1. load_digits 与 load_iris 属性有相同的也有不同的
         
            1. | load_iris        | load_digits      |
               | ---------------- | ---------------- |
               | `.data`          | `.data`          |
               | `.data.shape`    | `.data.shape`    |
               | `.feature_names` | `.feature_names` |
               | `.target`        | `.target`        |
               | `.target_names`  | `.target_names`  |
               |                  | `.images`        |
           
            
        
     2. 回归
        
        	1. sklearn.datasets.load_boston
        		
        	1. 源码
        	 	
        	    	1. ```
        	    	class  sklearn.datasets.load_boston(return_X_y=False)
        	    	  """
        	    	  加载并返回波士顿房价数据集
        	    	
        	    	  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False
        	    	
        	    	  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）
        	    	  """
        	```
        		
        	2. 示例
        	 	
        	    	1. ```
        	    	from sklearn.datasets import load_boston
        	    	boston = load_boston()
        	    	boston.data.shape
        	    	boston.feature_names
        	```
        	
        	2. sklearn.datasets.load_diabetes
        		
        	1. 源码
        	 	
        	    	1. ```
        	    	class sklearn.datasets.load_diabetes(return_X_y=False)
        	    	  """
        	    	  加载和返回糖尿病数据集
        	    	
        	    	  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False
        	    	
        	    	  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）
        	    	  """
        	```
        		
        	2. 示例
        	 	
        	    	1. ```
        	    	from sklearn.datasets import load_boston
        	    	diabetes = load_diabetes()
        	    	diabetes.data
        	```
        	
        	3. 小结
        		
        	1. 小结
        	 	
        	1. load_boston 与 load_diabetes 属性 相同
        	 	
        	    	1. | load_boston      | load_diabetes    |
        	    	   | ---------------- | ---------------- |
        	    	   | `.data`          | `.data`          |
        	    	   | `.data.shape`    | `.data.shape`    |
        	    	   | `.feature_names` | `.feature_names` |
        	    	   | `.target`        | `.target`        |

 3. 获取大数据集

     1. sklearn.datasets.fetch_20newsgroups

         1. 源码

             1. ```
                class sklearn.datasets.fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)
                  """
                  加载20个新闻组数据集中的文件名和数据
                
                  :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序
                
                
                  :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中
                
                  :param categories: 无或字符串或Unicode的集合，如果没有（默认），加载所有类别。如果不是无，要加载的类别名称列表（忽略其他类别）
                
                  :param shuffle: 是否对数据进行洗牌
                
                  :param random_state: numpy随机数生成器或种子整数
                
                  :param download_if_missing: 可选，默认为True，如果False，如果数据不在本地可用而不是尝试从源站点下载数据，则引发IOError
                
                  :param remove: 元组
                  """
                ```

           2. 示例

               1. ```
                  from sklearn.datasets import fetch_20newsgroups
                  data_test = fetch_20newsgroups(subset='test',shuffle=True, random_state=42)
                  data_train = fetch_20newsgroups(subset='train',shuffle=True, random_state=42)  	   
                  ```
     2. sklearn.datasets.fetch_20newsgroups_vectorized

        1. 源码

           1. ```
              class sklearn.datasets.fetch_20newsgroups_vectorized(subset='train', remove=(), data_home=None)
                """
                加载20个新闻组数据集并将其转换为tf-idf向量，这是一个方便的功能; 使用sklearn.feature_extraction.text.Vectorizer的默认设置完成tf-idf 转换。对于更高级的使用（停止词过滤，n-gram提取等），将fetch_20newsgroup与自定义Vectorizer或CountVectorizer组合在一起
              
                :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序
              
                :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中
              
              :param remove: 元组
                    """
              ```

        2. 示例

              1. ```
                    from sklearn.datasets import fetch_20newsgroups_vectorized
                    bunch = fetch_20newsgroups_vectorized(subset='all')
                    from sklearn.utils import shuffle
                    X, y = shuffle(bunch.data, bunch.target)
                    offset = int(X.shape[0] * 0.8)
                    X_train, y_train = X[:offset], y[:offset]
                    X_test, y_test = X[offset:], y[offset:]
                    ```

                    1. `shuffle()`  #洗牌数据    	

 4. 获取本地生成数据

     1. 本地分类数据

         1. sklearn.datasets.make_classification

             1. 源码

                 1. ```
                    class make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)
                    """
                    生成用于分类的数据集
                    
                    :param n_samples:int，optional（default = 100)，样本数量
                    
                    :param n_features:int，可选（默认= 20），特征总数
                    
                    :param n_classes:int，可选（default = 2),类（或标签）的分类问题的数量
                    
                    :param random_state:int，RandomState实例或无，可选（默认=无）
                      如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random
                    
                    :return :X,特征数据集；y,目标分类值
                    """
                    ```

                     	1. 样本数, 特征数, 分类数

             2. 示例

                 1. ```
                    from sklearn.datasets.samples_generator import make_classification
                    X,y= make_classification(n_samples=100000, n_features=20,n_informative=2, n_redundant=10,random_state=42)
                    ```

                 	2. 生成本地回归数据
	
                  	1. sklearn.datasets.make_regression
	
                   	1. 源码
	
                           	1. ```
                       class make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)
                         """
                         生成用于回归的数据集
                             
                         :param n_samples:int，optional（default = 100)，样本数量
                             
                         :param  n_features:int,optional（default = 100)，特征数量
                             
                         :param  coef:boolean，optional（default = False），如果为True，则返回底层线性模型的系数
                             
                         :param random_state:int，RandomState实例或无，可选（默认=无）
                           如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random
                             
                         :return :X,特征数据集；y,目标值
                         """
                       
	            ```
                    
	           ```
        ```
                    
                 1. 示例
                 
                    1. ```
                       from sklearn.datasets.samples_generator import make_regression
                       X, y = make_regression(n_samples=200, n_features=5000, random_state=42)
        ```

#### 2.2 模型的选择

1. 机器学习 种类
   1. 监督学习 : 要预测目标变量的值
      1. 分类 : k-近邻算法、决策树、贝叶斯、逻辑回归(LR)、支持向量机(SVM)
      2. 回归 : 线性回归、岭回归
      3. 标注 : 隐马尔可夫模型(HMM)
   2. 无监督学习 : 不预测目标变量的值
      1. 聚类 k-means
2. 选择合适的算法模型
   1. 两个问题
      1. 机器学习算法的目的，想要算法完成何种任务
         1. 监督学习 : 要预测目标变量的值
         2. 无监督学习 : 不预测目标变量的值
      2. 需要分析或者收集的数据是什么
         1. 特征值是 **离散型变量** 还是 **连续型变量** 
         2. 特征值中是否存在缺失的值
3. 监督学习中三类问题
   1. 分类问题 
      1. 监督学习的 一个重要问题
      2. 输出变量取有限个离散值
      3. 输入变量可以是离散的，也可以是连续的
      4. 从数据中学习一个 分类模型 或 分类决策函数，称为 分类器。分类器对新的输入预测输出，称为分类
      5. 模型: 
         1. 输入(X1, Y1) , (X2, Y2)...--> 学习系统 --> 模型 <--> 分类系统
         2. 输入 X(n+1) --> 分类系统 --> Y(n+1)
      6. 分类问题包括 学习 和 分类 两个过程
         1. 学习过程: 根据已知的训练数据集利用有效的学习方法学习一个分类器
         2. 分类过程: 利用学习的分类器对新的输入实例 分类
      7. 应用: 银行业务, 网络安全领域, 图像处理, 文本分类(新闻报道、网页、电子邮件、学术论文)
   2. 回归问题
      1. 监督学习的 一个重要问题
      2. 预测输入变量和输出变量之间的关系
      3. 回归模型: 表示从输入到输出变量之间映射的函数
      4. 模型: 
         1. 输入(X1, Y1) , (X2, Y2)... --> 学习系统 --> 模型 <--> 预测系统 Y=f(X)
         2. 输入 X(n+1) --> 预测系统 Y=f(X) --> Y(n+1)
      5. 回归问题分类
         1. 按 输入变量的个数，分 一元回归 多元回归
         2. 按 输入变量和输出变量之间关系 分 线性回归 非线性回归
      6. 应用: 市场趋势预测、产品质量管理、客户满意度调查、偷袭风险分析
   3. 标注问题
      1. 标注问题是分类问题的一个推广
      2. 输入是一个 观测序列，输出 是一个 标记序列 或 状态序列
      3. 应用: 信息抽取、自然语言处理 -- 词性标注
      4. 模型: 
         1. 输入(X1, Y1) , (X2, Y2)...--> 学习系统 --> 模型 <--> 标注系统
         2. 输入 X(n+1) --> 标注系统 --> Y(n+1)
   4. 小结
      1. 主要关注 分类和回归问题，并且标注问题的算法复杂

#### 2.3 模型检验-交叉验证

* 给定的样本空间,  大部分样本 训练集, 小部分样本 预测

1. `sklearn.cross_validation.train_test_split()`	

   1. cross_validation中的train_test_split方法

      1. 用途: 分割 训练集与测试集
         1. 特点: 它使用 交叉验证迭代器, 后者内有打散索引的选项

   2. 源码

      1. ```
         def train_test_split(*arrays,**options)
           """
           :param arrays: 输入 : 列表，数字阵列
         
           :param test_size:float，int或None（默认为无）,如果浮点数应在0.0和1.0之间，并且表示要包括在测试拆分中的数据集的比例。如果int，表示测试样本的绝对数
         
           :param train_size:float，int或None（默认为无）,如果浮点数应在0.0到1.0之间，表示数据集包含在列车拆分中的比例。如果int，表示列车样本的绝对数
         
           :param random_state:int或RandomState, 伪随机数发生器状态，参数 random_state 默认设置为 None，意味着每次打散是不同的。
           """
         ```

   3. 示例

      1. ```
         from sklearn.cross_validation import train_test_split
         from sklearn import datasets
         
         iris = datasets.load_iris()
         iris.data.shape,iris.target.shape
         X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=42)
         X_train.shape
         y_train.shape
         X_test.shape
         y_test.shape
         ```

2. 验证方法

   1. holdout method (留一法)

      1. holdout交叉验证(holdout cross validation) 方法
         1. 评估 模型泛化能力 的典型方法
         2. 一般, 少于原本样本的 三分之一 的数据选做 验证数据. 结果不具有说服性
         3. 其实: Holdout 验证并非一种交叉验证，因为数据 没有交叉使用

   2. k-折交叉验证

      1. k-折交叉验证: 

         1. 过程
            1. 全部样本分割成K个子样本，1 个子样本作验证数据，K-1个子样本训练。(只分割一次)
            2. 交叉验证重复K次，每个子样本验证一次，平均K次的结果 或 其它结合方式，得到一个估测
         2. 最常用 10 折交叉验证

      2. 好处:  所有数据 有被训练和验证的机会, 模型可信

      3. `sklearn.cross_validation.cross_val_score`

         1. 交叉验证的最简单的方法.  在估计器和数据集上使用

         2. 源码

            1. ```
               def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')
                 """
                 :param estimator:模型估计器
               
                 :param X:特征变量集合
               
                 :param y:目标变量
               
                 :param cv:int，默认的3折交叉验证，整数指定一个（分层）KFold中的折叠数
               
                 :return :预估系数
                 """
               ```

         3. 示例

            1. ```
               from sklearn.cross_validation import cross_val_score
               diabetes = datasets.load_diabetes()
               X = diabetes.data[:150]
               y = diabetes.target[:150]
               lasso = linear_model.Lasso()
               cross_val_score(lasso, X, y)
               ```

      4. 小结 : 交叉验证方法的目的 有2个

         1. 从有限的学习数据中获取尽可能多的 有效信息；
         2. 一定程度上避免 过拟合问题

#### 2.4 sklearn的estimator

1. 估计器 estimator
   1. sklearn中 一个重要的角色
   2. 分类器,回归器 属于estimator
2. estimator的工作流程
   1. 方法 fit, transform
      1. fit() 从训练集中学习模型 `参数`
      2. transform() 用学习到的参数 `转换` 数据
   2. 流程
      1. Training Data -- estimator.fit(x_train) --> Model --> estimator.trainform(x_train) --> Transformed Training Data
      2. Test Data --> Model -- estimator.transform(x_test) --> Transformed Test Data

### 3 sklearn的分类器算法

* 分类算法以及案例， 分类模型的 评估方法

#### 3.1 分类算法 之 k-近邻

1. k-近邻算法

   1. 测量不同特征值之间的距离 分类
   2. 优点: 精度高、 异常值不敏感、无数据输入假定
   3. 缺点：计算 复杂度高、空间复杂度高
   4. 使用数据范围：数值型和标称型

2. 一个例子弄懂k-近邻

   1. `K-近邻算法`  分类爱情片和动作片
   2. 方法:
      1. 训练样本集 中有M个 样本
      2. 一个未知类型数据集合中选一个测试样本 计算与 M个训练样本 的距离, 排序后选出最近的K个. K个中选最近数据中次数最多的分类. K取值一般不大于20个.
   3. 计算距离
      1. 欧式距离

3. sklearn.neighbors

   1. 基于邻居的 监督学习方法

   2. `sklearn.neighbors.KNeighborsClassifier` 最近邻分类器

      1. 源码

         1. ```
            class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)**
              """
              :param n_neighbors：int，可选（默认= 5），k_neighbors查询默认使用的邻居数
            
              :param algorithm：{'auto'，'ball_tree'，'kd_tree'，'brute'}，可选用于计算最近邻居的算法：'ball_tree'将会使用 BallTree，'kd_tree'将使用 KDTree，“野兽”将使用强力搜索。'auto'将尝试根据传递给fit方法的值来决定最合适的算法。
            
              :param n_jobs：int，可选（默认= 1),用于邻居搜索的并行作业数。如果-1，则将作业数设置为CPU内核数。不影响fit方法。
            
              """
            ```

      2. 示例

         1. ```
            import numpy as np
            from sklearn.neighbors import KNeighborsClassifier  #最近邻分类器
            
            neigh = KNeighborsClassifier(n_neighbors=3)
            ```

   3. Method

      1. fit(X, y)  #训练数据

         1. 参数

            1. X 训练数据, 拟合模型
            2. y 作为X的类别值
            3. X，y 为 数组 或 矩阵

         2. ```
            X = np.array([[1,1],[1,1.1],[0,0],[0,0.1]])
            y = np.array([1,1,0,0])
            neigh.fit(X,y)
            ```

      2. kneighbors(X=None, n_neighbors=None, return_distance=True)   #求最近的k个邻居

         1. 参数

            1. 指定点集 X 的 `n_neighbors` 个邻居
            2. return_distance为False的话，不返回距离

         2. ```
            neigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False)  #array([[1, 0, 3]], dtype=int64)
            
            neigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False,n_neighbors=2)  #array([[1, 0]], dtype=int64)
            ```

      3. predict(X)  #预测 数据的类标签

         1. `neigh.predict(np.array([[0.1,0.1],[1.1,1.1]]))`

      4. predict_proba(X)  #返回测试数据X属于某一类别的概率估计

         1. `neigh.predict_proba(np.array([[1.1,1.1]]))`

#### 3.2 k-近邻算法案例

1. 读入Iris数据集

   1. ```
      from sklearn.datasets import load_iris
      # 使用加载器读取数据并且存入变量iris
      iris = load_iris()
      
      # 查验数据规模
      iris.data.shape
      
      # 查看数据说明（这是一个好习惯）
      iris.DESCR
      ```

2. Iris数据集 分割

   1. ```
      from sklearn.cross_validation import train_test_split
      X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.25,random_state=42)
      ```

3. 特征数据 标准化

   1. ```
      from sklearn.preprocessing import StandardScaler
      
      ss = StandardScaler()
      X_train = ss.fit_transform(X_train)
      X_test = ss.fit_transform(X_test)
      ```

4. K近邻算法 小结

   1. 代码

      1. ```
         from sklearn.datasets import load_iris
         from sklearn.cross_validation import train_test_split
         from sklearn.preprocessing import StandardScaler
         from sklearn.neighbors import KNeighborsClassifier
         from sklearn.metrics import classification_report
         from sklearn.model_selection import GridSearchCV
         
         def knniris():
             """
             鸢尾花分类
             :return: None
             """
         
             # 数据集获取和分割
             lr = load_iris()
         
             x_train, x_test, y_train, y_test = train_test_split(lr.data, lr.target, test_size=0.25)
         
             # 进行标准化
         
             std = StandardScaler()
         
             x_train = std.fit_transform(x_train)
             x_test = std.transform(x_test)
         
             # estimator流程
             knn = KNeighborsClassifier()
         
             # 得出模型
             # knn.fit(x_train,y_train)
             #
             # 进行预测或者得出精度
             # y_predict = knn.predict(x_test)
         
             # score = knn.score(x_test,y_test)
         
             # 通过网格搜索,n_neighbors为参数列表
             param = {"n_neighbors": [3, 5, 7]}
         
             gs = GridSearchCV(knn, param_grid=param, cv=10)
         
             # 建立模型
             gs.fit(x_train,y_train)
         
             # print(gs)
         
             # 预测数据
             print(gs.score(x_test,y_test))
         
             # 分类模型的精确率和召回率
             # print("每个类别的精确率与召回率：",classification_report(y_test, y_predict,target_names=lr.target_names))
         
             return None
         
         if __name__ == "__main__":
             knniris()
         ```

5. 小结

   1. K近邻算法
      1. 非常直观的机器学习模型
      2. K近邻算法没有参数训练过程
      3. 根据测试样本训练数据的分布直接作出分类决策

#### 3.3 分类算法 之 朴素贝叶斯

1. 朴素贝叶斯

   1. Naive Bayes
   2. 朴素贝叶斯分类器的理论基础 : 贝叶斯理论
      1. 概率 P(X)

2. 概率论基础

   1. 联合概率与条件概率

      1. 联合概率 : 两件事情同时发生的概率
         1. `P(XY)`
      2. 条件概率
         1. P(X|Y)  :  在Y发生的条件下，X发生的概率
      3. 每个事件之间相互独立
         1. $P(X_1,X_2,X_3,...,X_n∣Y_i)=P(X_1∣Y_i)P(X_2∣Y_i)P(X_3∣Y_i)...P(X_n∣Y_i)$
            1. 给定条件Y下 所有的X的概率 为 Y条件下每个X发生的概率乘积

   2. 贝叶斯公式

      1. $P( C_i ∣ W )= \frac{P ( W ∣ C_i ) P ( C_i )}{P ( W ) }$
         1. $C_i$  类别，W 特征向量(文本词向量),
         2. $P(W ∣ C_i )$  : 在给定类别 $C_i$ 的情况下，文档的词向量是W的概率
         3. 用于文本分类 : 
            1. 意义:  给定一个文本词向量W ， W属于类别 $C_i$ 的概率是多少

   3. 词袋法的特征值计算

      1. 已分类的文档

         1. ```
            a = "life is short,i like python"
            b = "life is too long,i dislike python"
            c = "yes,i like python"
            label=[1,0,1]
            ```

      2. 词袋法 

         1. 训练集中 文本的词汇表, 即 训练集中的文本的单词(不重复)统计为 词典

         2. |      | life |  is  |  i   | short | long | like | dislike | too  | python | yes  |
            | :--: | :--: | :--: | :--: | :---: | :--: | :--: | :-----: | :--: | :----: | :--: |
            |  a'  |  1   |  1   |  1   |   1   |  0   |  1   |    0    |  0   |   1    |  0   |
            |  b'  |  1   |  1   |  1   |   0   |  1   |  0   |    1    |  1   |   1    |  0   |
            |  c'  |  0   |  0   |  1   |   0   |  0   |  1   |    0    |  0   |   1    |  1   |

            1. a',b' 是两个文档(或文本)的 词向量 的 表现形式

         3. 已给定的文档类别，每个单词特征向量的概率是多少

            1. TF计算方法

               1. $P_i = \frac{N_i}{N}$
               2. 文档类别$y_k$每个单词出现的次数$N_i$, 除以文档类别 $y_k$ 中所有单词出现的总数N

            2. 应用

               1. 先求出现总数

                  1. `1`类别文档， a'中， 总数为1+1+1+1+1+1=6，c'中，总数为1+1+1+1=4，故在1类别文档中总数为10次

               2. 每个单词出现次数

                  1. 假设是两个列表，a'+c'就能得出每个单词出现次数

               3. 每个单词特征向量的概率

                  1. 如$P\left({w=python}\right) = \frac{2}{10} = {0.20000000}$,

                  2. 同样得到其它的单词概率

                  3. 结果

                     1. ```
                        # 类别1文档中的词向量概率
                        p1 = [0.10000000, 0.10000000, 0.20000000, 0.10000000, 0, 0.20000000, 0, 0, 0.20000000, 0.10000000]
                        # 类别0文档中的词向量概率
                        p0 = [0.16666667, 0.16666667, 0.16666667, 0,0.16666667, 0, 0.16666667, 0.16666667, 0.16666667, 0]
                        ```

   4. 拉普拉斯平滑系数

      1. 为避免训练集样本 一些特征的缺失(即某些特征出现的次数为0)， 计算$P\left({X_1,X_2,X_3,...,X_n}\mid{Y_i}\right)$  时 ，各个概率相乘 为零, 对 概率公式做 平滑处理
      2. $ P_i =\frac{N_i + α}{N + α m}  $
         1. m 特征词向量的个数
         2. α为平滑系数，当α=1，称为拉普拉斯平滑

3. sklearn.naive_bayes.MultinomialNB

   1. 源码

      1. ```
         class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
           """
           :param alpha：float，optional（default = 1.0）加法（拉普拉斯/ Lidstone）平滑参数（0为无平滑）
           """
         ```

   2. 方法

      1. | 方法                                          | 说明                             |
         | --------------------------------------------- | -------------------------------- |
         | fit(X, y[,sample_ weight])                    | 根据X，y适合朴素贝叶斯分类器     |
         | get_params ([深])                             | 获取此估计器的参数               |
         | partial_ fit (X, y[,classes, sample_ weight]) | 增量适合一批样品                 |
         | predict(X)                                    | 对测试向量X的数组 分类           |
         | predict_log_proba (X)                         | 返回测试矢量X的对数概率估计      |
         | predict_proba (X)                             | 测试矢量X的返回概率估计          |
         | score(X，y[,sample_weight])                   | 返回给定测试数据和标签的平均精度 |
         | `set_ params (\*\*PARAMS)`                    | 设置该估计器的参数               |

4. 互联网新闻分类

   1. `读取` 20类新闻文本的数据

      1. ```
         from sklearn.datasets import fetch_20newsgroups
         news = fetch_20newsgroups(subset='all')
         news.data[0]
         ```

   2. 20类新闻文本数据 `分割`

      1. ```
         from sklearn.cross_validation import train_test_split
         
         X_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=42)
         ```

   3. 文本转换为 特征向量, TF特征抽取

      1. ```
         from sklearn.feature_extraction.text import CountVectorizer
         
         vec = CountVectorizer()  #向量转换器
         # 训练数据输入，并转换为特征向量
         X_train = vec.fit_transform(X_train)
         # 测试数据转换
         X_test = vec.transform(X_test)
         ```

   4. 朴素贝叶斯分类器 预测文本数据 类别

      1. ```
         from sklearn.naive_bayes import MultinomialNB
         
         #平滑处理 初始化的朴素贝叶斯模型
         mnb = MultinomialNB(alpha=1.0)
         
         # 用训练数据估计 模型参数
         mnb.fit(X_train,y_train)
         
         #预测 测试验本类别。结果存储在y_predict
         y_predict = mnb.predict(X_test)
         ```

   5. 性能测试

      1. 特点分析
         1. 朴素贝叶斯模型 广泛应用于海量互联网文本 分类任务
         2. 较强的特征条件独立假设
            1. 模型预测的参数规模减少,  幂指数量级 减小为 线性量级 
            2. 数据特征关联性较强的分类任务 性能不佳

#### 3.4 分类算法 之 逻辑回归

1. 逻辑回归（Logistic Regression），简称LR

   1. 特点: 将特征输入集合转化为 `0` 和 `1` 两类的概率
   2. 优点：计算代价不高，易于理解和实现
   3. 缺点：容易欠拟合，分类精度不高
   4. 适用数据：数值型和标称型

2. 逻辑回归

   1. Logistic回归 分类 `0/1` 问题
   2. Logistic回归本质 是线性回归
      1. 在特征到结果的映射中加入了一层 `函数映射`，即先对特征线性求和，然后使用函数g(z)作假设函数来预测。g(z) 将连续值映射到 `0` 或 `1`
      2. 映射函数
         1. $g(z) = \frac{1}{1 + e^{-z}}$
            1. $z=\theta_0 + \theta_1 x_1 + \theta_2 x_2 $

3. sklearn.linear_model.LogisticRegression   逻辑回归类

   1. 源码

      1. ```
         class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)
           """
           :param C: float，默认值：1.0
         
           :param penalty: 特征选择的方式
         
           :param tol: 公差停止标准
           """
         ```

   2. 方法

      1. | 方法                              | 说明                                                         |
         | --------------------------------- | ------------------------------------------------------------ |
         | decision_function(X)              | 预测样品的直信度分数                                         |
         | densify()                         | 将系数矩阵转换为密集数组格式                                 |
         | fit(X, y[,sample_weight])         | 根据给定的训练数据拟合模型                                   |
         | fit_transform(X[, y])             | 适合数据，然后转换                                           |
         | get_params([深])                  | 获取此估计器的参数                                           |
         | predict (X)                       | 预测X中样本的类标签                                          |
         | predict_log_proba (X)             | 概率估计的记录                                               |
         | predict_proba (X)                 | 概率估计                                                     |
         | score (X, y[,sample_ weight)      | 返回给定测试数据和标签的评估精度                             |
         | `set_params(\*\*PARAMS)`          | 设置该估计器的参数                                           |
         | sparsify ()                       | 将系数矩阵转换为稀疏格式                                     |
         | `transform (\*args, \*\*\kwargs)` | DEPRECATED: 在版本0.19中将删除对使用估计器作为特征选择器的支持。 |
   
3. 示例
   
      1. ```
         from sklearn.model_selection import train_test_split
         from sklearn.datasets import load_digits
         from sklearn.linear_model import LogisticRegression
         
         digits = load_digits()
         X = digits.data
         y = digits.target
         
         LR = LogisticRegression(C=1.0, penalty='l1', tol=0.01)
         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
         LR.fit(X_train,y_train)
         LR.predict(X_test)
         LR.score(X_test,y_test) #0.96464646464646464
         ```
   ```
   
   ```
   
4. 属性
   
      1. coef_  #决策功能的特征系数
      1. 返回 `numpy.ndarray` 二维数据
      2. C  #数组C，即用于交叉验证的正则化参数值的倒数
         1. 返回 float 类型
      
   5. 特点分析
   
      1. 线性分类器 是最为基本和常用的机器学习模型
   ```
   
   ```

#### 3.5 逻辑回归算法 之 案例

1. 良／恶性乳腺癌肿瘤预测

   1. 数据 

      1. 原始数据的下载地址：`https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/`

   2. 数据预处理

      1. ```
         import pandas as pd
         import numpy as np
         
         # 根据官方数据构建类别
         column_names = ['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class'],
         
         data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin//breast-cancer-wisconsin.data',names = column_names)
         
         # 将？替换成 标准缺失值NaN
         data = data.replace(to_replace='?',value = np.nan)
         
         # 丢弃带有缺失值的数据（只要一个维度有缺失）
         data = data.dropna(how='any')
         
         data.shape
         ```

   3. 准备训练测试数据

      1. ```
         from sklearn.cross_validation import train_test_split
         
         X_train,X_test,y_train,y_test = train_test_split(data[column_names[1:10]], data[column_names[10]], test_size=0.25,random_state=42)
         
         # 查看训练和测试样本的数量和类别分布
         y_train.value_counts()
         
         y_test.value_counts()
         ```

   4. 使用逻辑回归进行良／恶性肿瘤预测任务

      1. ```
         from sklearn.preprocessing import StandardScaler
         from sklearn.linear_model import LogisticRegression
         
         
         # 标准化数据, 每个维度的特征数据方差为1，均值为0。预测结果不会被某些维度过大的特征值而主导
         ss = StandardScaler()
         
         X_train = ss.fit_transform(X_train)
         X_test = ss.transform(X_test)
         
         # 初始化 LogisticRegression
         
         lr = LogisticRegression(C=1.0, penalty='l1', tol=0.01)
         
         # 跳用LogisticRegression中的fit函数／模块来训练模型参数
         lr.fit(X_train,y_train)
         
         lr_y_predict = lr.predict(X_test)
         
         #查看类型
         type(lr_y_predict)  #numpy.ndarray
         ```

   5. 性能分析

      1. ```
         from sklearn.metrics import classification_report
         
         #利用逻辑斯蒂回归自带的评分函数 score 获得模型在测试集上的结果
         '精确率为：',lr.score(X_test,y_test)  #('精确率为：', 0.9532163742690059)
         
         #分类报告
         classification_report(y_test,lr_y_predict,target_names = ['Benign','Maligant'])
         ```

#### 3.6 分类器 性能评估

 1. 二类分类 性能评估

     1. 四种 组合，构成 混淆矩阵

         1. <table>
                <tr><td rowspan=2 colspan=2>二分类混淆矩阵</td><td colspan=2>真实值</td> </tr>
                <tr><td>正例</td><td>负例</td></tr>
                <tr><td rowspan=2>预测值</td><td>真例</td><td>真正例TP</td><td>假负例FN</td></tr>
                <tr><td>假例</td><td>假正例FP</td><td>真反例TN</td></tr>
            </table>

           	2. 二级指标（最底层指标加减乘除得到的)

                     	1. |                            | 公式                                     | 意义                                                    |
         
            	| -------------------------- | ---------------------------------------- | ------------------------------------------------------- |
            	| 准确率, ACC                | $Accuracy=\frac{TP+ TN}{TP+TN+FP+FN}$    | 分类模型 判断正确的结果的 个数 占总观测值的 个数 的比重 |
            	| 精确率, PRE                | $Precision =\frac{TP}{TP+ FP}$           | 预测是Positive的结果中,模型预测对的个数比重             |
            	| 灵敏度, 又称 召回率 Recall | $Sensitivity= Recall =\frac{TP} {TP+FN}$ | 真实值是Positive的所有结果中,模型预测对的比重           |
            	| 特异度, TNR                | $Specifidity=\frac{TN}{TN+ FP}$          | 真实值是Negative的所有结果中,模型预测对的比重           |
            	
            	3. ### 三级指标
            	
            	1. 四个指标的基础上在进行拓展
            	2. F1 Score
            		1. $F1 = \frac{2PR}{P+R}$
            		2. $\frac{1}{F1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R})$
            		3. F1-Score 取值范围从 0到1 ，1 模型的输出最好，0代表模型的输出结果最差。

 2. sklearn.metrics.classification_report

     1. sklearn中 metrics 提供了 四个指标的模块， `classification_report`

     2. 源码

         1. ```
            classification_report(y_true, y_pred, labels=None, target_names=None, digits=2)
              """
              计算分类指标
              :param y_true:真实目标值
            
              :param y_pred:分类器返回的估计值
            
              :param target_names:可选的，计算与目标类别匹配的结果
            
              :param digits:格式化输出浮点值的位数
            
              :return :字符串，三个指标值
            
              """
            ```

     3. 示例

         1. ```
            from sklearn.metrics import classification_report
            y_true = [0, 1, 2, 2, 2]  #真实值, 3类
            y_pred = [0, 0, 2, 2, 1]  #预测值
            target_names = ['class 0', 'class 1', 'class 2']
            classification_report(y_true, y_pred, target_names=target_names)
            ```

            1. ```
                            precision    recall  f1-score   support
                        
                   class 0       0.50      1.00      0.67         1
                   class 1       0.00      0.00      0.00         1
                   class 2       1.00      0.67      0.80         3
                        
               avg / total       0.70      0.60      0.61         5
               ```

               

#### 3.7 分类算法 之 决策树、随机森林

1. 决策树

   1. 决策树是 基本的分类方法

   2. 决策树学习 

      1. 三步：特征选择、决策树的生成, 决策树剪枝
      2. 适用数据类型：数值型, 标称型

   3. 特征选择

      1. 选取对训练数据具有 分类 能力的特征

      2. 特征选择的过程

         1. |  ID  | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |
            | :--: | :--: | :----: | :----------: | :------: | :--: |
            |  1   | 青年 |   否   |      否      |   一般   |  否  |
            |  2   | 青年 |   否   |      否      |    好    |  否  |
            |  3   | 青年 |   是   |      否      |    好    |  是  |

      3. 特征选择的准则: 信息增益

   4. 信息的度量和作用

      1. 信息的度量
         1. 香农
         2. $H = -(p_1*logp_1 + p_2*logp_2 + ... + p_{32}*logp_{32})$
         3. $H(X)=-∑_{x∈X}P(x)logP(x)$
         4. H 信息熵，单位 比特
      2. 信息增益（information gain）
         1. 信息增益 意义
            1. 得知特征X的信息是类Y的信息的 不确定性减少的程度
         2. 特征A对 数据集D的信息增益g(D,A)
            1. ,定义为集合D的 经验熵H(D) 与特征A给定条件下D的 经验条件熵H(D|A) 之差
         3. 公式
            1. $g(D,A)=H(D)−H(D∣A)$
         4. 信息增益的准则的特征选择方法
            1. 对训练数据集D，计算每个特征的 `信息增益`，选择信息增益最大的特征
      3. 信息增益的计算
         1. 经验熵
            1. $H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}$
               2. 训练数据集为D，|D| 训练数据集样本个数。
               2. 有K个类$C_k$, k=1,2,3,4...k,$|C_k|$ 为类$C_k$的样本个数, $\sum_{k=1}^{K}|C_k|$=|D|
         2. 条件熵
            1. $H(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}$
               2. 特征A有n个不同的取值{a1,a2,...,an},
               2. 根据 A 的值, D分为 `n` 个子集D1，D2，...，Dn，|Di|为样本个数，$D_ik$为$D_i$中属于$C_k$类的样本的集合
         3. 信息增益 g(D, A) = H(D) - H(D|A)

   5. sklearn.tree.DecisionTreeClassifier  #对数据集 多分类的类

      1. 源码

         1. ```
            class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)
              """
              :param max_depth：int或None，可选（默认=无）树的最大深度。如果没有，那么节点将被扩展，直到所有的叶子都是纯类，或者直到所有的叶子都包含少于min_samples_split样本
            
              :param random_state：random_state是随机数生成器使用的种子
              """
            ```

      2. 示例

         1. 导入类, 数据集, 数据分成 训练数据集 和 测试数据集 两部分

         2. ```
            from sklearn.model_selection import train_test_split
            from sklearn.datasets import load_iris
            from sklearn.tree import DecisionTreeClassifier
            iris = load_iris()
            X = iris.data
            y = iris.target
            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)  #默认 X_train: y_train = 3:1
            estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)
            estimator.fit(X_train, y_train)
            ```

      3. DecisionTreeClassifier 对象的方法(method)
         1. apply  #返回每个被预测样本的 `叶子的索引`
            1. `estimator.apply(X)`
         2. decision_path  #返回树的决策路径
            1. `dp = estimator.decision_path(X_test)`
         3. fit_transform(X,y=None，fit_params)  #输入数据，然后转换
         4. predict(X)   #预测输入数据的类型,完整代码
            1. `estimator.predict(X_test)`
         5. score(X,y,sample_weight=None)   #返回给定测试数据的准确精度
            1. `estimator.score(X_test,y_test)`

   6. 决策树本地保存

      1. sklearn.tree.export_graphviz()   #函数导出DOT格式

         1. 示例

            1. ```
               from sklearn.datasets import load_iris
               from sklearn import tree
               clf = tree.DecisionTreeClassifier()
               iris = load_iris()
               clf = clf.fit(iris.data, iris.target)
               tree.export_graphviz(clf,out_file='tree.dot')
               ```
         
      2. tree.dot文件 通过命令转换为 png 或 pdf格式.

         1. 方法一

            1. 首先 安装graphviz
               1. ubuntu: `sudo apt-get install graphviz`
               2. Mac: `brew install graphviz`
            2. 转换
               1. $ `dot -Tps tree.dot -o tree.ps`
               2. $ `dot -Tpng tree.dot -o tree.png`

         2. 方法二

            1. 安装 Python模块 pydotplus, 直接在 `Python` 中生成PDF文件

               1. `pip install pydotplus`

            2. 运行

               1. ```
                  import pydotplus
                  dot_data = tree.export_graphviz(clf, out_file=None)
                  graph = pydotplus.graph_from_dot_data(dot_data)
                  graph.write_pdf("iris.pdf")
                  ```

   7. 各种决策树算法

      1. `ID3`
         1. 信息增益 最大的准则
      2. `C4.5`
         1. 信息增益比 最大的准则
      3. `CART`回归树 (平方误差 最小 分类树)
         1. 基尼系数 最小的准则 

   8. 决策树优缺点分析

      1. 优点
         1. 简单的理解和解释。树木可视化
         2. 很少的数据准备
      2. 缺点
         1. 过拟合
         2. 决策树可能不稳定.
            1. 合奏中的决策树来减轻这个问题

2. 集成方法（分类）之随机森林

   1. 随机森林

      1. 一种`分类器`, 包含多个决策树的
      2. 输出的类别, 由个别树输出的类别的众数而定
         1. 用相同的训练数 搭建多个独立的分类模型
         2. 投票, 少数服从多数 原则作出最终的分类决策

   2. 学习算法

      1. 建造每棵树的算法
         1. N: 训练用例（样本）个数，M: 特征数目。
         2. m: 输入特征数目，用于确定决策树的每个节点的决策结果； m << M。
         3. N个训练用例（样本） 有放回抽样，N次，形成一个训练集（即bootstrap取样）, 未抽到的用例（样本）作预测，评估其误差。
         4. 每个节点，随机选择m个特征，决策树上每个节点的决定 基于这些特征。根据这m个特征，计算 最佳的分裂方式

   3. sklearn.ensemble，集成方法模块

      1. RandomForestClassifier(随机森林)方法

         1. sklearn.ensemble提供 集成方法， 包含 RandomForestClassifier(随机森林)方法

         2. 源码

            1. ```
               class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None)
                 """
                 :param n_estimators：integer，optional（default = 10） 森林里的树木数量。
               
                 :param criteria：string，可选（default =“gini”）分割特征的测量方法
               
                 :param max_depth：integer或None，可选（默认=无）树的最大深度
               
                 :param bootstrap：boolean，optional（default = True）是否在构建树时使用自举样本。
               
                 """
               ```

         3. 属性

            1. classes_：shape = [n_classes]的数组或这样的数组的列表，类标签（单输出问题）或类标签数组列表（多输出问题）
         2. feature importances ：array = [n_features]的数组， 特征重要性（越高，功能越重要）。
         
         4. 随机森林的方法
         
            1. `fit（X，y [，sample_weight]）` #用训练集（X，Y）构建一棵树林
            2. `predict（X）` #预测X的类
            3. `score（X，y [，sample_weight]）` #返回给定测试数据和标签的平均精度
            4. `decision_path（X）` # 返回森林的决策路径

   4. 泰坦尼克号乘客数据案例 -- 随机森林案例

      1. ```
         import pandas as pd
         import sklearn
         ~~from sklearn.cross_validation import train_test_split~~  #已经删除了
         from sklearn.model_selection import train_test_split
         from sklearn.feature_extraction import DictVectorizer
         from sklearn.tree import DecisionTreeClassifier  #决策树分类器
         from sklearn.metrics import classification_report   #分类报告
         from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
         
         
         titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')
         
         #选取一些特征作为我们划分的依据
         x = titanic[['pclass', 'age', 'sex']]
         y = titanic['survived']
         
         # 填充缺失值
         x['age'].fillna(x['age'].mean(), inplace=True)
         
         x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)
         
         dt = DictVectorizer(sparse=False)   #转换器
         
         print(x_train.to_dict(orient="record"))
         
         # 按行，样本名字为键，列名也为键，[{"1":1,"2":2,"3":3}]
         x_train = dt.fit_transform(x_train.to_dict(orient="record"))
         
         x_test = dt.fit_transform(x_test.to_dict(orient="record"))
         
         # 使用决策树
         dtc = DecisionTreeClassifier()
         
         dtc.fit(x_train, y_train)
         
         dt_predict = dtc.predict(x_test)
         
         print(dtc.score(x_test, y_test))
         
         print(classification_report(y_test, dt_predict, target_names=["died", "survived"]))
         
         # 使用随机森林
         
         rfc = RandomForestClassifier()
         
         rfc.fit(x_train, y_train)
         
         rfc_y_predict = rfc.predict(x_test)
         
         print(rfc.score(x_test, y_test))
         
         print(classification_report(y_test, rfc_y_predict, target_names=["died", "survived"]))
         ```

### 4 回归算法

1. 机器学习监督学习算法分为 `分类`算法和 `回归`算法两种，根据 `类别标签分布类型` 为离散型、连续性而定义的
   1. 归算法用于连续型分布预测，针对的是数值型的样本，
   2. 在给定输入的时候, 回归算法预测出一个数值. 是对分类方法的提升
2. 一元线性回归分析
   1. 只含一个自变量和一个因变量, 用一条直线近似表示
3. 多元线性回归分析
   1. 两个或两个以上的自变量，且因变量和自变量之间是线性关系
4. 线性回归:
   1. 一条直线描述 关系, 叫线性关系
5. 回归的目的: 建立一个回归方程（函数） `预测目标值`
6. 回归的求解: 求回归方程的 `回归系数`

#### 4.1 回归算法之线性回归

1. 线性回归的定义

   1. 定义: 目标值预期是 输入变量 的线性组合
   2. 分类 与 数学表示
      1. 单变量线性回归
         1. 房子的价格: f(x) = w1\*x+w0
         2. 通用公式: $ h(θ)=θ_0+ θ_1 x $
      2. 多变量线性回归
         1. 瓜的好坏程度: f(x) = w0+0.2\*色泽+0.5\*根蒂+0.3\*敲声
         2. 通用公式 : $h(θ)=θ_0+θ_1x_1+θ_2x_2$
         3. 向量表示. (其中 权重 W 值,特征 X 值)
            1. $ θ=\begin{bmatrix}   θ_1 \\   θ_2 \\   θ_3 \\θ_4  \end{bmatrix}  $, $ X=\begin{bmatrix}   x_1 \\   x_2 \\   x_3 \\x_4  \end{bmatrix}  $
            2. 通用向量公式 --  线性模型
               1. $h(θ) = θ^T * X$   # 列向量的转置与特征的 乘积

2. 损失函数

   1. 误差
   2. 线性回归模型衡量标准 : 误差公式
      1. 模型与数据差的平方和
      2. $J(θ)=∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2$   #所有的误差和, 误差公式的通式
   3. 使误差和最小 的 方法: 两种方法
      1. 一种: 梯度下降算法
      2. 一种: 正规方程解法（适用于简单的线性回归）

3. 梯度下降算法

   1. 误差公式的通式 $J(θ)=∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2$ 展开 ,  取两个单个变量 求最小值，误差和 表示$cost(w_0+w_1x_1)=∑_{i=1}^{N}(w_0+w_1x_i−y_i)^2$
      1. 当 找到公式的最小值时，就能得到最好的$w_1,w_0$
      2. $cost(w_0+w_1x_1)$ 的图像 像一个山谷一样，有一个最低点, $w_1$的偏导数下降寻找最低点
         1. $w_1:=−w_1−α\frac{∂cost(w_0+w_1 x_1)}{∂w_1}$
         2. $w_0:=−w_0−α\frac{∂cost(w_0+w_1 x_1)}{∂w_1}$

4. LinearRegression

   1. sklearn.linear_model.LinearRegression

      1. 源码

         1. ```
            class LinearRegression(fit_intercept = True，normalize = False，copy_X = True，n_jobs = 1)
              """
              :param normalize: True时，数据进行标准化。请在使用normalize = False的估计器调时用fit之前使用preprocessing.StandardScaler
            
              :param copy_X:boolean，可选，默认为True，如果为True，则X将被复制
            
              :param n_jobs：int，可选，默认1。用于计算的CPU核数
              """
            ```

      2. 示例

         1. ```
            from sklearn.linear_model import LinearRegression
            reg = LinearRegression()
            ```

   2. 方法

      1. fit(X,y,sample_weight = None)
         1. X 训练数据拟合模型，y 为X的类别值。X，y为数组或者矩阵
         2. `reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])`
      2. predict(X)
         1. 预测提供的数据对应的结果
         2. `reg.predict([[3,3]])`   #输出 array([ 3.])

   3. 属性

      1. `coef_`
         1. 回归系数w=(w1,w2....)
         2. `reg.coef_`   #array([ 0.5,  0.5])
      2. `intercept_`
         1. 表示w0

5. 加入交叉验证

   1. sklearn的岭回归、lasso回归都提供了 CV方法

      1. linear_model.Lasso，交叉验证 `linear_model.LassoCV`
      2. linear_model.Ridge，交叉验证 `linear_model.RidgeCV`

   2. 示例

      1. ```
         from sklearn.datasets.samples_generator import make_regression
         from sklearn.model_selection import cross_val_score
         from sklearn import linear_model
         import matplotlib.pyplot as plt
         
         lr = linear_model.LinearRegression()  #线性回归模型
         X, y = make_regression(n_samples=200, n_features=5000, random_state=0) #特征X, 标签y,
         result = cross_val_score(lr, X, y)  #交叉验证
         result
         ```

#### 4.2 线性回归案例分析 - 波士顿房价预测

1. 美国波士顿地区房价数据描述

   1. ```
      from sklearn.datasets import load_boston
      
      boston = load_boston()
      
      boston.DESCR
      ```

2. 波士顿地区房价数据分割

   1. ```
      from sklearn.model_selection import train_test_split
      import numpy as np
      X = boston.data
      y = boston.target
      
      X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=33,test_size = 0.25)
      ```

3. 训练与测试数据标准化处理

   1. ```
      from sklearn.preprocessing import StandardScaler
      ss_X = StandardScaler()
      ss_y = StandardScaler()
      
      X_train = ss_X.fit_transform(X_train)
      X_test = ss_X.transform(X_test)
      y_train = ss_X.fit_transform(y_train)
      y_test = ss_X.transform(y_test)
      ```

4. 使用 最简单的线性回归模型`LinearRegression`  和 梯度下降估计 `SGDRegressor` 对房价进行预测

   1. ```
      from sklearn.linear_model import LinearRegression
      lr = LinearRegression()
      lr.fit(X_train,y_train)
      lr_y_predict = lr.predict(X_test)
      ```

   2. ```
      from sklearn.linear_model import SGDRegressor
      sgdr = SGDRegressor()
      sgdr.fit(X_train,y_train)
      sgdr_y_predict = sgdr.predict(X_test)
      ```

      1. SGDRegressor是 随机梯度下降线性回归 ，随机梯度下降是不将所有样本都进行计算后再更新参数，而是选取一个样本，计算后就更新参

   3. 比较

      1. | LinearRegression         | SGDRegressor                           |
         | ------------------------ | -------------------------------------- |
         | 使用 解析方法            | 使用随机梯度下降估计参数               |
         | 最为简单、易用的回归模型 | 训练数据规模十分庞大, 表现的十分高效   |
         |                          | 随机梯度下降（SGD）分为 分类器和回归器 |

5. 性能评测

   1. 均方误差(Mean Squared Error)MSE	

      1. MSE的计算方法
         1. $MSE=\frac{1}{m} ∑_{i=1}^{m}(y^i − \overline{y})^2$
            1. m 样本个数

   2. 使用MSE评价机制对两种模型的回归性能作出评价

      1. ```
         from sklearn.metrics import mean_squared_error
         
         '线性回归模型的均方误差为：',mean_squared_error(sy_test, lr_y_predict)
         
         '梯度下降模型的均方误差为：',mean_squared_error(y_test,sgdr_y_predict)
         ```

6. 完整代码

   1. ```
      from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge
      from sklearn.preprocessing import StandardScaler
      from sklearn.datasets import load_boston
      from sklearn.cross_validation import train_test_split
      from sklearn.metrics import mean_squared_error,classification_report
      from sklearn.cluster import KMeans
      
      
      def linearmodel():
          """
          线性回归对波士顿数据集处理
          :return: None
          """
      
          # 1、加载数据集
      
          ld = load_boston()
      
          x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)
      
          # 2、标准化处理
      
          # 特征值处理
          std_x = StandardScaler()
          x_train = std_x.fit_transform(x_train)
          x_test = std_x.transform(x_test)
      
      
          # 目标值 处理
      
          std_y  = StandardScaler()
          y_train = std_y.fit_transform(y_train)
          y_test = std_y.transform(y_test)
      
          # 3、估计器流程
      
          # LinearRegression
          lr = LinearRegression()
      
          lr.fit(x_train,y_train)
      
          # print(lr.coef_)
      
          y_lr_predict = lr.predict(x_test)
      
          y_lr_predict = std_y.inverse_transform(y_lr_predict)
      
          print("Lr预测值：",y_lr_predict)
      
      
          # SGDRegressor
          sgd = SGDRegressor()
      
          sgd.fit(x_train,y_train)
      
          # print(sgd.coef_)
      
          y_sgd_predict = sgd.predict(x_test)
      
          y_sgd_predict = std_y.inverse_transform(y_sgd_predict)
      
          print("SGD预测值：",y_sgd_predict)
      
          # 带有正则化的岭回归
      
          rd = Ridge(alpha=0.01)
      
          rd.fit(x_train,y_train)
      
          y_rd_predict = rd.predict(x_test)
      
          y_rd_predict = std_y.inverse_transform(y_rd_predict)
      
          print(rd.coef_)
      
          # 两种模型评估结果
      
          print("lr的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))
      
          print("SGD的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))
      
          print("Ridge的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))
      
          return None
      ```

#### 4.3 回归性能评估与欠拟合、过拟合

1. 欠拟合与过拟合

   1. 泛化: 模型对没有遇见过的样本的表现

      1. 泛化的好坏评价：过拟合和欠拟合

   2. 过拟合:  模型在新的数据上表现很差

   3. 欠拟合:模型过于简单

      

2. 解决过拟合的方法

   1. 线性回归中,特征集过小,  欠拟合（underfitting）. 特征集过大,  过拟合（overfitting）
   2. 欠拟合
      1. 模型在训练和预测时, 表现都不好
      2.  欠拟合通常不 讨论
   3. 过拟合
      1. 特征集合数目过多 , 造成过拟合
      2. 解法方法
         1. 减少特征数
            1. 这是权宜之计
            2. 放弃特征 等同于丢弃信息
         2. 正则化
            1. 示例 讲解
               1. 房价: $θ_0+ θ_1 x + θ_2 x^2+ θ_3 x^3+ θ_4 x^4$
               2. 消除$x_3$和$x_4$的影响，也就是想让$\theta_3{,}\theta_4$都等于0
               3. 简单的方法对 $\theta_3{,}\theta_4$进行惩罚，增加 很大的系数

#### 4.4 回归算法 之 岭回归(算法模型)

1. 岭回归

   1. 即`L2`正则化的 线性最小二乘法
      1. 专用于 `线性数据分析` 的有偏估计回归方法，实质是改良的 `最小二乘估计法`，放弃最小二乘法的无偏性
      2. 以 损失部分信息、降低精度为 代价, 获得回归系数, 更为符合实际、更可靠的回归方法

2. sklearn.linear_model.Ridge

   1. 源码

      1. ```
         class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)**
           """
           :param alpha:float类型，正规化的程度
           """
         ```

   2. 示例

      1. ```
         from sklearn.linear_model import Ridge
         clf = Ridge(alpha=1.0)
         clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
         ```

   3. 方法

      1. score(X, y, sample_weight=None)
         1. clf.score()

   4. 属性

      1. coef_
         1. `clf.coef_`   # array([ 0.34545455,  0.34545455])
      2. intercept_
         1. `clf.intercept_`   # 0.18571428571428572

#### 4.5 岭回归案例分析

1. ```
   def linearmodel():
       """
       线性回归对波士顿数据集处理
       :return: None
       """
   
       # 1、加载数据集
   
       ld = load_boston()
   
       x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)
   
       # 2、标准化处理
   
       # 特征值处理
       std_x = StandardScaler()
       x_train = std_x.fit_transform(x_train)
       x_test = std_x.transform(x_test)
   
   
       # 目标值进行处理
   
       std_y  = StandardScaler()
       y_train = std_y.fit_transform(y_train)
       y_test = std_y.fit_transform(y_test)
   
       # 3、估计器流程
   
       # LinearRegression
       lr = LinearRegression()
   
       lr.fit(x_train,y_train)
   
       # print(lr.coef_)
   
       y_lr_predict = lr.predict(x_test)
   
       y_lr_predict = std_y.inverse_transform(y_lr_predict)
   
       print("Lr预测值：",y_lr_predict)
   
   
       # SGDRegressor
       sgd = SGDRegressor()
   
       sgd.fit(x_train,y_train)
   
       # print(sgd.coef_)
   
       y_sgd_predict = sgd.predict(x_test)
   
       y_sgd_predict = std_y.inverse_transform(y_sgd_predict)
   
       print("SGD预测值：",y_sgd_predict)
   
       # 带有正则化的岭回归
   
       rd = Ridge(alpha=0.01)
   
       rd.fit(x_train,y_train)
   
       y_rd_predict = rd.predict(x_test)
   
       y_rd_predict = std_y.inverse_transform(y_rd_predict)
   
       print(rd.coef_)
   
       # 两种模型评估结果
   
       print("lr的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))
   
       print("SGD的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))
   
       print("Ridge的均方误差为：",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))
   
       return None
   ```

### 5 非监督学习

1. 不受监督的学习，一种自由的学习

   1. 不需要先验知识，而是不断地自我认知，自我巩固，最后自我归纳

2. 对比

   1. | 有监督学习                                         | 无监督学习                                            |
      | -------------------------------------------------- | ----------------------------------------------------- |
      | 训练集：(x(1),y(1)),(x(2),y2)(x(1),y(1)),(x(2),y2) | 训练集:  (x(1)),(x(2)),(x(3))(x(1)),(x(2)),(x(3))     |
      | 有监督学习, 样本分类称为分类（Classification）     | 无监督学习，样本划分到不同集合 称为聚类（Clustering） |

      

#### 5.1 非监督学习之k-means

1. k-means 

   1. 4个阶段
      1. 首先，随机设 `K`个特征空间内的点 为 初始化的聚类中心
      2. 然后，对每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，标记为聚类中心
      3. 接着， 所有的数据 被标记聚类中心之后，计算新分配的类簇的平均值,  作为K个聚类的新中心
      4. 最后 , 计算旧和新中心之间的差异, 如果没有变化，迭代停止，否则回到步骤2继续循环
   2. K均值 等于 具有小的全对称协方差矩阵的期望最大化算法

2. sklearn.cluster.KMeans

   1. 源码

      1. ```
         class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')
           """
           :param n_clusters:要形成的聚类数以及生成的质心数
         
           :param init:初始化方法，默认为'k-means ++',以智能方式选择k-均值聚类的初始聚类中心，以加速收敛;random,从初始质心数据中随机选择k个观察值（行
         
           :param n_init：int，默认值：10使用不同质心种子运行k-means算法的时间。最终结果将是n_init连续运行在惯性方面的最佳输出。
         
           :param n_jobs：int用于计算的作业数量。这可以通过并行计算每个运行的n_init。如果-1使用所有CPU。如果给出1，则不使用任何并行计算代码，这对调试很有用。对于-1以下的n_jobs，使用（n_cpus + 1 + n_jobs）。因此，对于n_jobs = -2，所有CPU都使用一个。
         
           :param random_state:随机数种子，默认为全局numpy随机数生成器
           """
         ```

   2. 示例

      1. ```
         from sklearn.cluster import KMeans
         import numpy as np
         X = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])
         kmeans = KMeans(n_clusters=2, random_state=0)
         ```

   3. 方法

      1. fit(X,y=None), 用 `X` 作训练数据 拟合 模型

         1. `kmeans.fit(X)` 

            1. 输出:

               ```
               KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
                   n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',
                   random_state=0, tol=0.0001, verbose=0)
               ```

               

      2. predict(X),  预测新的`数据` 的类别

         1. `kmeans.predict([[0, 0], [4, 4]])`   #array([0, 1], dtype=int32)

   4. 属性

      1. cluster_centers_  , 集群中心的点坐标

         1. `kmeans.cluster_centers_`   #array([[ 1.,  2.], [ 4.,  2.]]),
         2. 必须经过 fit_transform()之后才有这个属性

      2. labels_  , 每个点的类别

         1. `kmeans.labels_`  #array([0, 0, 0, 1, 1, 1])

         

#### 5.2 k-means案例分析

1. 手写数字 K-Means聚类的演示

   1. ```
      from sklearn.metrics import silhouette_score
      from sklearn.cluster import KMeans
      from sklearn import datasets
      
      def kmeans():
          """
          手写数字聚类过程
          :return: None
          """
          # 加载数据
      
          ld = datasets.load_digits()  #每一行是一个数字
      
          print(ld.target[:20])
      
      
          # 聚类
          km = KMeans(n_clusters=810)
      
          km.fit_transform(ld.data)
      
          print(km.labels_[:20])
      
          print(silhouette_score(ld.data,km.labels_))
      
          return None
      
      
      
      if __name__=="__main__":
          kmeans()
      ```

### 6 推荐系统

1. 什么是推荐系统
   1. 购买商品明确时, 用不到推荐系统
   2. 要看的电影不明确时, 用到推荐系统
   3. `信息过载`, 用推荐系统
      1. 推荐系统的基本任务 联系用户和物品，解决信息过载
2. 个性化推荐系统的应用
   1. 推荐系统 领域: 电子商务、电影和食品、音乐、社交网络、阅读、基于位置的服务、个性化邮件和广告等

#### 6.1 推荐系统评测

1. 推荐系统的实验方法
   1. 获取推荐系统的指标的主要实验方法:
      1. 离线实验
      2. 用户调查
      3. 在线实验
   2. 离线实验 步骤
      1. 日志系统获取用户行为数据集, 并格式化
      2. 将数据集 分成训练集和测试集 
      3. 训练集 训练用户模型， 测试集 预测
      4. 用 `离线标准评测算法` 在测试机上的预测结果
         1. 缺点是无法获取商业上的指标
         2. 并且离线实验和商业指标 存在差距
   3. 用户调查
      1. 真实的用户在推荐系统上完成任务，根据答案和行为了解系统的性能。
   4. 在线实验
      1. AB测试: 将它和旧的算法 比较
         1. 公平获得不同算法实际在线时的 `性能指标`
         2. 缺点周期长
   
2. 评测指标

   1. 用户满意度

      1. 用户满意度主要是 调查问卷的形式
      2. 问卷的问题特点:
         1. 从不同的侧面询问用户对结果的感受

   2. 预测准确度

      1. 推荐系统离线评测指标

      2. 离线推荐算法 两个不同的方向: 

         1. 评分预测

            1. 用户给物品打分, 从中习得用户的兴趣模型
            2. 预测准确度 用 `均方根误差`（RMSE）和 `平均绝对误差`（MAE）计算

            3. 均方根误差

               1. ```
                  def RMSE(records):  
                      return math.sqrt(sum([(rui-pui)*(rui-pui) for u,i,rui,pui in records])/float(len(records)))
                  ```

            4. 平均绝对误差(MAE)

               1. ```
                  def MAE(records):  
                      return sum([abs(rui-pui) for u,i,rui,pui in records])/ float(len(records))
                  ```

         2. TopN推荐

            1. R(u) : 根据用户在训练集上的行为给用户作出的`推荐列表`， T(u)是用户在测试集上的行为列表。

            2. 推荐结果的准确率定义

               1. ```
                  def PrecisionRecall(test, N):  
                      hit = 0
                      n_recall = 0
                      n_precision = 0
                      for user, items in test.items():  
                          rank = Recommend(user, N)  
                          hit += len(rank & items)  
                          n_recall += len(items)  
                          n_precision += N  
                      return [hit / (1.0 * n_recall), hit / (1.0 * n_precision)]
                  ```

         3. 评分预测和TopN推荐的讨论

            1. TopN推荐
               1.  给用户的个性化的推荐列表，叫做TopN推荐
               2. TopN推荐的预测准确率 的 度量 : 准确率（precision）/召回率（recall）
            2. 电影推荐的目的 找到用户最可能感兴趣的 `电影`，而不是预测用户看了电影后的`评分`. 因此, TopN推荐更符合实际的应用需求

#### 6.2 基于协同过滤的推荐系统

1. 协同过滤算法

   1. 是 基于`用户行为`的推荐算法
   2. 是 个性化推荐系统的重要算法

2. 用户行为数据

   1. 用户行为数据日志 
      1. 用户行为数据 存在形式 是日志(原始rizh)
      2. 原始日志， 存储在文件系统, 汇总成 `会话日志`
      3. 会话日志 存储在 `分布式数据仓库`, 如 支持离线分析的 `Hadoop Hive`
      4. 总结: 用户数据 --> 原始日志 --> 会话日志 --> 分布式数据仓库(Hadoop Hive)
   2. 在个性化推荐系统中 用户行为分 两种 : `显性反馈行为` ,  `隐性反馈行为`
      1. 显性反馈行为: 明确反应 用户 对物品喜好的行为
      2. 隐性反馈行为:不能明确反应用户喜好的行为 
   3. 使用较多的数据集
      1. 无上下文信息的显性反馈数据集, 
         1. 每一条记录 含用户ID、物品ID, 用户对物品的评分

3. 用户行为分析

   1. 基于用户行为数据 的推荐算法  是 `协同过滤算法` :
      1. 协同过滤算法 有很多方法: 如 基于 `领域` 的方法、隐语义模型
   2. 基于 `领域` 的方法 有 两种
      1. 基于物品的协同过滤算法, 推荐和他之前喜欢的 物品 相似的物品
      2. 基于用户的协同过滤算法, 推荐和他兴趣相似的 其他用户 喜欢的物品

4. 基于物品的协同过滤算法

   1. 基于物品的协同过滤算法

      1. 介绍
         1. 业界应用 最多 的算法
         2. 亚马逊网, Netflix, YouTube
      2. 步骤
         1. 计算物品之间的 `相似度`
         2. 计算被推荐物品的 `兴趣度`
         3. 根据物品的 `相似度` 和用户的 `历史行为` 生成 `推荐列表`

   2. 计算物品相似度

      1. 余弦相似度

         1. 公式: 
            1. 共同喜欢i和j的人数／sqrt(喜欢i的人数*喜欢j的人数)
            2. N(i)是喜欢物品i的用户数，N(j)是喜欢物品j的用户数
         2. 计算相似度，需要构建 基于物品的矩阵

      2. 示例

         1. 用户A,B,C,D，电影m1,m2,m3,m4,m5, 用户看过这些电影

            1. A:m1,m3,m5
            2. B:m2,m4
            3. C:m2,m3,m4
            4. D:m4,m5

         2. 用户A

            1. <table>
                   <tr><td>A</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td></td><td>1</td><td></td><td>1</td></tr>
                   <tr><td>m2</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m3</td><td>1</td><td></td><td></td><td></td><td>1</td></tr>
                   <tr><td>m4</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m5</td><td>1</td><td></td><td>1</td><td></td><td></td></tr>
               </table>

         3. 用户B

            1. <table>
                   <tr><td>B</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td></td><td></td><td>1</td><td></td></tr>
                   <tr><td>m2</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m3</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m4</td><td></td><td>1</td><td></td><td></td><td></td></tr>
                   <tr><td>m5</td><td></td><td></td><td></td><td></td><td></td></tr>
               </table>

         4. 用户C

            1. <table>
                   <tr><td>C</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m2</td><td></td><td></td><td>1</td><td>1</td><td></td></tr>
                   <tr><td>m3</td><td></td><td>1</td><td></td><td>1</td><td></td></tr>
                   <tr><td>m4</td><td></td><td>1</td><td>1</td><td></td><td></td></tr>
                   <tr><td>m5</td><td></td><td></td><td></td><td></td><td></td></tr>
               </table>

         5. 用户D

            1. <table>
                   <tr><td>D</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m2</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m3</td><td></td><td></td><td></td><td></td><td></td></tr>
                   <tr><td>m4</td><td></td><td></td><td></td><td></td><td>1</td></tr>
                   <tr><td>m5</td><td></td><td></td><td></td><td>1</td><td></td></tr>
               </table>

         6. 矩阵合并得到一个公共矩阵

            1. <table>
                   <tr><td>D</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td></td><td>1</td><td></td><td>1</td></tr>
                   <tr><td>m2</td><td></td><td></td><td>1</td><td>2</td><td></td></tr>
                   <tr><td>m3</td><td>1</td><td>1</td><td></td><td>1</td><td>1</td></tr>
                   <tr><td>m4</td><td></td><td>2</td><td>1</td><td></td><td>1</td></tr>
                   <tr><td>m5</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr>
               </table>

            2. 矩阵的里面的数字是 公共人数

         7. 计算相似度，利用公式,其中看过各个电影的人数为m1:1, m2:2, m3:2, m4:3, m5:2 人。得到相似度矩阵

            1. <table>
                   <tr><td>D</td><td>m1</td><td>m2</td><td>m3</td><td>m4</td><td>m5</td></tr>
                   <tr><td>m1</td><td></td><td>0</td><td>0.71</td><td>0</td><td>0.71</td></tr>
                   <tr><td>m2</td><td>0</td><td></td><td>0.50</td><td>0.82</td><td>0</td></tr>
                   <tr><td>m3</td><td>0.71</td><td>0.50</td><td></td><td>0.41</td><td>0.50</td></tr>
                   <tr><td>m4</td><td>0</td><td>0.82</td><td>0.41</td><td></td><td>0.41</td></tr>
                   <tr><td>m5</td><td>0.71</td><td>0</td><td>0.50</td><td>0.41</td><td></td></tr>
               </table>

   3. 计算被推荐物品的兴趣度

      1. 以A用户为例子，假设评分为：m1:2,m3:4,m5:3

         1. <table>
                <tr><td></td><td>评分</td></tr>
                <tr><td>m1</td><td>2</td></tr>
                <tr><td>m2</td><td>0</td></tr>
                <tr><td>m3</td><td>4</td></tr>
                <tr><td>m4</td><td>0</td></tr>
                <tr><td>m5</td><td>3</td></tr>
            </table>

         2. 被推荐物品的 `兴趣度` 公式

            1. p(ij) = W(ij) * r(i)
               1. W(ij) 推荐的与i相近的 `j` 物品
               2. r(i)为用户对物品 `i` 的评分

         3. 推荐电影当然要去除已经看过的，用户A对电影m1,m3,m5每个相似度中只有m2,m4没有看过，所以计算m2,m4的兴趣度

            1. ```
               m3-m2: 评分 * 相似度 = 4 * 0.50 = 2.00, #根据m3推荐m2
               m3-m4: 评分 * 相似度 = 4 * 0.41 = 1.64, #根据m3推荐m4
               m5-m4: 评分 * 相似度 = 3 * 0.41 = 1.23, #根据m5推荐m4
               ```

            2. 通过比较, 我们得出兴趣度最高的是m2

#### 6.3 代码案例 - 推荐算法

1. ```
   import recsys.algorithm
   recsys.algorithm.VERBOSE = True
   
   
   from recsys.algorithm.factorize import SVD
   from recsys.datamodel.data import Data
   from recsys.evaluation.prediction import RMSE
   import os,sys
   
   tmpfile = "/tmp/movielens.zip"
   moviefile = "./ml-1m/movies.dat"
   
   
   class RecommendSystem(object):
   
       def __init__(self, filename, sep, **format):
           self.filename = filename
           self.sep = sep
           self.format = format
   
           # 训练参数
           self.k = 100
           self.min_values = 10
           self.post_normalize = True
   
           self.svd = SVD()
   
           # 判断是否加载
           self.is_load = False
   
           # 添加数据处理
           self.data = Data()
   
           # 添加模型评估
           self.rmse = RMSE()
   
       def get_data(self):
           """
           获取数据
           :return: None
           """
           # 如果模型不存在
           if not os.path.exists(tmpfile):
               # 如果数据文件不存在
               if not os.path.exists(self.filename):
                   sys.exit()
               # self.svd.load_data(filename=self.filename, sep=self.sep, format=self.format)
               # 使用Data()来获取数据
               self.data.load(self.filename, sep=self.sep, format=self.format)
               train, test = self.data.split_train_test(percent=80)
               return train, test
           else:
               self.svd.load_model(tmpfile)
               self.is_load = True
               return None, None
   
   
       def train(self, train):
           """
           训练模型
           :param train: 训练数据
           :return: None
           """
           if not self.is_load:
               self.svd.set_data(train)
               self.svd.compute(k=self.k, min_values=self.min_values, post_normalize=self.post_normalize, savefile=tmpfile[:-4])
           return None
   
       def rs_predict(self, itemid, userid):
           """
           评分预测
           :param itemid: 电影id
           :param userid: 用户id
           :return: None
           """
           score = self.svd.predict(itemid, userid)
           print "推荐的分数为：%f" % score
           return score
   
       def recommend_to_user(self, userid):
           """
           推荐给用户
           :param userid: 用户id
           :return: None
           """
           recommend_list = self.svd.recommend(userid, is_row=False)
   
           # 读取文件里的电影名称
           movie_list = []
   
           for line in open(moviefile, "r"):
               movie_list.append(' '.join(line.split("::")[1:2]))
   
           # 推荐具体电影名字和分数
           for itemid, rate in recommend_list:
               print "给您推荐了%s,我们预测分数为%s" %(movie_list[itemid],rate)
           return None
   
       def evaluation(self, test):
           """
           模型的评估
           :param test: 测试集
           :return: None
           """
           # 如果模型不是直接加载
           if not self.is_load:
   
               # 循环取出测试集里面的元组数据<评分，电影，用户>
               for value, itemid, userid in test.get():
                   try:
                       predict = self.rs_predict(itemid, userid)
                       self.rmse.add(value, predict)
                   except KeyError:
                       continue
               # 计算返回误差（均方误差）
               error = self.rmse.compute()
   
               print "模型误差为%s:" % error
   
           return None
   
   
   if __name__ == "__main__":
       rs = RecommendSystem("./ml-1m/ratings.dat", "::", row=1, col=0, value=2, ids=int)
       train, test = rs.get_data()
       rs.train(train)
       rs.evaluation(test)
       # rs.rs_predict(1,1)
       rs.recommend_to_user(1)
   ```











































