#### 总结:

1. requests.request()返回的response对象 与 urllib.request.urlopen() 返回的response的属性不同

   1. requests.request()返回的response有丰富的属性
      1. urllib.request.urlopen() 返回的response用 `.read()`方法处理
   2. urllib.request.urlopen()的返回值response的属性方法:
      1. response是一个HTTPResposne类型的对象
      2. 方法有read()、readinto()、getheader(name)、getheaders()、fileno()等函数
      3. msg、version、status、reason、debuglevel、closed等属性

2. url编码. url编码解码是针对含义汉字的url的编码, 没有汉字时只是把`:`编程`=`

   1. requests的url编码解码: requests.utils.quote(),  requests.utils.unquote().  requests.get()或requests.post() 基本上没有url编码/解码的问题
   2. urllib方式: urllib.parse.urlencode()把字典转成`=`连接的字符串, 并对其中的汉字16进制码
   3. url编码,  不是二进制编码. 
   4. 已经url编码的url再被url编码后传入 urllib.request.Request()封装后不能访问想要的网站

3. requests与urllib的post传输请求数据处理方法不同

   1. requests.post()或get(), 用参数 url, params, headers把实参传入, 不需要手动二进制编码, 自动会编码
   2. urllib.parse.urlopen() 传输的数据先经urllib.parse.urlencode() 实现url编码, 再`.encode()`编码成二进制格式, 最后传入urllib.parse.Request()的参数data 并封装. 即, urllib.request.Request()对url, data, headers封装, 它没有 encode()方法
   3. 小结: `urllib.request.Request()`是数据`urllib.request.urlopen()` 加 `.encode()`的封装

   

4. requests与urllib处理 SSL证书的方法不同

   1. requests方式 : response = requests.get("url ", verify= False)

   2. urllib方式: 用ssl模块处理

      1. ```
         # 1. 导入Python SSL处理模块
         import ssl
         
         # 2.忽略未经核实的SSL证书认证
         context = ssl._create_unverified_context()
         request = urllib.request.Request(url, headers = headers)
         
         # 3. 在urlopen()方法里 添加 context 参数
         response = urllib.request.urlopen(request, context = context)
         ```


5. python3与python2 有很多不同

   1. python3中

      1. ```
         import urllib.parse
         import urllib.request
         
         word = urllib.parse.urlencode(word)
         request = urllib.request.Request(newurl, headers=headers)
         response = urllib.request.urlopen(request)
         ```

   2. python2中

6. import urllib 与 import urllib.parse, import urllib.request不同, import urllib不能把子包导入

7. 源代码与element中内容区别

   1. 源代码。是request get到的内容. 网页右键， 查看
   2. element内容是浏览器js渲染过的
      1. 服务器端响应浏览器的请求，返回文件和内容以条的形式存放在network中. `Network`面板记录各个请求资源信息（包括状态、资源类型、大小、所用时间、Request和Response等）详情信息
      2. element 是当前页面的组织结构,  当前页面所有东西,  解析 HTML 建立 DOM 树的展现
   3. network 显示了服务器给浏览器的文件，浏览器组装这些文件得到 elements

8. 爬虫如何寻找要爬取的内容:

   1. element 有js动态加载的， 推荐用network里，用ctrl加f搜索你要找的内容，随便几个关键词，在网页源代码中还是js中还是josn中的都可以找到，
   2. 如果还找不到建议用selenium。

9. ```
   import requests  #http请求与响应模块
   import lxml  #html/xml解析模块
   ```

10. 

#### 问题:

1. AttributeError:'Nonetype' object has no attribute 'get'
   1. https://blog.csdn.net/weixin_44052055/article/details/108632006?ops_request_misc=&request_id=&biz_id=102&utm_term=AttributeError:%27Nonetype%27%20obje&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-108632006.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187
   2. 原因就是**该网站被爬太多次，我们被该网页服务器的反爬虫程序发现了，并禁止我们爬取。因此我们需要模拟浏览器，重新给服务器发送请求，并且添加头等信息headers，headers是解决requests请求反爬的方法之一，相当于我们进去这个网页的服务器本身，假装自己本身在爬取数据**



https://www.w3school.com.cn/xpath/xpath_nodes.asp



### 18 通用爬虫模块使用

IDE和编辑器:

1. IDE：`PyCharm`、`Spyder`、`Visual Studio`
2. 编辑器：`Vim`、`emacs`、`Atom`

### 1 爬虫原理与数据抓取

1. 问题
   1. python3 不会自动导入urllib模块下的request模块
      1. module 'urllib' has no attribute 'request'
      2. 方案 `import urllib.request`

爬虫:  模拟浏览器发送请求，获取响应

#### 1.1 (了解)通用爬虫和聚焦爬虫

1. 爬虫的概念

   1. **通用爬虫** 和 **聚焦爬虫** 两种
   2. 通用爬虫 :
      1.  捜索引擎抓取系统（Baidu、Google、Yahoo等）的 组成部分
      2. 网页下载到本地，形成互联网内容的镜像备份

2. 爬虫的流程

   1. 通用爬虫流程(搜索引擎网络爬虫)

      1. 第一步：抓取网页

         1. 搜索引擎 获取 网站的URL
            1. 选取种子URL，将这些URL放入待抓取URL队列
            2. 取待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中， URL放入已抓取URL队列
            3. 分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环
         2. Robots协议（也叫爬虫协议、机器人协议等）
            1. “网络爬虫排除标准”（Robots Exclusion Protocol）
            2. 网站通过Robots协议告诉 搜索引擎 哪些页面可以抓取，哪些页面不能抓取

      2. 第二步：数据存储

      3. 第三步：预处理

         1. 抓取 的页面，预处理

      4. 第四步：提供检索服务，网站排名

         1. PageRank值（链接访问量的排名）网站排名
      2. Money 购买 搜索引擎 网站排名
      
         

   2. 聚焦爬虫:  爬虫工程师的主要工作对象

   3. 小结: 

      1. 爬虫过程
         1. 获取url--->发送请求，获取响应--->提取数据 ---> 保存
         2. 发送请求，获取响应--->提取url
      2. 爬虫 以当前url地址对应的 `响应`为准, 当前url地址的elements的内容和url的响应不一样

#### 1.2 (复习)HTTP/HTTPS的请求与响应

 1. HTTP和HTTPS

      1. `HTTP协议`  HyperText Transfer Protocol，超文本传输协议
      2. `HTTPS`（Hypertext Transfer Protocol over Secure Socket Layer）
          1. HTTP的安全版
          2. 在HTTP下加入SSL层
              1. `SSL`（Secure Sockets Layer 安全套接层）
                  1. 用于Web的安全传输协议
                  2. 在传输层对网络连接 加密，保障 Internet数据传输安全。
      3. 端口
          1. `HTTP`的端口号为`80`   
          2. `HTTPS`的端口号为`443`

 2. HTTP的请求与响应

      1. HTTP通信 两部分 ： **客户端 请求消息** 与 **服务器 响应消息**
       2. 浏览器发送HTTP请求的过程
              	1. 浏览器 地址栏 输入 URL并 回车 
                      	1. 浏览器 向HTTP服务器发送HTTP请求(Request请求), HTTP请求 分为“Get”和“Post”两种方法
                      	2. 服务器把 Response文件对象 发送回给浏览器
                           	3. 浏览器分析Response中的 HTML, 发现 引用 很多其他文件, 如Images文件，CSS文件，JS文件。浏览器 自动再 发送Request 获取图片，CSS文件，或者JS文件. 这些文件 下载完，网页 根据HTML语法结构， 显示
               3. URL
                   	1. Uniform / Universal Resource Locator的缩写,  统一资源定位符
                    	2. Internet 上网页和其他资源的 地址标识方法
                    	3. 基本格式: `scheme://host[:port#]/path/…/[?query-string][#anchor]`
                    	1. scheme：协议(例如：http, https, ftp)
                      	     	2. host：服务器 的IP地址或 域名
                                	3. port#：服务器的端口（ 协议默认端口，缺省80）
                                	4. path： 资源的路径
                                	5. query-string：参数，给http服务器的数据
                                	6. anchor：锚（跳转到网页的指定位置）

 3. 客户端HTTP请求

     1. HTTP请求 的组成: `请求行`、`请求头部`、`空行`、`请求数据`

         1. 请求行: `方法 空格 URL 空格 协议/版本 回车符换行符`  

         1. 请求头部:

             1. `字段名1 : 值 回车符 换行符' `   

                ................

                `字段名n : 值 回车符 换行符`  #请求头部n

         3. 空行, 必须有: `回车符 换行符`           
         4. 请求数据 : ` Aaaaaaaaaaa............BBBBBB`  
         
         1. HTTP请求示例
         
            1. ```
               GET https://www.baidu.com/ HTTP/1.1
               Host: www.baidu.com
               Connection: keep-alive
               
               Aaaaaaaaaaa............BBBBBB
               ```

 4. 请求方法

     1. | 序号 | 方法    | 描述                                                         |
        | ---- | ------- | ------------------------------------------------------------ |
        | 1    | GET     | 请求指定的页面信息，返回实体。                               |
        | 2    | HEAD    | 类似get请求，响应中没有具体的内容，用于获取报头              |
        | 3    | POST    | 向指定资源提交数据（如提交表单或者上传文件），数据在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 |
        | 4    | PUT     | 从客户端向服务器传送的数据取代指定的文档的内容。             |
        | 5    | DELETE  | 请求服务器 删除 指定的 页面 。                               |
        | 6    | CONNECT | HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。     |
        | 7    | OPTIONS | 允许客户端查看服务器的性能。                                 |
        | 8    | TRACE   | 回显服务器收到的请求，主要用于测试或诊断。                   |

 5. HTTP请求主要分为`get`和`post`两种方法

     	1. get 从服务器 获取数据，post 向服务器 传送数据

 6. 常用 请求 头(报头)

     1. Host (主机和端口号), 指定被请求资源的Internet主机和端口号

     2. Connection (链接类型)

     3. Upgrade-Insecure-Requests (升级为HTTPS请求)

     4. User-Agent (浏览器名称) : `User-Agent： 客户浏览器的名称 `

     5. Accept (可接受的传输文件类型)

         1. Accept： 

             1. 浏览器接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型
             2. 服务器 根据它 返回适当的文件格式
     
         2. 举例：
     
             1. `Accept: */*`： 什么都可以接收
             2. `Accept：image/gif`： 客户端希望接受GIF图像格式的资源；
             3. `Accept：text/html`： 客户端希望接受html文本。
             4. `Accept: text/html, application/xhtml+xml; q=0.9, image/*;q=0.8`：
             
                 1.  浏览器支持的 MIME 类型 : html文本、xhtml和xml文档、所有的图像格式资源
                 2. q是权重系数, 范围 0 =< q <= 1, q 值越大，倾向于获得其“;”之前的类型表示的内容\*
     
                
         
     6. Referer (页面跳转处)
     
         1. 请求的网页来自哪个URL，用户从 Referer页面跳转到请求的页面。跟踪Web请求来自哪个页面，是从什么网站来的等
         3. 下载某网站图片，需要对应的referer, 否则无法下载图片，人家做了防盗链
          	2. 原理 : 根据 referer 判断是否是本网站的地址
         
     7.  Accept-Encoding（文件编解码格式）
     
         1. 览器可 接受的编码方式.  举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0
         3. 请求 没有设置,  客户端对各种内容编码都 接受
         
     8. Accept-Language（语言种类）
     
     9. Accept-Charset（字符编码）
     
         1. 浏览器可 接受的字符编码, 举例：Accept-Charset:iso-8859-1,gb2312,utf-8
         
     10. Cookie （Cookie）
     
         1. Cookie 浏览器 寄存在客户端的 小型数据体
         2. 记载和服务器相关的用户信息, 实现会话功能
     
     11. Content-Type (POST数据类型)
     
         1. POST请求 的内容类型
         1. 举例：`Content-Type = Text/XML; charset=gb2312`, 请求的消息体 是纯文本的XML类型的数据, 字符编码 “gb2312”
     
 7. 服务端HTTP响应

     1. HTTP响应 四个部分： `状态行`、`消息报头`、`空行`、`响应正文`

         1. 示例

            1. ```
               HTTP/1.1 200 OK
               Server: Tengine
               Connection: keep-alive
               ......
               Content-Length: 180945
               #空行
               <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" ....
               ```

            

     2. 常用的响应头(报头)(了解)

         1. ```
            1. Cache-Control：must-revalidate, no-cache, private。
             	1. 值告诉客户端，服务端不希望客户端缓存资源
             	2. 下次请求资源时，必须要从新请求服务器，不能从缓存副本中获取资源
            2.  Connection：keep-alive
             	1. 回应客户端的Connection：keep-alive
             	2. 告诉客户 端服务器的tcp连接 也是一个长连接
            3. Content-Encoding:gzip
             	1. 服务端发送的资源是采用gzip编码的
            4. Content-Type：text/html;charset=UTF-8
             	1. 告诉客户端，资源文件的类型，还有字符编码
             	2. 客户端通过utf-8对资源 解码，对资源进行html解析
            5.  Date：Sun, 21 Sep 2016 06:18:21 GMT
             	1. 服务端发送资源时的服务器时间
             	2. GMT 格林尼治 标准时间
            6. Expires:Sun, 1 Jan 2000 01:00:00 GMT
             	1. 响应头也是跟缓存有关的
             	2. 告诉客户端 这个时间前，可直接访问缓存副本
            7. Pragma:no-cache
             	1. 与Cache-Control等同
            8. Server：Tengine/1.4.6
             	1. 服务器和 对应的版本
            9. Transfer-Encoding：chunked
             	1. 告诉客户端，服务器发送的资源的方式是分块发送的
            10. Vary: Accept-Encoding
              	1. 告诉缓存服务器，缓存压缩文件和非压缩文件两个版本
            11. Cookie 和 Session：
              	1. 为了维护 服务器和客户端  间的链接
              	2. Cookie： 在 客户端 记录的信息确定 用户 的身份。
              	3. Session： 在 服务器端 记录的信息确定 用户 的身份
            ```

     3. 响应状态码

         1. ```
            1. 三位数字
             	1. 第一个数字 响应的类别, 从1到5
                  	1. 100~199, 
                       	1. 服务器接收部分请求,要客户端继续提交其他请求信息
                  	2. 200~299
                       	1. 服务器成功接收请求并已完成整个处理过程, 常用200（OK 请求成功）
                  	3. 300~399
                       	1. 为完成请求，客户需进一步细化请求
                       	2. 如：请求的资源 移动一个新地址
                  	4. 400~499
                       	1. 客户端的请求有错误
                       	2. 404（服务器无法找到被请求的 页面）
                       	3. 403（服务器 拒绝访问，权限不够）
                  	5. 500~599
                       	1. 服务器端出现错误
                       	2. 常用500（请求未完成。服务器遇到不可预知的情况）
            2. 常见状态码：
             	1. 1xx:信息
             	2. 2xx:成功
             	3. 3xx:重定向
             	4. 4xx:客户端错误
             	5. 5xx:服务器错误
            3. HTTP响应状态码参考：
            ```
     
 8. 小结:

      1. 页面上的数据的位置:
          	1. 当前url地址对应的响应中
           	2. 其他的url地址对应的响应中
                	1. ajax请求中
                	2. js生成的,   部分数据在响应中全部通过js生成

#### 1.3 str和bytes的区别

 1. bytes类

     1. `二进制字节序列`, 用于记录 对象，对象是什么（ 如:什么字符）, 由编码格式决定

     2. bytes是Python3中特有的，Python2 里不区分bytes和str

         1. Python2

             1. `type(b'xxxxx')`   #<type 'str'>
                
                2. `type('xxxxx')`  #<type 'str'>

         2. Python3

             1. `type(b'xxxxx')`  #<class 'bytes'>
             2. `type('xxxxx')`  #<class 'str'>

         3. python3中：

             1. str 转化为 bytes,   encode 默认用 bytes 编码.

                 1. ```
                     str1='人生苦短，我用Python!'
                     type(str1)   #str
                     type(str1.encode())  #默认是bytes类型
                     b=str1.encode()  
                     #
                     b'\xe4\xba\xba\xe7\x94\x9f\xe8\x8b\xa6\xe7\x9f\xad\
                     xef\xbc\x8c\xe6\x88\x91\xe7\x94\xa8Python!'
                     ```

             2. bytes转str,  用 decode(), 默认 bytes格式解码

                 1. ```
                     b  # b'\xe4\xba\xba\xe7\x94\x9f\xe8\x8b\xa6\xe7\x9f\xad\xef\xbc\x8c\xe6\x88\x91\xe7\x94\xa8Python!'
                     type(b)  #bytes
                     b.decode()  #人生苦短，我用Python!'
                     ```

         4. Python 2 不区分str和bytes, 直接通过encode()和decode()方法进行编码解码

         5. encode(), decode() 原因

             1. 互联网用 `二进制` 传输 , 所以 将str转换成bytes 传输 
             2. 接收数据用 decode()解码          	
         
     3. bytearray
     
         1. bytearray和bytes区别:  bytearray 可变的
     
         2. ```
             str1  #'人生苦短，我用Python!'
             b1=bytearray(str1.encode())
             b1.decode()  #'人生苦短，我用Python!'
             
             b1[:6]=bytearray('生命'.encode())
             b1.decode()   #'生命苦短，我用Python!'
             ```
     
 2. 小结: response 编解码

     1. response.text :  返回unicode 编码的文本数据
        response.content : 返回 bytes 编码的二进制数据
     
     2. response.content.decode(), response.content.decode("gbk")
     
        

#### 1.4 requests 之 get

 1. Python 标准库中 urllib 模块已含很多 功能, 但不友好

 2. requests库  : 自称 "HTTP for Humans", 继承了urllib的所有特性, requests 的底层是 urllib
       	   	1. 支持HTTP连接和连接池, 支持 cookie保持会话, 支持文件上传, 支持自动确定响应内容的编码, 支持国际化的 URL 和 POST 数据自动编码。
            	2. requests在python2, python3中通用, 方法完全一样
       
 3. 安装方式

     	1. pip 安装   `pip install requests`
      2. easy_install 安装  `easy_install requests`

 4. 基本get请求（headers参数 和 parmas参数）

     1. 用get方法: `response = requests.get("http://www.baidu.com/")`, 返回response对象

         1. 也可以这么写: `response = requests.request("get", "http://www.baidu.com/")`

         2. requests.request()返回的response的常用属性

            1. response.text
            2. response.content
            3. response.status_code
            4. response.status_code
            5. response.request.headers
            6. response.headers
         
         3. | response               | response.request         |
            | ---------------------- | ------------------------ |
            | reponse.headers        | response.request.headers |
            | reponse.headers        | response.request.headers |
            | reponse.url            | response.request.url     |
            | reponse.text  #str类型 | response.request.content |
            | reponse.status_code    |                          |
         
         4. response.text  与 response.content 
         
            1. response.text: 类型: str, 基于 HTTP响应的 文本编码 自动解码, 修改编码方式: response.encoding=" gbk"
            2. response.content  : 类型: bytes, 返回 服务器响应数据的 `原始二进制字节流`, 保存图片等二进制文件. response.content.deocde( "utf8" )
         
            
     
  	2. requests.get(url,  headers, params)
       
	 1. 参数:
       
	       1. `headers`参数 增加请求头
              2. `params` 参数, 参数放在url中传递
	
        2. 返回值: response对象
       
        3. 示例
       
            1. ```
                 import requests
                 
                 kw = {'wd':'长城'}
                 #User-Agent指定浏览器
                 headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
                 
                 # params参数，字典类型自动转换为url编码，不需要urlencode(), 同时params参数自动二进制编码
                 response = requests.get("http://www.baidu.com/s?", params = kw, headers = headers)
                 
                 # response.text 返回Unicode编码的数据
                 print (response.text)
                 
                 #response.content返回字节序列编码数据
                 print (respones.content)
                 
                 # 查看完整url地址
                 print (response.url)  #'http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E'
                 
                 # 查看响应头部字符编码
                 print (response.encoding)  #'utf-8'
                 
                 # 查看响应码
                 print (response.status_code)  #200
       
        4. 用requests获取新浪首页
       
             1. ```
                 #coding=utf-8
                 import  requests
                 response = requests.get("http://www.sina.com")
                 print(response.request.headers)
                 print(response.content.decode())
                 #requests 自带的Accept-Encoding导致 或 新浪默认发送的就是压缩之后的网页
                 #当收到一个响应时，requests  猜测 编码方式，用于 response.text 方法中解码
                 #猜测编码方式（存在误差）,推荐response.content.deocde() 解码
                 ```
       
        5. BytesIO 和StringIO
       
             1. StringIO 在内存中读写str。BytesIO  内存中读写bytes类型数据(二进制数据)
       
             2. ```
                 from io import BytesIO,StringIO
                 import requests
                 from PIL import Image
                 img_url = "http://imglf1.ph.126.net/pWRxzh6FRrG2qVL3JBvrDg==/6630172763234505196.png"
                 response = requests.get(img_url)
                 f = BytesIO(response.content)
                 img = Image.open(f)
                 print(img.size)
       
6. 面对对象编程

    1.  对象 --> 类 --> 实例
    2. 客观的事物  --抽象--> 代码中对客观事物的抽象 --实例化,调用--> 具体我们能用的

#### 1.5 requests 之 post

1. requests.post(url, data)

   1. 参数: `data` , 传入data数据

      1. `response = requests.post("http://www.baidu.com/", data = data)`

      2. 示例

         ```
         import requests
         
         #字典格式数据
         formdata = {
             "type":"AUTO",
             "i":"i love python",
             "doctype":"json",
             "xmlVersion":"1.8",
             "keyfrom":"fanyi.web",
             "ue":"UTF-8",
             "action":"FY_BY_ENTER",
             "typoResult":"true"
         }
         
         url = "http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null"
         
         headers={ "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"}
         
         response = requests.post(url, data = formdata, headers = headers)
         
         print (response.text)
         
         # 如果是json文件 直接显示
         print (response.json())
         ```
      
         

2. proxies参数, 代理

   1.  代理

      ```
      ┌──────────┐ req  ┌──────────┐ req  ┌──────────┐
      │          │----->│    代    │----->│   Web    │
      │ Browser  │ resp │    理    │ resp │   server │ 
      │          │<-----│          │<-----│          │ 
      └──────────┘      └──────────┘      └──────────┘
      ```

      1. 正向代理: 用户知道服务器IP, 浏览器 --> 代理 --> google 服务器
      2. 反向代理: 用户不知道服务器IP,  浏览器 --> nginx --> 服务器
      
   3. ```
      import requests
      
      #根据协议类型选择不同的代理, 键值都是字符串
      proxies = {
        "http": "http://12.34.56.79:9527",
        "https": "http://12.34.56.79:9527",
      }
      
      response = requests.get("http://www.baidu.com", proxies = proxies)
      response.text
      ```

   4. 也可 本地环境变量 `HTTP_PROXY` 和 `HTTPS_PROXY` 配置代理

      ```
      export HTTP_PROXY="http://12.34.56.79:9527"
      export HTTPS_PROXY="https://12.34.56.79:9527"
      ```

      

3. 私密代理验证（特定格式） 

   1. 私密代理

      ```
      import requests
      
      # 如果代理需要使用HTTP Basic Auth，可以使用下面这种格式：
      proxy = { "http": "mr_mao_hacker:sffqry9r@61.158.163.130:16816" }
      
      response = requests.get("http://www.baidu.com", proxies = proxy)
      
      print (response.text)
      ```

      

4. auth :  Web客户端验证（auth 参数）

   1. auth = (账户名, 密码),  authority授权

      ```
      import requests
      
      auth=('test', '123456')
      
      response = requests.get('http://192.168.199.107', auth = auth)
      
      print (response.text)
      ```

      

5. HTTPS请求的 SSL证书验证

   1. Requests 为HTTPS请求验证SSL证书

      1. 主机的SSL证书，  verify 参数（也可以不写）

         ```
         import requests
         response = requests.get("https://www.baidu.com/", verify=True)
         
         # 也可以省略不写
         # response = requests.get("https://www.baidu.com/")
         print (r.text)
         ```

         1. SSL证书 验证不通过，或 不信任服务器的 安全证书，则会报出SSLError

   2. 据说 12306 SSL证书是自己做的

      ```
      #测试一下
      import requests
      response = requests.get("https://www.12306.cn/mormhweb/")
      print (response.text)
      ```

      1. 出错: `SSLError: ("bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)",)`

      2. 跳过 12306 的证书验证，参数 verify 设为 False 就可以正常请求了。

         `r = requests.get("https://www.12306.cn/mormhweb/", verify = False)`

         

6. requests 技巧

   1. reqeusts.utils.dict_from_cookiejar  把cookie对象转化为字典: `cookiedict = requests.utils.dict_from_cookiejar(cookiejar)`

   2. 参数: verify, 请求SSL证书验证: `response = requests.get("https://www.12306.cn/mormhweb/ ", verify= False)`

   3. 参数: timeout 设置超时: `response = requests.get(ur), timeout= 10)`

   4. 配合状态码判断是否请求成功: `assert(response.status_code==200)`

   5. requests的 url编码解码

      1. requests.utils.unquote() #  网络url解码为普通字符
      2. requests.utils.quote() # 普通url解码为网络url，网络ur l带%

   6. 重复请求刷新

      1. ```
         pip install retrying
         ```

      2. ```
         from retrying import retry # 重复请求，或网页刷新
         @retry(stop_ max_ _attempt_ number=3) #用装 饰器实现
         def _parse_url(ur1) :
         	print("*"*20)
         ```
      
   7. requests.get(url, verify, timeout)


#### requests之 session

1. cookies 属性

   1. cookies参数拿到 响应中的`cookie`, 返回CookieJar对象, 将CookieJar转为字典

      1. ```
         import requests
         
         response = requests.get("http://www.baidu.com/")
         
         # 返回CookieJar对象:
         cookiejar = response.cookies
         
         # 将CookieJar转为字典：
         cookiedict = requests.utils.dict_from_cookiejar(cookiejar)
         
         print (cookiejar) #<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
         
         print (cookiedict) #{'BDORZ': '27315'}
         ```

         

2. session() 对象

   1. session类实现客户端和服务端的 会话保持

   2. session类使用流程

      1. 实例化 一个session对象: `session = requests. session()`

      2. session对象发 送get或者post请求: `response = session.get(url,headers)`

   3. session对象

      1. 一次用户会话, 从客户端浏览器连接服务器 开始，到客户端浏览器与服务器 断开。
      2. 跨请求保持 参数: session 实例保持 cookies

   4. session对象, 保存Cookie值

      1. ```
         import requests
         
         # 1\. 创建session对象，可以保存Cookie值
         ssion = requests.session()
         ```

   

3. session方法 实现人人网登录

   1. ```
      import requests
      
      # 1 创建session对象， 保存Cookie值
      ssion = requests.session()
      
      # 2 处理 headers
      headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
      
      # 3 需要登录的用户名和密码
      data = {"email":"mr_mao_hacker@163.com", "password":"alarmchime"}  
      
      # 4 session对象请求,发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在session里
      ssion.post("http://www.renren.com/PLogin.do", data = data)
      
      # 5 session包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面
      response = ssion.get("http://www.renren.com/410043129/profile")
      
      # 6 打印响应内容
      print (response.text)
      ```

4. session()响应的返回值response 与 requests的响应的返回值response相同

#### 1.6 HTTP/HTTPS抓包工具-Fiddler

* Fiddler :  Web调试工具, 记录所有 HTTP请求
  * Fiddler启动的时，默认IE的代理`127.0.0.1:8888`，其他浏览器要手动设置

1. 工作原理: Fiddler相当于代理服务器

   1. ```
      Browser           Fiddler(Proxy)    Web server
      ┌──────────┐ req  ┌──────────┐ req  ┌──────────┐
      │          │----->│Break     │----->│          │
      │          │ resp │Request   │ resp │          │ 
      │          │<-----│          │<-----│          │ 
      └──────────┘      └──────────┘      └──────────┘
      ```

   2. Fiddler以`代理web服务器`的方式工作 , Fiddler地址：127.0.0.1，端口：8888

2. Fiddler抓取HTTPS设置

   1. 启动Fiddler，菜单栏 `Tools > Telerik Fiddler Options`，打开“Fiddler Options”对话框。
   2. Fiddler设置
      1. 工具栏->Tools->Fiddler Options->HTTPS
      2. 选中Capture HTTPS CONNECTs (捕捉HTTPS连接)，
      3. 选中Decrypt HTTPS traffic（解密HTTPS通信）
      4. Fiddler获取本机HTTPS请求，中间的下拉菜单选中...from all processes （从所有进程）
      5. 选中Ignore server certificate errors（忽略服务器证书错误）
   3. Fiddler 配置Windows信任 `根证书`, 解决警告：Trust Root Certificate（受信任的根证书）。
      1. Actions 选 Trust Root Certificate
   4. Fiddler 主菜单 Tools -> Fiddler Options…-> Connections
      1. 选中Allow remote computers to connect（允许远程连接）
      2. Act as system proxy on startup（作为系统启动代理）
   5. 重启Fiddler，使配置生效（这一步很重要，必须做）

3. Fiddler 捕获Chrome的会话

   1. 安装`SwitchyOmega` 代理, Chrome 浏览器插件
   2. 设置 代理服务器 为127.0.0.1:8888.  
      1. 设置  --> Fiddler
      2. 代理服务器  127.0.0.1:8888
   3. 浏览器插件切换为设置好的代理
      1. 选 Fiddler

4. Fiddler界面

   1. 本机HTTP通信 经过Fiddler (127.0.0.1:8888)代理，被Fiddler拦截到
   2. 请求 (request) 部分详解
      1. Headers ——  客户端发送到服务器的 HTTP 请求的 header， 分级视图, 含  Web 客户端信息、Cookie、传输状态等。
      2. Textview ——  显示 POST body 部分为 文本。
      3. WebForms —— 显示请求的 GET 参数 和 POST body 内容。
      4. HexView —— 用十六进制数据显示请求。
      5. Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息.
      6. Raw ——  整个请求显示为 纯文本。
      7. JSON —— 显示 JSON格式 文件。
      8. XML —— 如 请求的 body 是 XML 格式, 用分级的 XML 树显示它
   3. 响应 (response) 部分详解
      1. Transformer —— 显示响应的编码信息。
      2. Headers —— 用分级视图显示响应的 header。
      3. TextView —— 使用文本显示相应的 body。
      4. ImageVies —— 如果请求是图片资源，显示响应的图片。
      5. HexView —— 用十六进制数据显示响应。
      6. WebView —— 响应在 Web 浏览器中的预览效果。
      7. Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息。
      8. Caching —— 显示此请求的缓存信息。
      9. Privacy —— 显示此请求的私密 (P3P) 信息。
      10. Raw —— 将整个响应显示为纯文本。
      11. JSON - 显示JSON格式文件。
      12. XML —— 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它 。

5. 浏览器中  pretty print  #格式化打印



#### 1.7 urllib模块

1. urllib库

   1. 网页抓取: URL指定的 网络资源 从网络流 读取, 保存到本地
   2. urllib : python 抓取网页库, python2 `urllib` 分为urllib,urllib2

2. urllib.request.urlopen()方法

   1. ```
      #urllib_request.py
      
      #导入urllib.request 库
      import urllib.request
      
      #向指定的url发送请求，返回服务器响应的类文件对象
      response = urllib.request.urlopen("http://www.baidu.com")
      
      #类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容，返回字符串
      html = response.read()
      
      #打印字符串
      print (html)
      ```

      1. urllib.request.urlopen()返回 类文件对象  支持文件对象的 操作方法
      2. urlopen()的参数 是 url地址

3. urllib.request.Request() : 执行更复杂的操作

   1. 增加HTTP报头, 必须用  `Request` 类的实例作 urlopen()的参数

      1. ```
         #urllib_request.py
         import urllib.request
         
            # url 作为Request()方法的参数，构造并返回一个Request对象
         
            request = urllib.request.Request("http://www.baidu.com")
         
            # Request对象作为urlopen()方法的参数，发送给服务器并接收响应
         
            response = urllib.request.urlopen(request)
         
            html = response.read().decode()

      2.  创建`Request` 实例作 `urlopen()`的参数

   2. urllib.request.Request(url, data, headers)类 的参数: 

      1. url 参数
      2. data（默认空）
         1. 伴随 url 提交的数据（比如要post的数据）
         2.  HTTP 请求 从 "GET"  改为 "POST"
      3. headers（默认空）： 字典, HTTP报头 键值对。

4. User-Agent : 模拟合法的浏览器 请求别人网站

   1. 浏览器 是互联网世界 公认的被允许的 身份

      1. 爬虫程序 伪装成 公认的浏览器, 不同的浏览器发 有不同的User-Agent头
      3. urllib默认的User-Agent头：`Python-urllib/x.y`
         1. x 是Python主版本, y是Python 次版本号, 如:  Python-urllib/2.7 
      
   2. 示例

      ```
      #urllib_request.py
      
      import urllib.request
      
      url = "http://www.itcast.cn"
      
      #IE 9.0 的 User-Agent
      ua_header = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
      
      #  url 连同 headers，一起构造Request请求，
      request = urllib.request.Request(url, headers = ua_header)
      
      # 向服务器发送 请求
      response = urllib.request.urlopen(request)
      
      html = response.read()
      print (html)
      ```

      

5. urllib.request.Request() 类实例的方法: 

   1. `Request.add_header("报文头", "报文值")` 添加/修改 一个报文头

   2. `Request.get_header(header_name)` 查看header, 参数: header_name

   3. ```
      # urllib_headers.py
      
      import urllib.request
      
      url = "http://www.itcast.cn"
      
      #IE 9.0 的 User-Agent
      header = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
      request = urllib.request.Request(url, headers = header)
      
      #用Request.add_header() 添加/修改一个特定的header
      request.add_header("Connection", "keep-alive")
      
      # 也可以通过调用Request.get_header()来查看header信息
      # request.get_header(header_name="Connection")
      
      response = urllib.request.urlopen(request)
      
      print (response.code) #查看响应状态码
      html = response.read().decode()
      
      print (html)
      ```

   4. 随机添加/修改User-Agent

      ```
      #urllib_add_headers.py
      
      import urllib
      import random
      
      url = "http://www.itcast.cn"
      
      ua_list = [
          "Mozilla/5.0 (Windows NT 6.1; ) Apple.... ",
          "Mozilla/5.0 (X11; CrOS i686 2268.111.0)... ",
          "Mozilla/5.0 (Macintosh; U; PPC Mac OS X.... ",
          "Mozilla/5.0 (Macintosh; Intel Mac OS... "
      ]
      
      #构建User-Agent
      user_agent = random.choice(ua_list)
      
      request = urllib.request.Request(url)
      
      #也可以通过调用Request.add_header() 添加/修改一个特定的header
      request.add_header("User-Agent", user_agent)
      
      # get_header()的字符串参数，第一个字母大写，后面的全部小写
      request.get_header("User-agent")
      
      response = urllib.request.urlopen(requestr)
      
      html = response.read()
      print (html)
      ```
      

6. 小结: 

   1. urllib.request.Request() 与 urllib.request.urlopen() 搭配使用
   2. requests.get(), requests.post(), 是 urllib的封装
   3. session.get(), session.post(), 带cookies的请求
   4. urllib.request.urlopen()的返回值response的方法与属性
      1. 方法: read()、readinto()、getheader(name)、getheaders()、fileno()等函数
      2. 属性: msg、version、status、reason、debuglevel、closed等属性


#### 1.8 urllib：GET和POST

* urllib默认只支持HTTP/HTTPS的 `GET` 和 `POST` 方法

1. urllib中url的编码: urllib.parse.urlencode()

   1. url编码:   `urllib.parse.urlencode()`, 将字典格式{`key:value`} 键值对 转成`"key=value"`字符串并进行url编码

      2. 解码 用 `urllib.parse.unquote()`
      
   2. ```
      # IPython3 中的测试结果
      import urllib.parse
      
      word = {"wd" : "传智播客"}
      
      #urllib.parse.urlencode() 将字典键值对按URL编码
      urllib.parse.urlencode(word) #转码后 "wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2"
      
      #urllib.parse.unquote()方法，URL编码字符串 转换为 字符串
      urllib.parse.unquote("wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2")   #wd=传智播客
      ```
   
2. Get实现方式 : 用于 向服务器获取数据

   1. urllib.request.urlopen() 提交数据方法:  数据编码成 URL格式, 然后连接到url中, 作为参数传到Request对象, Request对象做 urllib.request.urlopen()的参数

   2. ```
      import urllib.parse
      import urllib.request
      # urllib_get.py
      
      url = "http://www.baidu.com/s"
      word = {"wd":"传智播客"}
      word = urllib.parse.urlencode(word) #转换成url编码格式（字符串）
      #url中?后的是数据
      newurl = url + "?" + word    # url首个分隔符就是 ?
      #newurl = 'http://www.baidu.com/s?wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2'
      
      headers={ "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"}
      
      #Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36
      
      request = urllib.request.Request(newurl, headers=headers)
      
      response = urllib.request.urlopen(request)
      
      print (response.read())
      ```
      
   3. 实例: 批量爬取 贴吧页面数据

      1. 先写一个main, 提示输入贴吧名:`lol`, urllib.parse.urlencode() 转码，然后连接成url.  `http://tieba.baidu.com/f?kw=lol`

         1. ```
            # 模拟 main 函数
            if __name__ == "__main__":
            
                kw = raw_input("请输入需要爬取的贴吧:")
                # 输入起始页和终止页，str转成int类型
                beginPage = int(input("请输入起始页："))
                endPage = int(input("请输入终止页："))
            
                url = "http://tieba.baidu.com/f?"
                
                #返回值是'kw=lol', 赋给key
                key = urllib.parse.urlencode({"kw" : "lol"}) 
            
                #组合url示例：'http://tieba.baidu.com/f?kw=lol'
                url = url + key
                tiebaSpider(url, beginPage, endPage)
            ```

      2. 再写 百度贴吧爬虫接口. 传递3个参数,  url地址, 起始页码, 终止页码

         1. ```
            def tiebaSpider(url, beginPage, endPage):
                """
                    作用：负责处理url，分配每个url去发送请求
                    url：需要处理的第一个url
                    beginPage: 爬虫执行的起始页面
                    endPage: 爬虫执行的截止页面
                """
            
                for page in range(beginPage, endPage + 1):
                    pn = (page - 1) * 50
            
                    filename = "第" + str(page) + "页.html"
                    # 组合完整的 url，并且pn值每次增加50
                    fullurl = url + "&pn=" + str(pn)
                    
            
                    # 调用loadPage()发送请求,获取HTML页面
                    html = loadPage(fullurl, filename)
                    # 将获取到的HTML页面写入本地磁盘文件
                    writeFile(html, filename)
            ```

      3. 爬取一个网页的代码 封装成 函数loadPage

         1. ```
            def loadPage(url, filename):
                '''
                    作用：根据url发送请求，获取服务器响应文件
                    url：需要爬取的url地址
                    filename: 文件名
                '''
                print ("正在下载" + filename)
            
                headers = {"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
            
                request = urllib.request.Request(url, headers = headers)
                response = urllib.request.urlopen(request)
                return response.read()
            ```

      4. 爬取的每页的信息存储在本地磁盘, 写一个存储文件的接口

         1. ```
            def writeFile(html, filename):
                """
                    作用：保存服务器响应文件到本地磁盘文件里
                    html: 服务器响应文件
                    filename: 本地磁盘文件名
                """
                print ("正在存储" + filename)
                with open(filename, 'w') as f:
                    f.write(html)
                print "-" * 20
            ```

   4. 关于url的编码

      1. ```
         headers = {"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
         
         url = "http://tieba.baidu.com/f?kw=lol&pn=100"
         request = urllib.request.Request(url, headers = headers)
         response = urllib.request.urlopen(request)
         response.read().decode()

3. POST方式：

   1. urllib.request.Request() 创建请求对象, 参数: data, 值是字典，元素是键值对

   2. 有道词典翻译网站

      1. ```
         import urllib
         
         # POST请求的目标URL
         url = "http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null"
         
         headers={"User-Agent": "Mozilla...."}
         
         formdata = {
             "type":"AUTO",
             "i":"ilovepython",
             "doctype":"json",
             "xmlVersion":"1.8",
             "keyfrom":"fanyi.web",
             "ue":"UTF-8",
             "action":"FY_BY_ENTER",
             "typoResult":"true"
         }
         
         data = urllib.parse.urlencode(formdata).encode()  #网络传输数据要二进制编码
         
         request = urllib.request.Request(url, data = data, headers = headers)
         response = urllib.request.urlopen(request)
         print (response.read())
         ```

         1. POST请求, 注意headers 属性：
            1. `Content-Length: 144`： 是指发送的表单数据长度为144，字符个数是144个。
            2. `Content-Type: application/x-www-form-urlencoded` ： 浏览器提交 Web 表单时, 表单数据 按 **name1=value1&name2=value2** 键值对 编码。
            3. `X-Requested-With: XMLHttpRequest` ：表示Ajax异步请求。

   3. GET与POST的区别 :  urllib.request.Request() 否有 参数 data

4. AJAX请求 内容

   1. AJAX请求返回的数据, 无法直接用 `url` 获取

   2. AJAX特点:AJAX请求 返回`JSON` ,即: post或get `AJAX地址`，返回`JSON`数据

   3. 示例

      ```
      import urllib
      
      #demo1
      
      url = "https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action"
      
      headers={"User-Agent": "Mozilla...."}
      
      #变动的是这两个参数，从start开始往后显示limit个
      formdata = {
          'start':'0',
          'limit':'10'
      }
      data = urllib.parse.urlencode(formdata).encode()  #请求数据url编码后再二进制编码
      
      request = urllib.request.Request(url, data = data, headers = headers)
      response = urllib.request.urlopen(request)
      
      print (response.read())
      
      
      #demo2
      
      url = "https://movie.douban.com/j/chart/top_list?"
      headers={"User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36"}
      
      #post传输的数据
      formdata = {
          'type':'11',
          'interval_id':'100:90',
          'action':'',
          'start':'0',
          'limit':'10'
      }
      data = urllib.parse.urlencode(formdata).encode()
      
      request = urllib.request.Request(url, data = data, headers = headers)
      response = urllib.request.urlopen(request)
      
      print (response.read())
      ```

   4. GET与POST

      1. GET, url包含所有的参数
         2. 服务器端用 `Request.QueryString` 获取变量的值
         3. 不安全, 直观地看到自己提交了什么内容
      2. POST :  url不显示参数
         1. 服务器端用 `Request.Form`  获取数据
         2. POST data should be bytes, an iterable of bytes, or a file object. It cannot be of type str.
      3. HTML代码 不指定 method 属性，默认为GET请求
         1. Form 提交的数据 附加在url之后，以`?`分开与url分开

5. 处理HTTPS请求 SSL证书验证

   1. urllib验证 HTTPS 请求的网站的 SSL证书

      1. 像web浏览器一样

      2. 如果网站的SSL证书 经过CA认证的，则 正常访问

      3. 如访问 '12306'

         ```
         import urllib
         
         url = "https://www.12306.cn/mormhweb/"
         
         headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
         
         #没有附加数据,不要.encode()转二进制格式
         request = urllib.request.Request(url, headers = headers)
         
         response = urllib.request.urlopen(request)
         
         print (response.read().decode())  #从二进制格式解码
         ```
         
         1. 报出SSLError

   2. 单独处理SSL证书, 程序忽略`SSL证书`验证错误

      1. ```
         import urllib
         # 1. 导入Python SSL处理模块
         import ssl
         
         # 2.忽略未经核实的SSL证书认证
         context = ssl._create_unverified_context()
         
         url = "https://www.12306.cn/mormhweb/"
         
         headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
         
         request = urllib.request.Request(url, headers = headers)
         
         # 3. 在urlopen()方法里 添加 context 参数
         response = urllib.request.urlopen(request, context = context)
         
         print (response.read().decode())
         ```

         1. 流程:
            1. import  ssl #导入 ssl
            2. context= ssl._create_unverified_context()   #忽略SSL验证
            3. urllib.request.urlopen(..., context=context)  #添加context参数

6. 关于CA

   1. CA(Certificate Authority) 数字证书认证中心的简称
      1. [北京数字认证股份有限公司](http://www.bjca.org.cn)、[上海市数字证书认证中心有限公司](http://www.sheca.com)等
      2. 指发放、管理、废除数字证书的受信任的第三方机构
   2. 网络世界里，数字证书 是身份证

#### 1.9 urllib：Handler处理器和自定义Opener

 1. Handler处理器 和 自定义Opener

     	1. opener  :   urllib.request.OpenerDirector 的实例,  是模块 构建好的
          1. 背景 :  urllib.request.urlopen()不支持 `代理`、`cookie`等 HTTP/HTTPS高级功能
          2. 解决方法:
              1. 先创建 `Handler`处理器对象,  它有特定功能
              2. 再 `urllib.request.build_opener()` 用Handler对象创建opener对象；
              3. 最后: `opener`对象，调用`open()` 发送请求
         2. `urllib.request.install_opener()` 定义 全局opener
             	1. 程序 所有的请求都 可以用opener
                	2. 凡是调用urlopen()方法，都可以用opener
      
 2. opener() 语法:

     1. ```
        import urllib.request
        
        # 构建 HTTPHandler 处理器对象，支持 HTTP请求
        http_handler = urllib.request.HTTPHandler()
        
        # 构建 HTTPHandler 处理器对象，支持 HTTPS请求
        # http_handler = urllib.request.HTTPSHandler()
        
        # urllib.request.build_opener() 创建opener对象
        opener = urllib.request.build_opener(http_handler)
        
        # 构建 Request对象
        request = urllib.request.Request("http://www.baidu.com/")
        
        # 调用opener对象的open()方法，发送request请求
        response = opener.open(request)
        
        # 获取服务器响应内容
        print (response.read().decode())
        ```

         	1. handler --> opener  --> opener.open(request)

     2. HTTPHandler()增加 `debuglevel=1`参数

         1. 打开Debug Log , 

             1. 程序执行时 自动打印 收包和发包的报头在屏幕，方便调试
             2. 省去抓包的工作

         2. 仅需要修改的代码部分： 

            ```
            # 构建一个HTTPHandler 处理器对象，支持处理HTTP请求，同时开启Debug Log，debuglevel 值默认 0
            http_handler = urllib.request.HTTPHandler(debuglevel=1)
            
            # 构建一个HTTPHSandler 处理器对象，支持处理HTTPS请求，同时开启Debug Log，debuglevel 值默认 0
            https_handler = urllib.request.HTTPSHandler(debuglevel=1)
            ```

 3. ProxyHandler 代理处理器

     1. 代理IP : 这是爬虫/反爬虫的第二大招, 每隔一段时间换一个代理

     2. urllib.request的ProxyHandler 设置 代理服务器(自定义opener使用代理)

        ```
        #urllib_proxy1.py
        
        import urllib.request
        
        # 构建 两个代理ProxyHandler，一个有代理IP，一个没有代理IP
        httpproxy_handler = urllib.request.ProxyHandler({"http" : "124.88.67.81:80"})
        nullproxy_handler = urllib.request.ProxyHandler({})
        
        proxySwitch = True #定义一个代理开关
        
        #urllib.request.build_opener()用ProxyHandler对象创建opener对象
        # 根据代理开关是否打开，使用不同的代理模式
        if proxySwitch:  
            opener = urllib.request.build_opener(httpproxy_handler)
        else:
            opener = urllib.request.build_opener(nullproxy_handler)
        
        request = urllib.request.Request("http://www.baidu.com/")
        
        # 1.urllib.request.build_opener定义的opener 能用 opener.open()使用 代理发送请求. urlopen()不能用自定义的代理。
        response = opener.open(request)
        
        # 2. urllib.request.install_opener(opener)定义opener,opener是全局的， opener.open()和urlopen()都可用自定义的代理。
        # urllib.request.install_opener(opener)
        # response = urlopen(request)
        
        print (response.read().decode())
        ```

     3. 免费短期代理网站

        1. ```
           1.  西刺免费代理IP http://www.xicidaili.com/ 
           2.  快代理免费代理 http://www.kuaidaili.com/free/inha/ 
           3.  Proxy360代理 http://www.proxy360.cn/default.aspx 
           4.  全网代理IP http://www.goubanjia.com/free/index.shtml 
           1. 专业爬虫工程师或爬虫公司 用高品质的私密代理
           ```

     4. 代理IP足够多,随机选 一个代理去访问网站,  像随机获取User-Agent一样

        1. ```
           import urllib.request
           import random
           
           proxy_list = [
               {"http" : "124.88.67.81:80"},
               {"http" : "124.88.67.81:80"},
               {"http" : "124.88.67.81:80"},
               {"http" : "124.88.67.81:80"},
               {"http" : "124.88.67.81:80"}
           ]
           
           # 随机选择一个代理
           proxy = random.choice(proxy_list)
           # 使用选择的代理构建代理处理器对象
           httpproxy_handler = urllib.request.ProxyHandler(proxy)
           
           opener = urllib.request.build_opener(httpproxy_handler)
           
           request = urllib.request.Request("http://www.baidu.com/")
           response = opener.open(request)
           print (response.read())
           
           ```

           

 4. Cookie

     1. Cookie 保持登录信息  :  网站服务器 辨别用户身份, 跟踪Session，储存在用户浏览器上的 文本文件

     2. Cookie原理
     
         1. 背景: HTTP是无状态的面向连接的协议
     
         2. 解决方法: 引入了`Cookie`机制.  Cookie是http报文头中的一种
     
            ```
            Cookie名字（Name）
            Cookie的值（Value）
            Cookie的过期时间（Expires/Max-Age）
            Cookie作用路径（Path）
            Cookie所在域名（Domain），
            使用Cookie进行安全连接（Secure）。
            ```
     
            1. 前两个参数 Cookie应用的必要条件
            2. Cookie大小（Size，不同浏览器对Cookie个数及大小限制 有差异 ）

     3. 设置Cookie语法:  `Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE`,  Cookie组成 : 变量名 值, Netscape公司的规定
     
         
     
     4. Cookie应用
     
         1. 爬虫中应用, 判定注册用户是否 登录网站

         1. headers中添加语法:  "Cookie":"cookie值"

         2. ```
            #添加有登录信息的Cookie, 模拟登陆
            
            import urllib
            
            #1. 构建已经登录的用户的headers信息
            headers = {
                "Host":"www.renren.com",
                "Connection":"keep-alive",
                "Upgrade-Insecure-Requests":"1",
                "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",
                "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language":"zh-CN,zh;q=0.8,en;q=0.6",
                "Referer":"http://www.renren.com/SysHome.do",
                # 便于终端阅读，表示不支持压缩文件
                # Accept-Encoding: gzip, deflate, sdch,
            
                #Cookie保存了密码,无需重复登录. Cookie里有用户名，密码(经过RAS加密)
                "Cookie": "anonymid=j3jxk555-nrn0wh; depovince=BJ; _r01_=1; JSESSIONID=abcnLjz9MSvBa-3lJK3Xv; ick=3babfba4-e0ed-4e9f-9312-8e833e4cb826; jebecookies=764bacbd-0e4a-4534-b8e8-37c10560770c|||||; ick_login=84f70f68-7ebd-4c5c-9c0f-d1d9aac778e0; _de=7A7A02E9254501DA6278B9C75EAEEB7A; p=91063de8b39ac5e0d2a57500de7e34077; first_login_flag=1; ln_uact=13146128763; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=39fca09219c06df42604435129960e1f7; societyguester=39fca09219c06df42604435129960e1f7; id=941954027; xnsid=8868df75; ver=7.0; loginfrom=null; XNESSESSIONID=a6da759fe858; WebOnLineNotice_941954027=1; wp_fold=0"
            }
            
            #2.构建Request对象, 添加headers报头信息（含Cookie信息），
            request =  urllib.request.Request("http://www.renren.com/941954027#", headers = headers)
            
            #3. 直接访问renren主页，服务器会根据headers报头信息（主要是Cookie信息），判断这是一个已经登录的用户，并返回相应的页面
            response = urllib.request.urlopen(request)
            
            # 4. 打印响应内容
            print (response.read().decode())
            ```
     
            1. 这样做
               1. 先 在浏览器登录账户，并 设置保存密码
               2. 再 通过抓包 获取 Cookie
     
     5. Python用 cookiejar库 和 HTTPCookieProcessor处理器 处理Cookie方法

         1. `cookiejar`模块：存储cookie的对象, 该模块 的类有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar
     
              1. CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向HTTP请求添加cookie的对象。对象的属性有`.name, .value`
              2. cookie 存储在内存, CookieJar实例 垃圾回收后cookie丢失
     
         2. `HTTPCookieProcessor`处理器：构建handler对象, 处理cookie对象, 。
     
         3. 案例
     
             1. 获取Cookie，保存到CookieJar()对象
     
                ```
                # urllib_cookiejar_test1.py
                
                import urllib
                from http import cookiejar
                
                #1. 构建 CookieJar对象实例 保存cookie
                cookiejar = cookiejar.CookieJar()
                
                #2. HTTPCookieProcessor()创建cookie处理器对象，参数cookieJar实例
                handler=urllib.request.HTTPCookieProcessor(cookiejar)
                
                #3. build_opener()构建opener
                opener = urllib.request.build_opener(handler)
                
                #4. get方法访问页面，访问后 自动保存cookie到cookiejar中
                opener.open("http://www.baidu.com")
                
                #5. 按标准格式将保存的Cookie打印出来
                cookieStr = ""
                for item in cookiejar:
                    cookieStr = cookieStr + item.name + "=" + item.value + ";"
                
                ## 舍去最后一位的分号
                print (cookieStr[:-1])
                ```
     
                
     
             2. 利用cookiejar和post登录人人网
     
                ```
                import urllib
                from http import cookiejar
                
                #1. 构建 CookieJar对象实例 保存cookie
                cookie = cookiejar.CookieJar()
                
                #2. HTTPCookieProcessor() 创建cookie处理器对象，参数 CookieJar的实例
                cookie_handler = urllib.request.HTTPCookieProcessor(cookie)
                
                #3. build_opener() 构建opener
                opener = urllib.request.build_opener(cookie_handler)
                
                #4 opener添加headers信息, 用.addheaders属性,list类型,每个元素是一个headers的元素
                opener.addheaders = [("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36")]
                
                # 5. 登录的账户和密码
                data = {"email":"13****46**8763", "password":"****"}  
                
                # 6. 通过urlencode()转码
                postdata = urllib.parse.urlencode(data).encode()
                
                # 7. 构建Request请求对象，包含需要发送的用户名和密码
                request = urllib.request.Request("http://www.renren.com/PLogin.do", data = postdata)
                
                # 8. opener发送 请求， 获取登录后的Cookie值，自动保存着CookieJar()对象中
                opener.open(request)                                              
                
                # 9. opener包含用户登录后的Cookie值，可以直接访问 登录后才可以访问的页面
                response = opener.open("http://www.renren.com/410043129/profile")  
                
                # 10. 打印响应内容
                response.read().decode()
                ```
     
         4. 模拟登录 注意点
     
             1. 先用 HTTP GET登录 , 拉取一些信息及获得Cookie，然后再HTTP POST登录。
             2. HTTP POST登录的链接 可能是动态的，从GET返回的信息中获取。
             3. password 有些是明文发送，有些是加密后发送
             1. 有些网站 采用动态加密, 包括了很多 加密信息，只能 查看JS源码获得加密算法，再 破解加密.非常困难。
             4. 网站的 登录流程 类似,  细节不一样. 不能保证其他网站登录成功。
             6. 网页 用JavaScript动态技术, 封锁基于 HttpClient 的模拟登录很容易
                1. 解决方案: 用内置浏览器引擎的爬虫(Selenium ，PhantomJS)

     

#### 1.10 编码故事

1. 两个字节长的编码，"全角"字符
2. 127号以下 叫"半角"字符
3.  UNICODE 
   1. 半角是英文字母，全角是汉字，都是统一的"一个字符"！都是统一的"两个字节"
   2. "字符"和"字节"两个术语的不同，“字节”是一个8位的物理存贮单元，“字符”是一个文化符号
4. GBK 与UNICODE 的汉字 内码编排 不一样
5. 面向传输  UTF（UCS Transfer Format）标准
   1. UTF8 每次8个位传输数据
   2. UTF16 每次16个位
   3. UNICODE到UTF 不是直接的对应，要过一些算法和规则来转换
6. 小结
   1. 字符(Character) :文字和符号的总称
      2. 各国家文字、标点符号、图形符号、数字等
      3. 字符集(Character set)是多个字符的集合
   2. 字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等
   3. ASCII编码 1个字节， Unicode编码 2个字节
   4. UTF-8 : Unicode的实现方式之一, UTF-8  变长的编码方式: 1，2，3个字节

#### 代码

1. baidufanyi.py

   1. ```
      # coding=utf-8
      import requests
      import execjs
      
      js = '''function a(r, o) {
              for (var t = 0; t < o.length - 2; t += 3) {
                  var a = o.charAt(t + 2);
                  a = a >= "a" ? a.charCodeAt(0) - 87 : Number(a),
                  a = "+" === o.charAt(t + 1) ? r >>> a : r << a,
                  r = "+" === o.charAt(t) ? r + a & 4294967295 : r ^ a
              }
              return r
          }
          function n(r) {
              var o = r.length;
              o > 30 && (r = "" + r.substr(0, 10) + r.substr(Math.floor(o / 2) - 5, 10) + r.substr(-10, 10));
              var t = void 0
                , n = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107);
              t = null !== C ? C : (C = window[n] || "") || "";
              for (var e = t.split("."), h = Number(e[0]) || 0, i = Number(e[1]) || 0, d = [], f = 0, g = 0; g < r.length; g++) {
                  var m = r.charCodeAt(g);
                  128 > m ? d[f++] = m : (2048 > m ? d[f++] = m >> 6 | 192 : (55296 === (64512 & m) && g + 1 < r.length && 56320 === (64512 & r.charCodeAt(g + 1)) ? (m = 65536 + ((1023 & m) << 10) + (1023 & r.charCodeAt(++g)),
                  d[f++] = m >> 18 | 240,
                  d[f++] = m >> 12 & 63 | 128) : d[f++] = m >> 12 | 224,
                  d[f++] = m >> 6 & 63 | 128),
                  d[f++] = 63 & m | 128)
              }
              for (var S = h, u = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), l = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), s = 0; s < d.length; s++)
                  S += d[s],
                  S = a(S, u);
              return S = a(S, l),
              S ^= i,
              0 > S && (S = (2147483647 & S) + 2147483648),
              S %= 1e6,
              S.toString() + "." + (S ^ h)
          }
          var C = null;
          t.exports = n
      }'''
      
      ctx = execjs.compile(js)
      ctx.call("n","你好啊")
      ```
      
   2. ```
      headers = {
          # "Accept":"*/*",
          # "Accept-Encoding":"gzip, deflate",
          # "Accept-Language":"zh-CN,zh;q=0.9,en;q=0.8,ru;q=0.7,zh-TW;q=0.6",
          # "Connection":"keep-alive",
          # "Content-Type":"application/x-www-form-urlencoded",
          # "Host":"fanyi.baidu.com",
          # "Origin":"http://fanyi.baidu.com",
          # "Referer":"http://fanyi.baidu.com/",
          "User-Agent":"Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Mobile Safari/537.36",
          # "X-Requested-With":"XMLHttpRequest",
          # "Cookie":"BAIDUID=B27C9ABD8A5079E6D8F42D0451E1CFA2:FG=1; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1515338548; from_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; to_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; BIDUPSID=B27C9ABD8A5079E6D8F42D0451E1CFA2; PSTM=1515339233; H_PS_PSSID=1437_24565_21117_17001; PSINO=6; locale=zh; Hm_lvt_afd111fa62852d1f37001d1f980b6800=1515340680; Hm_lpvt_afd111fa62852d1f37001d1f980b6800=1515340680; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1515340680"
      }
      
      data = {"query":"你好啊",
              "from":"zh",
              "to":"en"}
      r = requests.post("http://fanyi.baidu.com/basetrans",headers=headers,data=data)
      print(r.content.decode())

##### 1章总结:

1. 爬虫

   1. 网络爬虫定义(网页蜘蛛,网络机器人)
      1. 模拟 客户端浏览器 发送网络请求,接收 响应,  按照规则,自动地抓取信息的 程序
      2. 原则上,只要 浏览器能做的, 爬虫都能做 
   2. 分类:
      1.  通用爬虫  如搜索引擎爬虫
         1. 流程 : 抓取网页 --> 数据存储 --> 预处理 -->提供检索服务, 网站排名
         2. 网页中 90% 都无用
         3. 图片、音频、视频多媒体等内容 用  `通用搜索引擎` 无能为力
      2.  聚焦爬虫 特定网站的爬虫
         1. 流程
            1. url list --> 响应内容 --> 提取数据 --> 入库
            2. 响应内容 --> 提取url --> url list
   3. Robots协议: 告诉 爬虫哪些网页不能爬, 是道德约束
   
2. http, https

   1. 端口 80, 443

   2. 浏览器渲染(element)的页面 和 爬虫请求的页面 不一样, 以当前 url 对应的 response 为准

   3. 网页的数据的位置

      1. 当前 url 对应的响应
      2. 其他 url 对应的响应 , 如 ajax 请求
      3. js 生成的
   
   4. url形式:   `scheme://host[:port#]/path.../[?query-string][#anchor]`

      1. 含义: 
         1. scheme :协议(例如: http, https, ftp), 
         2. host :服务器的IP地址或者域名
         3. port :服务器的端口(如果是走协议默认端口, 80 or 443 )
         4. path : 资源在服务器上的路径
         5. query-string :参数,发送给http服务器的数据
         6. anchor :锚(跳转到网页的指定锚点位置)
      2. 示例: `http://item.jd.com/11936238.html#product-detail ;`
         1. schem: http, host: item.jd.com, port 省略, path: /11936238.html, anchor : `#product-detail`
   
   5. http请求形式: 

      1. ```
         请求行   GET / HTTP/1.1
         请求头   Host: www.baidu.com  很多
         空行
         请求数据
         ```
   
      2. 常见的请求头

         1. Host (主机和端口号)
         2. Connection (链接类型),  `Connection:  keep-alive` 
         3. Upgrade-Insecure- Requests (升级为HTTPS请求) http升级为https
         4. User-Agent (浏览器名称)
         5. Accept (传输文件类型)
         6. Referer(员(面跳转处)从哪个页面跳转过来
         7. Accept-Encoding (文件编解码格式)
         8. Cookie ( Cookie ) Cookie值有很多个，name=value， 其他不管
         9. **x-requested- with :XMLHttpRequest**  (是Ajax 异步请求)
         10. Accept-Language: 可接受的语义
   
      3. GET, POST 请求方法

      4. status_code   :  200, 300, 400, 500

   6. str, bytes,  字符串了两种形式

      1. bytes: 二进制字节序列
         1. 互联网 数据 二进制方式传输
         2. 以 `b` 开头的字符串
      2. str: unicode
      3. `a.encode(),  b.decode()`   #默认utf-8
         1. `b.decode('utf-8')`
      4. 字符集演进 :  ASCII --> GBK --> Unicode --> UTF-8
         1. utf-8 是 Unicode的升级
         2. 编码, 解码方式 一致, 否则乱码
   
3. requests   (是小写 r, 不是大写R), 底层 是 urllib

   2. python2, python3通用, requests 简单, 自动解压网页

   3. 作用: 发送网络请求, 返回响应数据

   4. 语句

      1. `response = requests.get(url)`

      2. requests.get()的返回对象response的常用属性, 方法:

         1. ```
         response.status_code
            response.text  #str 字符串, 根据头响应推测类型并解码
            respones.content  #bytes 字符串
            
            response.headers
            response.request.headers
            ```
   
         2. ````
         response.encoding="gbk"`  #修改编码, 两行代码
            response.decode()  #上句已设置 gbk解码
            
            #合成依据
            response.decode(encoding="gbk")
            ````
   
         3. `response.content.decode("utf-8")`   #按 utf-8 解码

   5. 为什么带 headers: 模拟浏览器, 欺骗服务器

   6. `requests.get(url, headers=headers)`
   
      1. 参数  params
      2. kw = {'wd': '长城'}
      3. `requests.get(url, params=kw)`
      4. requests.get(url, headers, params, cookies, proxies)
   
   7. 定位 js
   
      1. 方法1 : 选中触发 js 的按钮,  点击 `event listener` , 找到 js 位置
      2. 方法2 : chrome中 `search all file` 中搜索 url 的关键字
      3. 方法3 : 添加断点 查看js的操作，通过 python 进行同样的操作
   8. post() 方法
   
      1. `response = requests.post(url, data=data, headers=headers)`
      1. data 字典数据格式

   9. 代理: 让服务器以为不同的客户端请求, 产生错觉
   
      2. 防止 我们IP 泄露
   
      3. ```
         浏览器 --requests--> 代理 --requests--> web server
         浏览器 <--response-- 代理 <--response-- web server
         ```

         1. 正向代理:  客户端浏览器 知道 服务器IP , `浏览器 --> 代理 --> google服务器`
      2. 反向代理:  客户端浏览器 不知道 服务器IP, `浏览器 --> nginx --> 服务器`
   
      4. ```
         proxies={
         	"http":"http://12.34.56.78:9333",
         	"https":"http://12.34.56.78:9333"
         }
         requests.get("www.baidu.com",   proxies=proxies)
         ```
   
   10. cookie 与 session
   
       1. | cookie               | session        |
       | -------------------- | -------------- |
          | 数据 在 客户端浏览器 | 数据 在 服务器 |
          | 不安全               | 安全           |
          | <4K, 最多20个        |                |
   
       2. 带 cookie 的 session 好处:  能请求到 登录后的 页面
   
          1. 为获取登录之后的页面, 必须发送带 `cookies` 的请求
   
       3. 弊端: 容易被识别为 爬虫.  尽量不用 `cookie`
   
4. requests 的 session() 类实现客户端和服务器端 `会话保持`

   1. 使用方法:

      1.  先 实例化对象, 对象get 或 post(url,headers)

      2. ```
         session = requests.session()  #实例化一个session对象
         response = session.get(url,headers)  #发送get或者post请求
         ```

   2. requests技巧

      1. `reqeusts.util.dict_from_cookiejar`  #把cookie对象转化为字典
         
         1. `requests.get(url,cookies={})`
      2. 请求SSL证书验证

      1. `response = requests.get("https://www.12306.cn/mormhweb/ ", verify= False)`   #verify 证书验证
         
      3. 设置超时:`response = requests.get(url,timeout=10 )`   #
         
      4. 配合状态码判断是否请求成功: `assert response.status_code == 200`

      5. 示例 : 通过一个例子 看一下 上4点的用法: `requests.get(url, verify, timeout)`

      6. ```
         requests.utils.unquote ()  #网络ur1解码为普通字符
         requests.utils.quote() # 普通url编码为网络ur1，网络url带%
         
         #pip Instrall retrying  #安装retrying模块, 重复请求
         from retrying import retry # 重复请求,或网页刷新
         @retry(stop_ max_ attempt_ number=3) #用 装饰器实现
         def _parse_ url (url) :
         print("*"*20)
         ```

5. 不发送post请求，使用cookie获取登录后的页面

   1. cookie过期时间很长的网站
   2. 在cookie过期之前能够拿到所有的数据，比较麻烦
   3. 配合其他程序一起使用，其他程序专门获取cookie，当前程序专门请求页面

6. 获取登录后的页面的三种方式

   1. 方法1 : 实例化session. 使用 session.post() 发请求，再使用他获取登陆后的页面
   2. 方法2 : headers中添加cookie头，`cookie: 值为cookie字符串`
   3. 方法3 : 请求方法 添加cookies参数. 
      1. 接收字典形式的cookie
      2. 字典形式的cookie中的键是cookie的name对应的值，值是cookie的value对应的值

7. 字符串格式化的另一种方式

   1. `"传{}智播客".format(1)`

8. 寻找 post 地址

   1. `form`表单中寻找 action 对应的url地址
      1. post的数据是字典: input标签中name的值 为键， 用户名密码 为值
      2. post的url地址 是action对应的url地址
   2. 抓包，寻找url地址
      1. 勾选`perserve log`按钮，防止页面跳转找不到url
      2. 寻找`post`数据，确定参数
         1. 参数不变情况:直接用，如密码不是动态加密
         2. 参数变化情况: 参数在当前的响应中, 
            2. 通过js生成
   
9. json使用注意点

   1. ```
      import json
      t = json.loads("{}")
      ```

   2. json 字符串 **双引号** 引起 

      1. 如果不是 双引号 解决方法
         1. eval： 实现 简单的字符串和python类型 转化
         2. replace： 单引号 替换为 双引号

   3. 一个文件写入多个 json串，不再是 一个json串，不能直接读取

      1. 一行写一个json串，按行 读取

10. 安装第三方模块 方法3种

    1. 方式1: pip install retrying

    2. 源码安装: 下载源码解码，进入解压后的目录, `python setup.py install`
    3. `***.whl` 安装方法:  `pip install ***.whl`




### 2 非结构化数据与结构化数据提取

1. 页面解析和数据提取
   1. 非结构化数据：先有数据，再有结构，（http://www.baidu.com）
   2. 结构化数据：先有结构、再有数据
   3. 不同类型的数据， 用不同的方式来处理
2. 非结构化的数据处理
   1. 文本、电话号码、邮箱地址: 正则表达式
   2. HTML 文件: 正则表达式, XPath, CSS选择器
3. 结构化的数据处理
   1. JSON 文件: JSON Path处理JSON, 转化成Python类型, 进行操作（json类）
   2. XML 文件: 转化成Python类型（xmltodict）, XPath , CSS选择器, 正则表达式

#### 2.1 正则表达式re模块

https://www.runoob.com/python/python-reg-expressions.html

1. 爬虫 四 步骤

   1. 明确网站 ( 知道在哪个范围或网站 搜索)
   2. 爬 (将 网站的内容 爬下来)
   3. 取 (取有用的数据)
   4. 处理数据（按 我们想要的方式 `存储`和`使用`）

2. 正则表达式:又称规则表达式,

   2. 检索、替换 符合某个模式(规则)的文本

   3. 正则表达式是 字符串操作的一种`逻辑`公式

   4. 流程

      ```
      ┌───────────┐ 编译 ┌───────────┐ 匹配  ┌───────────┐
      │正则表达式引擎│ --> │正则表达式对象│ -->  │   匹配结果  │  
      └───────────┘  │  └───────────┘   │   └───────────┘
                     │                  │
               ┌───────────┐     ┌───────────┐ 
               │正则表达式文本│     │需要匹配的文本│
               └───────────┘     └───────────┘
      ```
   
      

3. 正则表达式匹配规则

   1. 字符

      1. | 字符    | 匹配自身                                                     | abc              | abc                 |
         | ------- | ------------------------------------------------------------ | ---------------- | ------------------- |
         | `.`     | 匹配任意除换行符"\n"外的字符。<br/>在DOTAL模式中也能匹配换行符。 | a.c              | abc                 |
         | `\`     | 转义字符,使后一个字符改变原来的意思。<br/>如果字符串中有字符*需要匹配,可以使用\*或者字符集[*]。 | `a.c`<br/>`a\\c` | `a\.c`<br/>`a\c`    |
         | `[...]` | 字符集(字符类)。对应的位置可以是字符集中任意字符。字符集中的字符可以逐个列出，也可以给出范围,如`[abc]`或`[a-c]`。第一个字符如果是`^`则表示取反，如`[^abc]`表示不是abc的其他字符.所有的特殊字符在字符集中都失去其原有的特殊含义。在字符集中如果要使用`]`、`-`或`^ `，可以在前面加上反斜杠，或把`]`、`-`放在第一个字符,把`^`放在非第一 个字符。 | a[bcd]e          | abe<br/>ace<br/>ade |

         

   2. 预定义字符集

      1. | 一般字符 | 匹配自身                    | abc    | abc   |
         | -------- | --------------------------- | ------ | ----- |
         | `\d`     | 数字:` [0-9]`               | `a\dc` | `a1c` |
         | `\D`     | 非数字: `[^\d]`             | `a\Dc` | `abc` |
         | `\s`     | 空白字符: [<空格>\t\r\n\fv] | `a\sc` | `a c` |
         | `\S`     | 非空白字符:` [^\s]`         | `a\Sc` | `abc` |
         | `\w`     | 单词字符: [A-Za-z0-9_ ]     | `a\WC` | `abc` |
         | `\W`     | 非单词字符: `[^\w]`         | `a\Wc` | `a c` |

         

   3. 数量词

      1. | 一般字符 | 匹配自身 | abc  | abc  |
         | -------- | -------- | ---- | ---- |
         |          |          |      |      |

         

   4. 边界匹配

      1. | 一般字符 | 匹配自身 | abc  | abc  |
         | -------- | -------- | ---- | ---- |
         |          |          |      |      |

         

   5. 逻辑,分组

      1. | 一般字符 | 匹配自身 | abc  | abc  |
         | -------- | -------- | ---- | ---- |
         |          |          |      |      |

         

   6. 特殊构造(不作为分组)

      1. | 一般字符 | 匹配自身 | abc  | abc  |
         | -------- | -------- | ---- | ---- |
         |          |          |      |      |

         

4. Python 的 re 模块

   1. re 模块 即python中的 正则表达式

      1. 正则表达式用 `\` 对特殊字符 转义,  

      2. 对应的, python中若用 原始字符串, 加`r`前缀

         `r'chuanzhiboke\t\.\tpython'`

5. re 模块的一般使用步骤如下

   1.  `re.compile()` 正则表达式编译为一个 `Pattern` 对象
   2.  用`Pattern` 对象的方法对 文本 查找，获得匹配结果(一个 Match 对象)。
   3. 用`Match` 对象的属性和方法 获得信息，

6. re.compile(): 编译正则表达式，返回 `Pattern` 对象

   1. ```
      import re
      
      # 正则表达式 编译 成 Pattern 对象
      pattern = re.compile(r'\d+')
      ```
      
   2. Pattern对象 方法

      1. match 方法：从`起始`位置开始查找，`一次`匹配
      2. search 方法：从`任何`位置开始查找，`一次`匹配
      3. findall 方法：`全部`匹配，返回`列表`
      4. finditer 方法：`全部`匹配，返回`迭代器`
      5. split 方法：分割字符串，返回`列表`
      6. sub 方法：替换
   
7. match 方法

   1. 字符串的头部（起始位置）， 一次匹配

      `match(string[, pos[, endpos]])`

      1. 参数: string 待匹配的字符串, pos 和 endpos  可选参数，指定字符串的起始/终点位置，默认值分别是 0 和 len (字符串长度)
      
   2. 示例1

      1. ```
         import re
         pattern = re.compile(r'\d+')  # 匹配至少一个数字
         #type(pattern)    #<class 're.Pattern'>
         
         m = pattern.match('one12twothree34four')  # 查找头部，没有匹配
         m  #None
         ```
         
      2. ```
         m = pattern.match('one12twothree34four', 3, 10) # 从'1'的位置开始匹配，没有匹配
         m  #'12'
         ```

      3. ```
         m.group(0)  #'12' 
         m.start(0)  #3
         m.end(0)  #5
         m.span(0)  #(3, 5)
         ```

      4.  返回一个 Match 对象

         1. group([group1, ...]) 方法  : 获得一个或多个分组匹配的字符串
            2. 当要获得整个匹配的子串时，用 group() 或 group(0)；
         2. start([group]) 方法
            1. 分组匹配的子串的起始位置（子串第一个字符的索引），参数默认值为 0；
         3. end([group]) 方法
            1. 分组匹配的子串的结束位置（子串最后一个字符的索引+1），参数默认值为 0；
         4. span([group]) 方法返回 (start(group), end(group))

   3. 示例2

      `pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I)  # re.I 表示忽略大小写`
      
      参数: 参数1: r'正则字符串',  参数2: 选项

8. search 方法: 字符串的任何位置查找, 一次匹配

   1. `search(string[, pos[, endpos]])`

      1. string  待匹配的字符串
      2. pos 和 endpos  可选参数, 字符串的起始和终点位置，默认值是 0 和 len (字符串长度)
      3. 返回值: 返回 Match 对象, 或 返回 None
      
   2. 示例
   
      1. ```
         import re
         pattern = re.compile('\d+')
         
         m = pattern.search('one12twothree34four')  #如用 match 则不匹配
         m  #<_sre.SRE_Match object at 0x10cc03ac0>
         m.group()  #'12'
         
         m = pattern.search('one12twothree34four', 10, 30)  #指定字符串区间
         m  #<_sre.SRE_Match object at 0x10cc03b28>
         m.group()  #'34'
         m.span()  #(13, 15)
         ```
   
      2. ```
         # -*- coding: utf-8 -*-
         
         import re
         # 将正则表达式编译成 Pattern 对象
         pattern = re.compile(r'\d+')
         #search() 查找匹配的子串，不匹配返回 None
         # 这里使用 match() 无法成功匹配
         m = pattern.search('hello 123456 789')
         if m:
             # 使用 Match对象 获得分组信息
             print ('matching string:',m.group())
             # 起始位置和结束位置
             print ('position:',m.span())
         ```
   
         结果:
   
         ```
         matching string: 123456
         position: (6, 12)
         ```
   
         
   
9. findall 方法

   1. 搜索整个`字符串`，获得所有匹配的结果

      `findall(string[, pos[, endpos]])`

      1. string  待匹配的字符串
      2. pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)
      2. 返回: list类型

   2. 示例1

      ```
      import re
      pattern = re.compile(r'\d+')   # 查找数字
      
      result1 = pattern.findall('hello 123456 789')
      result2 = pattern.findall('one1two2three3four4', 0, 10)
      
      print (result1)  #['123456', '789']
      print (result2)  #['1', '2']
      ```

   3. 示例2

      ```
      # re_test.py
      
      import re
      
      #re模块提供compile方法, 一个匹配的规则
      #返回一个pattern实例，根据这个规则 匹配字符串
      pattern = re.compile(r'\d+\.\d*')
      
      #partten.findall()方法 全部匹配 字符串
      result = pattern.findall("123.141593, 'bigcat', 232312, 3.15")
      
      #findall 以 列表形式 返回全部能匹配的子串给result
      for item in result:
          print (item)
          
      #运行结果：
      123.141593
      3.15
      ```

      

10. finditer 方法

    1. finditer 跟 findall 的类似, 搜索整个字符串

    2. 返回: 迭代器 -- 每一个匹配结果是一个Match 对象  

    3. 示例

       1. ```
          # -*- coding: utf-8 -*-
          
          import re
          pattern = re.compile(r'\d+')
          
          result_iter1 = pattern.finditer('hello 123456 789')
          
          print (type(result_iter1))  #<type 'callable-iterator'>
          
          for m1 in result_iter1:   # m1 是 Match 对象
              print ('matching string: {}, position: {}'.format(m1.group(), m1.span()))
          
          输出:
          matching string: 123456, position: (6, 12)
          matching string: 789, position: (13, 16)
          ```

          

11. split 方法

    1. 用匹配的`正则串` 分割字符串 返回列表

       `split(string[, maxsplit])`

       1. maxsplit  指定 最大分割次数

    2. 示例

       ```
       import re
       p = re.compile(r'[\s\,\;]+')
       print (p.split('a,b;; c   d'))  #['a', 'b', 'c', 'd']
       ```

       

12. sub 方法

    1. 替换

       `sub(repl, string[, count]`

       1. repl 字符串 或 函数
          1. 字符串，用 repl 替换字符串每一个匹配的子串, 返回替换后的字符串
             1. repl  用 id 的形式 引用分组，但不能 用编号 0；
          2. 函数， 函数的参数:Match 对象 , 返回一个字符串用于替换 。
       2. count  指定 替换次数，不指定时全部替换

    2. 示例

       ```
       import re
       p = re.compile(r'(\w+) (\w+)') # \w = [A-Za-z0-9]
       s = 'hello 123, hello 456'   #'123', '456'也是字符串
       
       print (p.sub(r'hello world', s))  #用'hello world'替换'hello 123,hello 456'的字串
       print (p.sub(r'\2 \1', s))        # 引用分组
       
       def func(m):
           print('m',m)
           return 'hi' + ' ' + m.group(2) #group(0) 表示本身，group(1)表示hello，group(2) 表示后面的数字
       
       #2次sub，每次sub的结果Match对象传递给func, 第一次逗号前, 第二次逗号后
       print (p.sub(func, s))  
       """
       输出
       m <re.Match object; span=(0, 9), match='hello 123'>
       m <re.Match object; span=(11, 20), match='hello 456'>
       'hi 123, hi 456'
       """
       
       # 指定替换次数: 替换一次
       print (p.sub(func, s, 1))         
       """
       输出
       m <re.Match object; span=(0, 9), match='hello 123'>
       'hi 123, hello 456'
       """
       ```

       

13. 匹配中文

    1. 中文的 unicode 编码范围 主要在 [u4E00-u9FA5], 这个范围并不完整, 如没有包括全角（中文）标点

    2. 示例
    
       ```
       import re
       
       title = '你好，hello，世界'
       pattern = re.compile(r'[\u4e00-\u9fa5]+')
       result = pattern.findall(title)
       
       print (result)  #['你好', '世界']
       ```
    
       1. 正则字符串前的前缀 `u, r`，`r ` 原始字符串,  `u ` unicode 字符串
    
14. 注意：贪婪模式与非贪婪模式

    1. 贪婪模式: 尽可能 多 的匹配 ( * )；Python里 数量词 默认是 **贪婪**
    2. 非贪婪模式: 尽可能 少 的匹配 ( ? )；
    3. 示例一 ： 源字符串：`abbbc`
       1. 贪婪的数量词的正则表达式 `ab*` ，匹配结果： abbb
          2. `*` 前的一个字符b, 尽可能多匹配 b，所以a后面所有的 b 都出现了
       2. 非贪婪: 数量词加?, 表达式`ab*?`, 匹配结果： a
          2. 即使 有 `*`， `?` 决定了尽可能少匹配 b，所以没有 b
    4. 示例二 ： 源字符串：`aa<div>test1</div>bb<div>test2</div>cc`
       1. 贪婪模式：`<div>.*</div>`,  匹配结果：`<div>test1</div>bb<div>test2</div>`
          2.  匹配到第一个"`</div>`"时已 使整个表达式匹配成功. 贪婪模式，仍向右尝试匹配
       2. 非贪婪模式：`<div>.*?</div>`,  匹配结果：`<div>test1</div>`
          2.  匹配到第一个"`</div>`"时 整个表达式匹配成功. 非贪婪模式，结束匹配，不再向右尝试 
    
15.  正则表达式测试网址    `http://tool.oschina.net/regex/`

#### 2.2 案例： 正则表达式的爬虫

准备目标: 找到 url规律

1. 第一步：获取数据

   1. 思路:  写一个加载页面的方法。

      1. 创建一个文件 duanzi_spider.py, 定义一个Spider类, 添加一个加载页面的成员方法
      
   2. 代码
   
      ```
      #每个网站的中文编码不同，解码html.decode('gbk')的写法不通用，根据网站的编码而异
      class Duanzi_spider():
          def __init__(self):
              self.url = "http://www.neihan8.com/article/list_5_%s.html"
              self.headers = {
                  "User_Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleW\
                  ebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
                  "Accept-Encoding":None,
                  "Accept-Language": "zh-CN,zh;q=0.8"
              }
      
          def load_page(self,url):
                  '''可以复用的页面请求方法
                  '''
                  response = requests.get(url,timeout=10,headers=self.headers)
                  if response.status_code==200:
                          print(response.request.headers)
                          return response.content.decode("gbk")
                  else:
                          raise ValueError("status_code is:",response.status_code)
      ```
   
      1. 每个网站的中文编码不同，解码html.decode('gbk')的写法不通用，根据网站的编码而异
   
2. 第二步：筛选数据

   1. 寻找规则 : 网页，鼠标点击右键 " 查看源代码 " , 寻找规律

   2. 代码

      ```
   def get_content(self,html):
              '''  根据网页内容， 匹配标题和段子内容 '''
              pattern = re.compile(r'<a\shref="/article/\d+\.html">(.*?)</a>.*?<div\sclass="f18 mb20">(.*?)</div>', re.S)
              t = pattern.findall(html)
              #定义list格式存储数据
              result = []
              for i in t:
                  temp = []
                  for j in i:
                  		#re.sub(a, b, c), 用a匹配c,匹配的内容用b代替
                          j = re.sub(r"[<b>|</b>|<br />|<br>|<p>|</p>|\\u3000|\\r\\n|\s]","",j)
                          j = j.replace("&ldqo;",'"').replace("&helli;","...").replace("&dqo;",'"').strip()
                          # j = re.sub(r"[&ldqo;|&dqo;]","\"",j)?
                          # j = re.sub(r"…","...",j)
                          temp.append(j)
                      print(temp)
                  result.append(temp)
          return result
      ```
   
   1. `re.S`, 将 所有的字符串 作为一个整体进行匹配, 没有`re.S` 只匹配一行
   
      3. ```
            import re
         a = """sdfkhellolsdlfsdfiooefo: \n #有 \n 回车换行符
            877898989worldafdsf"""
         b = re.findall('hello(.*?)world',a)
            c = re.findall('hello(.*?)world',a,re.S)
            print ('b is ' , b)  # b is  []
            
            print ('c is ' , c)  # c is  ['lsdlfsdfiooefo:\n877898989']
            
            c = re.findall('hello.*?world',a,re.S)
            print ('c is ' , c)  # c is  ['hellolsdlfsdfiooefo:\n877898989world']
         ```
         
            1. 注意 括号`()` 有无时的区别 
   
   3. 第一页的全部段子，不包含其他信息全部的打印了出来。
   
   1. 简单修改  get_content(), 有很多 `<p>` , `</p>` 很是不舒服
   
      ```
         j = re.sub(r"[<b>|</b>|<br />|<br>|<p>|</p>|\\u3000|\\r\\n|\s]","",j)
      j = j.replace("&ldqo;",'"').replace("&helli;","...").replace("&dqo;",'"').strip()
      ```
      
         

3. 第三步：保存数据

   1. 将得到的每个item 存放在 duanzi.txt 文件

      ```
      def save_content(self,content):
          myFile = open("./duanzi.txt", 'a')
          for temp in content:
              myFile.write("\n"+temp[0]+"\n"+temp[1]+"\n")
              myFile.write("--------------------")
          myFile.close()
      ```

      

4. 第四步：实现循环抓取

   1.  通过参数的传递对 page叠加 遍历内涵段子吧的 内容

      ```
      def run(self):
              i = 1
              while True:
                      html = self.load_page(self.url%i)
                      result = self.get_content(html)
                      print ("按回车继续...")
                      print ("输入 quit 退出")
                      command = input()
                      if (command == "quit"):
                              break
                      i+=1
      ```

      

#### 2.3 XPath与lxml类库

1. 使用xpath流程

   1. 先 HTML文件 转换成 XML文档对象， 后用 XPath 查找 HTML 节点
   2. 在 XPath 中，有七种类型的 节点：元素、属性、文本、命名空间、处理指令、注释以及文档节点（或称为根节点）
   
2. XML: 可扩展 标记语言（EXtensible Markup Language）, 类似 HTML. 宗旨: 传输 数据， 非显示数据 . 标签 自己定义,  有自我 描述性. W3C 的推荐标准. XML的代替者是json

   2. XML 和 HTML 的区别

      1. | 数据格式 |                      描述                       | 设计目标                                                     |
         | -------- | :---------------------------------------------: | ------------------------------------------------------------ |
         | XML      | Extensible Markup Language `（可扩展标记语言）` | `传输`和存储数据，焦点是数据的 内容                          |
         | HTML     | HyperText Markup Language `（超文本标记语言）`  | `显示`数据, 如何更好显示数据。                               |
         | HTML DOM | Document Object Model for HTML `(文档对象模型)` | HTML DOM 可 访问HTML 元素, 文本, 属性。修改和删除内容 ，创建新元素。 |

   3. XML文档示例

      1. ```
         <?xml version="1.0" encoding="utf-8"?>
         
         <bookstore>
         
           <book category="cooking">
             <title lang="en">Everyday Italian</title>  
             <author>Giada De Laurentiis</author>  
             <year>2005</year>  
             <price>30.00</price>
           </book>  
         
           <book category="children">
             <title lang="en">Harry Potter</title>  
             <author>J K. Rowling</author>  
             <year>2005</year>  
             <price>29.99</price>
           </book>  
         
           <book category="web">
             <title lang="en">XQuery Kick Start</title>  
             <author>James McGovern</author>  
             <author>Per Bothner</author>  
             <author>Kurt Cagle</author>  
             <author>James Linn</author>  
             <author>Vaidyanathan Nagarajan</author>  
             <year>2003</year>  
             <price>49.99</price>
           </book>
         
           <book category="web" cover="paperback">
             <title lang="en">Learning XML</title>  
             <author>Erik T. Ray</author>  
             <year>2003</year>  
             <price>39.95</price>
           </book>
         
         </bookstore>
         ```

         

   4. XML的节点关系

      ```
      <?xml version="1.0" encoding="utf-8"?>
      <bookstore>
      <book>
        <title>Harry Potter</title>
        <author>J K. Rowling</author>
        <year>2005</year>
        <price>29.99</price>
      </book>
      </bookstore>
      ```
      
      1. 父（Parent）
         1. 每个元素 及属性都有一个父
         2. 示例: book 是 title、author、year 及 price 元素的父
      2. 子（Children）
         1. 元素(节点)可有零个、一个或多个子
         2. title、author、year、price 元素都是 book 元素的子
      3. 同胞（Sibling）
         1. 有相同的父的节点
         2. title、author、year、 price 元素 是同胞
      4. 先辈（Ancestor）
         1. 节点的父、父的父
         2. title 元素的先辈  book、 bookstore 元素
      5. 后代（Descendant）
         1. 节点的子，子的子
         2. bookstore 的后代是 book、title、author、year 以及 price
      
   4. HTML DOM :  定义了访问和操作HTML文档的 `方法`，用`树结构` 表达 HTML 文档

      2. ```
                            ┌───────────┐  
                            │  Document │
                            └───────────┘
                                  │
                            ┌────────────┐  
                            │Root element│
                            │   <html>   │
                            └────────────┘
                                  │
               ┌─────────────────────────────────┐ 
         ┌─────────┐                        ┌─────────┐
         │ element │                        │ element │           
         │  <head> │                        │  <body> │
         └─────────┘                        └─────────┘
              │                                  │
                                             ┌─────────┐
         ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐
         │ element │    │Attribute│────│ element │────│ element │
         │ <title> │    │ "href"  │    │  <a>    │    │   <h1>  │
         └─────────┘    └─────────┘    └─────────┘    └─────────┘
              │                             │              │
         ┌─────────┐                   ┌─────────┐    ┌──────────┐
         │   Text  │                   │   Text  │    │   Text   │     
         │"Mytitle"│                   │"mylink" │    │"myheader"|
         └─────────┘                   └─────────┘    └──────────┘
         ```
      
         

3. XPath

   1. XPath，  XML Path Language，即XML路径语言， 在XML文档中查找信息的语言， 搜寻XML文档，也适用 HTML文档的搜索

      1. XPath的选择功能十分强大， 提供了 路径选择表达式. 提供了100多个内建函数，用于字符串、数值、时间的匹配以及节点、序列的处理等. 想要定位的节点，都可以用XPath选择

      2. xpath解析原理:

         1.etree对象实例化， 将要解析的页面源码 加载到对象中。

         2.调用etree对象的xpath方法, 其参数: xpath表达式, 实现标签的定位和内容的捕获。

      3. http://www.w3school.com.cn/xpath/index.asp

   2. XPath 开发工具

      1. 开源的XPath表达式编辑工具: `XMLQuire` (XML格式文件可用)
      2. Chrome插件 `XPath Helper`
      3. Firefox插件 `XPath Checker`

   3. XPath路径表达式, 与目录相似, 定位 XML 文档中的节点或者节点集

      1. 常用的路径表达式

         | 表达式   | 描述                                                     | 举例 |
         | -------- | -------------------------------------------------------- | ---- |
         | nodename | 节点名, 取 此节点的所有子节点。                          |      |
         | /        | 取 根节点 。                                             |      |
         | //       | 取所有节点， 不考虑 位置。                               |      |
         | .        | 当前节点。                                               |      |
         | ..       | 当前节点的父节点。                                       |      |
         | @        | 取属性。                                                 |      |
         | text()   | 直接获取标签夹着的内容,  text() 做 xpath表达式中的一部分 |      |
         | .text    | 先获取标签对象, 然后`标签对象.text` 获取元素夹着的内容   |      |

         ```
         div_temp_list = html.xpath("//div[@class='recmd-right']")
         for div_a in div_temp_list:
         	href = div_a.xpath("./a[@class='recmd-content']/@href")

      2. 示例

         ```
         <?xml version="1.0" encoding="utf-8"?>
         <bookstore>
         <book>
           <title>Harry Potter</title>
           <author>J K. Rowling</author>
           <year>2005</year>
           <price>29.99</price>
           <div class="a b c" href="www.abc.com">网站</div>
         </book>
         </bookstore>
         ```
         
         | 路径表达式      | 结果                                                         |
         | --------------- | ------------------------------------------------------------ |
         | bookstore       | 取 bookstore 节点的所有子节点                                |
         | /bookstore      | 取 根节点 bookstore。注释：假如路径起始于正斜杠( / )，此路径 代表 某元素的绝对路径！ |
         | bookstore/book  | 取 bookstore 的 子节点中的所有 book 节点。                   |
         | //book          | 取所有 book 子节点，而不管位置。                             |
         | bookstore//book | 取 bookstore的 后代 的所有 book节点，不管后代在 bookstore 下的位置。 |
         | //@lang         | 取属性名为 lang 的 所有 节点                                 |
         
         1. 节点 包含 元素 属性 

   4. 谓语（Predicates）:  指定的节点， 谓语标记: [] 方括号

      2. 带 谓语 路径表达式

         | 带 谓语 路径表达式                 | 结果: 符合条件的元素(节点)                                   |
         | ---------------------------------- | ------------------------------------------------------------ |
         | /bookstore/book[1]                 | 取属于 bookstore 的子节点的`第一个` book节点。               |
         | /bookstore/book[last()]            | 取属于 bookstore 的 子元素的`最后一个` book 元素。           |
         | /bookstore/book[last()-1]          | 取属于 bookstore 子元素的`倒数第二个` book 元素。            |
         | /bookstore/book[position()<3]      | 取 bookstore的子元素book的 最前两个 元素。                   |
         | //title[@lang]                     | 带 lang 属性的所有 title 元素。                              |
         | //title[@lang=’eng’]               | lang 属性 值为 eng 的所有 title 元素                         |
         | /bookstore/book[price>35.00]       | bookstore 节点的所有子节点 book ，且book的 price 元素的值大于 35.00。  备注: price 是元素 |
         | /bookstore/book[price>35.00]/title | 选取 bookstore 元素中的 book 元素的所有 title 节点，且book的 price 元素的值须大于 35.00。 |
         | /div[contains(@class,'a')]         | div节点集合中class属性值包含"a"的div节点                     |

   5. XPath 通配符:  选取未知的 XML 节点

      1. | 通配符 | 描述     |
         | ------ | -------- |
         | *      | 所有节点 |
         | @*     | 所有属性 |
         | node() | 任意节点 |

      2. 未知节点 路径表达式

         | 路径表达式          | 结果                                           |
         | ------------------- | ---------------------------------------------- |
         | /bookstore/*        | 选取 bookstore 元素的 所有子节点               |
         | //*                 | 选取文档中 所有节点                            |
         | html/node()/meta/@* | 选择html下面任意节点下的meta节点的所有属性的值 |
         | //title[@*]         | 选取带任意属性的 所有title 节点                |

   6. 选取若干路径

      1. `|`  运算符，并集关系,  选取若干个路径

      2. 若干路径 路径表达式

         | 路径表达式                       | 结果                                                         |
         | -------------------------------- | ------------------------------------------------------------ |
         | //book/title \| //book/price     | 选取 book 元素的所有 title 和 price 元素。                   |
         | //title \| //price               | 选取文档中的所有 title 和 price 元素。                       |
         | /bookstore/book/title \| //price | 选取属于 bookstore 元素的 book 元素的所有 title 元素，和 文档中所有的 price 元素。 |

   7. XPath的运算符

      | 运算符 | 描述       | 实例                      | 返回值                                                       |
      | :----- | :--------- | :------------------------ | :----------------------------------------------------------- |
      | \|     | 两节点并集 | //book \| //cd            | 返回所有拥有 book 和 cd 元素的节点集                         |
      | +      | 加法       | 6 + 4                     | 10                                                           |
      | -      | 减法       | 6 - 4                     | 2                                                            |
      | *      | 乘法       | 6 * 4                     | 24                                                           |
      | div    | 除法       | 8 div 4                   | 2                                                            |
      | =      | 等于       | price=9.80                | 如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。 |
      | !=     | 不等于     | price!=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
      | <      | 小于       | price<9.80                | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
      | <=     | 小于或等于 | price<=9.80               | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
      | >      | 大于       | price>9.80                | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
      | >=     | 大于或等于 | price>=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。 |
      | or     | 或         | price=9.80 or price=9.70  | 如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。 |
      | and    | 与         | price>9.00 and price<9.90 | 如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。 |
      | mod    | 余数       | 5 mod 2                   | 1                                                            |

4. lxml

   1. Python HTML/XML 解析库，解析和提取 HTML/XML 数据

      1. 用 C 实现的，`lxml`利用`XPath`语法 定位 元素 节点, lxml python 官方文档：http://lxml.de/index.html
      2. 安装lxml库, pip 安装：`pip install lxml` , 或wheel方式安装

   2. 使用 lxml库 的 etree模块的方法

      1. html = etree.HTML(text)    #将字符串解析lxml的文本实例, 返回对象, 自动补全标签
      2. result = etree.tostring(html)   #按字符串序列化HTML实例, 输出字符串 输出类型是bytes
      2. 读取之后, 还要decode()将其转化为str类型的方便我们查看
      2. HTML和etree.tostring 前后衔接的关系, HTML转换后可能出现部分错误，tostring进行修正并且读取

   3. 示例: 直接读取字符串, 用 lxml解析 HTML 代码

      ```
      # lxml_test.py
      
      # 使用 lxml 库 的 etree 模块
      from lxml import etree
      
      text = '''
      <div>
          <ul>
               <li class="item-0"><a href="link1.html">first item</a></li>
               <li class="item-1"><a href="link2.html">second item</a></li>
               <li class="item-inactive"><a href="link3.html">third item</a></li>
               <li class="item-1"><a href="link4.html">fourth item</a></li>
               <li class="item-0"><a href="link5.html">fifth item</a> # 注意，此处缺少一个 </li> 闭合标签
           </ul>
       </div>
      '''
      
      #利用etree.HTML，将字符串解析为HTML文档
      html = etree.HTML(text)
      
      # 按字符串序列化HTML文档
      result = etree.tostring(html)
      
      print(result)
      ```

      输出结果:

      ```
      <html><body>
      <div>
          <ul>
               <li class="item-0"><a href="link1.html">first item</a></li>
               <li class="item-1"><a href="link2.html">second item</a></li>
               <li class="item-inactive"><a href="link3.html">third item</a></li>
               <li class="item-1"><a href="link4.html">fourth item</a></li>
               <li class="item-0"><a href="link5.html">fifth item</a></li>
      </ul>
      </div>
      </body></html>
      ```

      

   4. 文件读取

      1. etree从文件 读取内容

         1. etree.parse('./hello.html')   #读取外部文件 hello.html

      2. hello.html文件

         ```
         <!-- hello.html -->
         
         <div>
             <ul>
                  <li class="item-0"><a href="link1.html">first item</a></li>
                  <li class="item-1"><a href="link2.html">second item</a></li>
                  <li class="item-inactive"><a href="link3.html"><span class="bold">third item</span></a></li>
                  <li class="item-1"><a href="link4.html">fourth item</a></li>
                  <li class="item-0"><a href="link5.html">fifth item</a></li>
              </ul>
          </div>
         ```

      3. 编写代码, 读取html

         ```
         `#lxml_parse.py
         
         from lxml import etree
         
         # 读取外部文件 hello.html, 并解析为HTML实例
         html = etree.parse('./hello.html')
         #参数pretty_print格式化打印
         result = etree.tostring(html, pretty_print=True)  
         
         print(result)
         ```
         
         输出结果与之前相同
         
         ```
         <html><body>
         <div>
             <ul>
                  <li class="item-0"><a href="link1.html">first item</a></li>
                  <li class="item-1"><a href="link2.html">second item</a></li>
                  <li class="item-inactive"><a href="link3.html">third item</a></li>
                  <li class="item-1"><a href="link4.html">fourth item</a></li>
                  <li class="item-0"><a href="link5.html">fifth item</a></li>
         	</ul>
         </div>
         </body></html>
         ```
         
         

5. XPath实例测试

   1. hello.html 文档

      ```
      <!-- hello.html -->
      
      <div>
          <ul>
               <li class="item-0"><a href="link1.html">first item</a></li>
               <li class="item-1"><a href="link2.html">second item</a></li>
               <li class="item-inactive"><a href="link3.html"><span class="bold">third item</span></a></li>
               <li class="item-1"><a href="link4.html">fourth item</a></li>
               <li class="item-0"><a href="link5.html">fifth item</a></li>
           </ul>
       </div>
      ```

   2. Xpath实测

      1. 获取所有的 `<li>` 标签

         ```
         #xpath_li.py
         from lxml import etree
         
         html = etree.parse('hello.html')
         print(type(html))  # 显示etree.parse() 返回类型
         
         result = html.xpath('//li')   #返回<li>标签对象的列表
         print(result)  
         print(len(result))
         print(type(result))
         print(type(result[0]))  #list的每个元素类型<class 'lxml.etree._Element'>
         ```
         
         输出结果：

         ```python
      <class 'lxml.etree._ElementTree'>
         [<Element li at 0x37030c0>, <Element li at 0x3703180>, <Element li at 0x37031c0>, <Element li at 0x3703200>, <Element li at 0x3703240>]
         5
         <class 'list'>
         <class 'lxml.etree._Element'>
         ```
         
         

      2. 获取`<li>` 节点的所有class属性的值

         ```python
         #xpath_li.py
         from lxml import etree
         
         html = etree.parse('hello.html')
         result = html.xpath('//li/@class')
         
         print(result)  #属性值组成的list,['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0']
         print(type(result))  #<class 'list'>
         print(type(result[0]))  #<class 'lxml.etree._ElementUnicodeResult'>
      
      3. 获取`<li>`标签下`hre`f 为 `link1.html` 的 `<a>` 标签的对象组成list
      
         ```python
         #xpath_li.py
            
         from lxml import etree
            
         html = etree.parse('hello.html')
         result = html.xpath('//li/a[@href="link1.html"]')
         
         print(result)   #[<Element a at 0x3b01140>]
         print(type(result))    #<class 'list'>
         print(type(result[0]))   #<class 'lxml.etree._Element'>
      
      
      
      4. 获取`<li>` 标签下的所有 `<span>` 标签的对象组成list
      
         ```python
         #xpath_li.py
         from lxml import etree
         
         html = etree.parse('hello.html')
         result = html.xpath('//li//span')
         
         #result = html.xpath('//li/span'),这么写是不对的：
         #/span取子元素<span>
         
         print(result)   #[<Element a at 0x3b01140>]
         print(type(result))    #<class 'list'>
         print(type(result[0]))   #<class 'lxml.etree._Element'>
      
      5. 获取 `<li>` 节点下的`<a>`节点下的带class属性的节点的class属性值组成的list
      
         ````
         #xpath_li.py
         
         from lxml import etree
         
         html = etree.parse('hello.html')
         result = html.xpath('//li/a//@class')
         
         print(result)    #['bold']
         print(type(result))    #<class 'list'>
         print(type(result[0]))    #<class 'lxml.etree._ElementUnicodeResult'>
      
      6. 获取最后一个 `<li>` 的 `<a>` 的 href的属性值的list
      
         ```
         #xpath_li.py
         
         from lxml import etree
         
         html = etree.parse('hello.html')
         result = html.xpath('//li[last()]/a/@href')
         # 谓语 [last()] 可以找到最后一个元素
         
         print(result)    #['link5.html']
         print(type(result))    #<class 'list'>
         print(type(result[0]))    #<class 'lxml.etree._ElementUnicodeResult'>
      
      7. 获取倒数第二个元素的内容
      
         1. `.text` ,  xpath返回两个标签夹着的内容的对象组成的list
      
            1. ```
               # xpath_li.py
               
               from lxml import etree
               result = html.xpath('//li[last()-1]/a')
               
               print(result)    #[<Element a at 0x39210c0>]
               print(type(result))    #<class 'list'>
               print(type(result[0]))    #<class 'lxml.etree._Element'>
               
               # .text属性可以获取元素内容
               print(result[0].text)    #fourth item
               ```
      
         2. `text()` ,  xpath返回两个标签夹着的内容的对象组成的list
      
            1. ```
               result = html.xpath('//li[last()-1]/a/text()')
               
               print(result)    #['fourth item']
               print(type(result))    #<class 'list'>
      
      
      8. `.tag`属性, 获取 `class` 值为 `bold` 的标签名
      
         ```python
         #xpath_li.py
         
         from lxml import etree
         
         html = etree.parse('hello.html')
         result = html.xpath('//*[@class="bold"]')
         print(result)    #[<Element span at 0x3662040>]
         print(type(result[0]))    #<class 'lxml.etree._Element'>
         
         #.tag属性可以获取标签名
         
         print(result[0].tag)    #span
         print(type(result))    #<class 'list'>
      
      

#### 2.4 案例:XPath的爬虫

 1. XPath 做 简单的爬虫， 爬取某个贴吧 ,并将 每个楼层 的图片下载到本地

    ```python
    #coding=utf-8
    import requests
    from lxml import etree
    import json
    
    class Tieba:
    
        def __init__(self,tieba_name):
            self.tieba_name = tieba_name #接收贴吧名
            #设置为手机端的UA
            self.headers = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1"}
    	
        #构建url, append到list, 
        def get_total_url_list(self):
            '''获取所有的urllist'''
            url = "https://tieba.baidu.com/f?kw="+self.tieba_name+"&ie=utf-8&pn={}&"
            #list格式存储代爬取的url
            url_list = []
            for i in range(100): #通过循环拼接100个url
                url_list.append(url.format(i*50))
            return url_list #返回100个url的urllist
    
        def parse_url(self,url):
            '''一个发送请求，获取响应，etree处理html'''
            print("parsing url:",url)
            
            #发送请求, 返回响应
            response = requests.get(url,headers=self.headers,timeout=10) #发送请求
            #decode() 解码
            html = response.content.decode() #获取html字符串
            html = etree.HTML(html) #element的字符串类型解析为html类型
            return html
    
        def get_title_href(self,url):
            '''获取一个页面的title和href'''
            html = self.parse_url(url)
            #定位并获取想要的节点
            li_temp_list = html.xpath("//li[@class='tl_shadow']") #分组，按照li标签分组
            total_items = []
            for i in li_temp_list: #遍历分组
                #if...else...守备语句
                href = "https:"+i.xpath("./a/@href")[0] if len(i.xpath("./a/@href"))>0 else None
                text = i.xpath("./a/div[1]/span[1]/text()")
                text = text[0] if len(text)>0 else None
                item = dict(  #放入字典
                    href = href,
                    text = text
                )
                total_items.append(item)
            return total_items #返回一个页面所有的item
    
        def get_img(self,url):
            '''获取一个帖子里面的所有图片'''
            html = self.parse_url(url) #返回elemet类型的html，具有xpath方法
            img_list = html.xpath('//div[@data-class="BDE_Image"]/@data-url')
            img_list = [i.split("src=")[-1] for i in img_list] #提取图片的url
            img_list = [requests.utils.unquote(i) for i in img_list]
            return img_list #url的list
    
        def save_item(self,item):
            '''保存一个item'''
            with open("teibatupian.txt","a") as f:
                f.write(json.dumps(item,ensure_ascii=False,indent=2))
                f.write("\n")
    
        def run(self):
            #1、找到了url规律，url list
            url_list = self.get_total_url_list()
            for url in url_list:
            #2、遍历urllist 发送请求，获得响应，etree处理html
            # 3、提取title，href
                total_item = self.get_title_href(url)
                for item in total_item:
                    href = item["href"]
                    img_list = self.get_img(href) #获取到了帖子的图片url的列表
                    item["img"] = img_list
                    # 4、保存到本地
                    print(item)
                    self.save_item(item)
    
    if __name__ == "__main__":
        tieba = Tieba("猫")
        tieba.run()
    ```

#### 2.5 json与jsonpath

1. json : (JavaScript Object Notation)  轻量级的 数据交换 格式, 数据交互: 如网站 前台与后台 间的数据交互. json和xml 不相上下, Python 2.7中自带了json包

   1. `import json`
   2. json文档的数据结构:  javascript中的 `对象`  `数组`,  
   
      1. 对象 : js中 `{ }`括起的 `{ key：value, key：value, ... }`  元素是键值对
         1. 面向对象的语言中，key 对象的属性，value 属性值
         2. `对象.key`   获取属性值
         3. 属性值的类型: 数字、字符串、false, true, 数组、对象
      2. 数组: 中括号`[ ]`括起的数据结构: `["Python", "javascript", "C++", ...]`，
         1. `索引`取值
         2. 属性值的类型: 数字、字符串、false, true, 数组、对象
      3. json文档的读出内容都是字符串, 最外层的引号在 json文档中看不到
   
   
   
2. import json

   1. json包, 提供 四个方法：`dumps()`、`dump`()、`loads`()、`load()`，

      1. 实现json格式 和 python数据类型 转换

   2. json.loads()

      1. Json格式 解码转换为 Python对象,   json字符串 --> python类型 

      2. python类型与 json类型对照表 , 

         | Python 类型      | JSON 类型 |
         | :--------------- | :-------- |
         | dict             | object    |
         | list, tuple      | array     |
         | str, unicode     | string    |
         | int, long, float | number    |
         | True             | true      |
         | False            | false     |
         | None             | null      |

      3. ```
         #json_loads.py
         
         import json
         
         strList = '[1, 2, 3, 4]'  #json字符串
         
         strDict = '{"city": "北京", "name": "大猫"}'  #json字符串
         
         json.loads(strList)
         # [1, 2, 3, 4]
         
         json.loads(strDict) #
         #{'city': '北京', 'name': '大猫'}
         ```

         1. json数据自动按Unicode存储

   3. json.dumps()

      1. python类型 转化为 json字符串,  python类型 --> json字符串

      2. ```
         # json_dumps.py
         
         import json
         import chardet
         
         listStr = [1, 2, 3, 4]
         tupleStr = (1, 2, 3, 4)
         dictStr = {"city": "北京", "name": "大猫"}
         
         json.dumps(listStr)
         # '[1, 2, 3, 4]'
         json.dumps(tupleStr)
         # '[1, 2, 3, 4]'
         
         # 注意：json.dumps() 序列化为json格式, 默认ascii编码, 以\\u开头
         # 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码
         # chardet.detect()返回字典, 其中confidence是检测精确度
         
         json.dumps(dictStr)
         # '{"city": "\\u5317\\u4eac", "name": "\\u5927\\u5218"}'
         
         chardet.detect(json.dumps(dictStr))
         # {'confidence': 1.0, 'encoding': 'ascii'}
         
         json.dumps(dictStr, ensure_ascii=False)
         # {"city": "北京", "name": "大刘"}
         
         chardet.detect(json.dumps(dictStr, ensure_ascii=False))
         # {'confidence': 0.99, 'encoding': 'utf-8'}
         ```

         1. json.dumps() 序列化为json格式, 默认 ascii编码
            1. 参数: ensure_ascii=False, 禁用ascii编码，按utf-8编码
         2. `chardet` 编码识别模块，可通过pip安装
            1. `chardet.detect()`

   4. json.dump()

      1. Python格式 序列化为 json格式,  并写入文件

      2. ```
         #json_dump.py
         
         import json
         
         listStr = [{"city": "北京"}, {"name": "大刘"}]
         json.dump(listStr, open("listStr.json","w"), ensure_ascii=False)
         
         dictStr = {"city": "北京", "name": "大刘"}
         json.dump(dictStr, open("dictStr.json","w"), ensure_ascii=False)
         ```

         

   5.  json.load()

      1. 读取json文件, json格式转化为 python格式

      2. ```
         #json_load.py
         
         import json
         
         strList = json.load(open("listStr.json"))
         print strList
         
         # [{u'city': u'\u5317\u4eac'}, {u'name': u'\u5927\u5218'}]
         
         strDict = json.load(open("dictStr.json"))
         print strDict
         # {u'city': u'\u5317\u4eac', u'name': u'\u5927\u5218'}
         ```

         

3. jsonpath（了解）

   1. jsonpath : 提取JSON文档的模块(py模块)，从 JSON文档抽取信息的工具

      1. 多种语言版本，包括：Javascript, Python, PHP, Java。jsonpath 对 JSON, 相当于 xpath对 XML。
      2. 下载地址：[https://pypi.python.org/pypi/jsonpath](https://pypi.python.org/pypi/jsonpath/)
      3. 安装：点击 `Download URL` 下载 jsonpath，解压 执行`python setup.py install`
      
   2. JsonPath与XPath语法对比

      |   xpath   | jsonpath   | 描述                                                         |
      | :-------: | ---------- | ------------------------------------------------------------ |
      |    `/`    | `$`        | 根节点                                                       |
      |    `.`    | `@`        | 当前节点                                                     |
      |    `/`    | `.`or`[]`  | 子节点                                                       |
      |   `..`    | n/a 不支持 | 取父节点，Jsonpath不支持                                     |
      |   `//`    | `..`       | 任意位置， 符合条件的元素                                    |
      |    `*`    | `*`        | 任意元素                                                     |
      |    `@`    | n/a 不支持 | 指定属性，Json不支持，因为Json是个Key-value递归结构，不需要。 |
      |   `[]`    | `[]`       | 谓语标识, 迭代器标识（简单的迭代操作，如数组下标，选值）     |
      |    \|     | `[,]`      | 迭代器中做多选。                                             |
      |   `[]`    | `?()`      | 过滤操作.                                                    |
      | n/a不支持 | `()`       | 表达式计算                                                   |
      |   `()`    | n/a 不支持 | 分组，JsonPath不支持                                         |
   
   3. 拉勾网城市json文件 http://www.lagou.com/lbs/getAllCitySearchLabels.json 为例，获取所有城市

      ```python
      # jsonpath_lagou.py
      
      import requests
      import jsonpath
      import json
      import chardet
      
      url = 'http://www.lagou.com/lbs/getAllCitySearchLabels.json'
      response = requests.get(url)
      html = response.text
      
      #json字符串 转换成 python对象
      jsonobj = json.loads(html)
      
      # 从根节点开始，匹配name节点, 返回list类型
      citylist = jsonpath.jsonpath(jsonobj,'$..name') 
      
      print (citylist)
      print (type(citylist))
      fp = open('city.json','w')
      
      content = json.dumps(citylist, ensure_ascii=False)
      print (content)
      
      fp.write(content)
      fp.close()
      ```
   
      1. jsonpath.jsonpath(jsonobj,'$..name')   #返回list类型

#### 2.6 糗事百科案例

1. 项目分析

   1. 爬取糗事百科段子
   2. 假设页面的URL是 http://www.qiushibaike.com/8hr/page/1

2. 要求：

   1. requests获取页面信息，xpath / re 提取数据
   2. 获取每个帖子里的`用户头像链接`、`用户姓名`、`段子内容`、`点赞次数`和`评论次数`
   3. 保存 `json` 文件内

3. 代码

   ```
   #coding=utf-8
   import requests
   #从retrying(py模块)中导入retry函数
   from retrying import retry
   from lxml import etree
   
   class Qiubai_spider():
       def __init__(self):
           self.url = "http://www.qiushibaike.com/8hr/page/{}/"
           self.headers = {
               "User-Agent":"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1 Trident/5.0;"
           }
   
       @retry(stop_max_attempt_number=5) #调用retry，当assert出错时候，重复请求5次
       def parse_url(self,url):
           response = requests.get(url,timeout=10,headers=self.headers) #请求url
           assert response.status_code==200  #当响应码不是200时候，做断言报错处理
           print(url)
           return etree.HTML(response.text) #返回etree之后的html
   
       def parse_content(self,html):
           item_temp = html.xpath("//li[@class='item typs_video']" )
           print(len(item_temp))
           for item in item_temp:
               #获取用户头像地址
               avatar = item.xpath("./div[1]/a[1]/img/@src")[0] if len(item.xpath("./div[1]/a[1]/img/@src"))>0 else None
               #为头像地址添加前缀
               if avatar is not None and not avatar.startswith("http:"):
                   avatar = "http:"+avatar
               print(avatar)
   			
   			#获取用户名
               name = item.xpath("./div/div/a/span/text()")[0] 
               print(name)
               
               #获取内容
               content = item.xpath("./a[@class='contentHerf']/div/span/text()")[0] 
               print(content)
               
               #获取点赞数
               star_number = item.xpath("./div[@class='stats']/span[1]/i/text()")[0] 
               print(star_number)
               
               #获取评论数
               comment_number = item.xpath("./div[@class='stats']/span[2]/a/i/text()")[0] 
               print(comment_number)
               print("*"*100)
   
       def run(self):
           '''函数的主要逻辑实现
           '''
           url = self.url.format(1) #获取到url
           html = self.parse_url(url) #请求url
           self.parse_content(html) #解析页面内容并把内容存入内容队列
   
   if __name__ == "__main__":
       qiubai = Qiubai_spider()
       qiubai.run()
   ```

   1. text()  #获取标签夹着的内容, 是 xpath表达式中的一项

4. etree.HTML, html.xpath返回值类型

   1. ```
      response = requests.get(url,timeout=10,headers=headers)
      
      html = etree.HTML(response.text)   #返回类型是 <class 'lxml.etree._Element'>
      
      #返回类型<class 'list'>, 每个元素的类型是<class 'lxml.etree._Element'>
      item_temp = html.xpath("//li[@class='item typs_video']")
      for item in item_temp:
      	#获取用户头像地址
      	avatar = item.xpath("./div/div/a/img/@src")[0] if len(item.xpath("./div/div/a/img/@src"))>0 else None
      	print(type(avatar))  #<class 'lxml.etree._ElementUnicodeResult'>
      ```

   2. 标签li的class属性有两个值item typs_video, 若只写一个, 将不能定位, 必须写两个


#### 2.7 多线程爬虫案例

1. queue（队列对象）

   1. python 多线程 : `加锁` 重要的环节, 原生的list,dict 是非线程安全的

   2. queue: python的标准库,  线程安全的, 常用于线程间 交换数据
   
      1. from queue import Queue 引用
      2. queue1 = Queue()
      
   3. 模块的 方法
   
      1. queue1.qsize()   返回队列的大小
   
      2. queue1.empty()   如果队列为空，返回True,反之False
   
      3. queue1.full()   如果队列满了，返回True,反之False
   
      4. queue1.full  与  maxsize 大小对应
   
      5. queue1.get([block[, timeout]])   #获取队列，timeout等待时间
   
      6. 创建一个“队列”对象
   
         ```
         import queue
         myqueue = queue.queue(maxsize = 10)
         ```

      7. 一个值 放入 队列实例中: `myqueue.put(10)`
   
      8. 一个值从队列实例中 取出  : `myqueue.get()`
   
      9.  队列实例 myqueue.task_done()  #task_done的时候，队列计数减一
   
2. 多线程示意图

   1. pagequeue 网页队列 --> pagequeue.get() 取出队列数据 --> crawlList 爬虫网页 多线程(crawl-1线程, crawl-2线程, crawl-3线程) --> 队列实例data_queue.put(网页源码) --> data_queue队列实例 --> data_queue.get() 队列实例取出网页源码 --> parserthreads  解析网页 多线程(parser-1线程, parser-2线程, parser-3线程) --> 存储json文件(互斥锁) 本地磁盘化存储
   2. ![img](file:///J:/0%20%E7%88%AC%E8%99%AB%20-%20%E9%BB%91%E9%A9%AC/18-20%E7%88%AC%E8%99%AB%E8%AF%BE%E4%BB%B6%20--PPT%2018%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80-%2019MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93-20%E7%88%AC%E8%99%ABScrapy%E6%A1%86%E6%9E%B6%E5%92%8C%E6%A1%88%E4%BE%8B/18-20%E7%88%AC%E8%99%AB%E8%AF%BE%E4%BB%B6V3.1/%E7%88%AC%E8%99%AB%E8%AF%BE%E4%BB%B6/file/images/03-mulithread.png)



3. ```
   # coding=utf-8
   import requests
   from lxml import etree
   import json
   from queue import Queue
   import threading
   
   class Qiubai:
       def __init__(self):
           self.headers = {
               "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWeb\
               Kit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}
           #实现三个队列实例,存放内容
           self.url_queue = Queue()   
           self.html_queue = Queue()
           self.content_queue = Queue()
   
   
       def get_total_url(self):
           '''
           获取了所有的页面url，并且返回urllist
           return ：list
           '''
           url_temp = 'https://www.qiushibaike.com/8hr/page/{}/'
           url_list = []
           for i in range(1,36):
               # url_list.append(url_temp.format(i))
               #url_queue中存放url
               self.url_queue.put(url_temp.format(i))
   
       def parse_url(self):
           ''' 一个发送请求，获取响应，同时etree处理html '''
           while self.url_queue.not_empty:
           	#url_queue中获取url
               url = self.url_queue.get()
               print("parsing url:",url)
               response = requests.get(url,headers=self.headers,timeout=10) #发送请求
               html = response.content.decode() #获取html字符串
               html = etree.HTML(html) #获取element 类型的html
               #html_queue中存放返回的response
               self.html_queue.put(html)
               #从队列中减去一个url
               self.url_queue.task_done()
   
       def get_content(self):
           '''
           :param url:
           :return: 一个list，包含一个url对应页面的所有段子的所有内容的列表
           '''
           while self.html_queue.not_empty:
               html = self.html_queue.get()
               total_div = html.xpath('//div[@class="article block untagged mb15"]') #返回divelememtn的一个列表
               items = []
               for i in total_div: #遍历div标枪，获取糗事百科每条的内容的全部信息
                   author_img = i.xpath('./div[@class="author clearfix"]/a[1]/img/@src')
                   author_img = "https:" + author_img[0] if len(author_img) > 0 else None
                   author_name = i.xpath('./div[@class="author clearfix"]/a[2]/h2/text()')
                   author_name = author_name[0] if len(author_name) > 0 else None
                   author_href = i.xpath('./div[@class="author clearfix"]/a[1]/@href')
                   author_href = "https://www.qiushibaike.com" + author_href[0] if len(author_href) > 0 else None
                   author_gender = i.xpath('./div[@class="author clearfix"]//div/@class')
                   author_gender = author_gender[0].split(" ")[-1].replace("Icon", "") if len(author_gender) > 0 else None
                   author_age = i.xpath('./div[@class="author clearfix"]//div/text()')
                   author_age = author_age[0] if len(author_age) > 0 else None
                   content = i.xpath('./a[@class="contentHerf"]/div/span/text()')
                   content_vote = i.xpath('./div[@class="stats"]/span[1]/i/text()')
                   content_vote = content_vote[0] if len(content_vote) > 0 else None
                   content_comment_numbers = i.xpath('./div[@class="stats"]/span[2]/a/i/text()')
                   content_comment_numbers = content_comment_numbers[0] if len(content_comment_numbers) > 0 else None
                   hot_comment_author = i.xpath('./a[@class="indexGodCmt"]/div/span[last()]/text()')
                   hot_comment_author = hot_comment_author[0] if len(hot_comment_author) > 0 else None
                   hot_comment = i.xpath('./a[@class="indexGodCmt"]/div/div/text()')
                   hot_comment = hot_comment[0].replace("\n：", "").replace("\n", "") if len(hot_comment) > 0 else None
                   hot_comment_like_num = i.xpath('./a[@class="indexGodCmt"]/div/div/div/text()')
                   hot_comment_like_num = hot_comment_like_num[-1].replace("\n", "") if len(hot_comment_like_num) > 0 else None
                   item = dict(
                       author_name=author_name,
                       author_img=author_img,
                       author_href=author_href,
                       author_gender=author_gender,
                       author_age=author_age,
                       content=content,
                       content_vote=content_vote,
                       content_comment_numbers=content_comment_numbers,
                       hot_comment=hot_comment,
                       hot_comment_author=hot_comment_author,
                       hot_comment_like_num=hot_comment_like_num
                   )
                   items.append(item)
               self.content_queue.put(items)
               #task_done的时候，队列计数减一
               self.html_queue.task_done()  
   
       def save_items(self):
           '''
           保存items
           :param items:列表
           '''
           while self.content_queue.not_empty:
               items = self.content_queue.get()
               f = open("qiubai.txt","a")
               for i in items:
                   json.dump(i,f,ensure_ascii=False,indent=2)
                   # f.write(json.dumps(i))
               f.close()
               self.content_queue.task_done()
   
       def run(self):
           #1.第一线程任务获取url list
           #url_list = self.get_total_url()
           #list存储各个线程对象
           thread_list = []
           thread_url = threading.Thread(target=self.get_total_url)
           thread_list.append(thread_url)
           #2.第er线程任务发送网络请求
           for i in range(10):
               thread_parse = threading.Thread(target=self.parse_url)
               thread_list.append(thread_parse)
           #3.第三线程任务提取数据
           thread_get_content = threading.Thread(target=self.get_content)
           thread_list.append(thread_get_content)
           #4.第四线程任务保存
           thread_save = threading.Thread(target=self.save_items)
           thread_list.append(thread_save)
           for t in thread_list:
               t.setDaemon(True)  #为每个线程设置为进程守护，效果是主进程退出子进程也退出
               t.start()          #为了解决程序结束无法退出的问题
           
           # for t in thread_list:
           #     t.join()
   
           self.url_queue.join()   #主线程等待，所有的队列为空的时候才能退出
           self.html_queue.join()
           self.content_queue.join()
   
   if __name__ == "__main__":
       qiubai = Qiubai()
       qiubai.run()
   ```

   1. run中 调用创建各个方法的函数, 
   2. 写方法的方法:
      1. 认为是一个进程的线程中处理任务
   3.  t.setDaemon(True)   #进程守护, 进程设置为后台进程，效果是主进程退出子进程也会退出

4. queue1.task_done() 与 queue1.join()  ,  queue1=Queue()

   1.  queue1.task_done() :  queue1.get()后完成一项工作，queue.task_done()函数向任务已经完成的队列发送一个信号
   2. queue1.join() 函数后面的代码被阻塞，直到队列中所有的元素处理完毕才执行后面的代码。即队列中既没有任务也没有元素时，取消阻塞。
   3. 关系：如 线程每次从队列里get()一次，但没有执行task_done()，则join无法判断队列结束，执行join()等不到结果，会一直挂起


#### 2.8 BeautifulSoup解析器

1. Beautiful Soup: HTML/XML的解析器, 解析和提取 HTML/XML 数据, 和 lxml 一样

   3. BeautifulSoup 解析 HTML 简单, API非常人性化
      2. 支持 `CSS选择器`、Python标准库的 `HTML解析器`， lxml 的 `XML解析器`
   4. 项目用Beautiful Soup 4, `Beautiful Soup3 `  停止开发
      2.  pip 安装 `pip install beautifulsoup4`

2. lxml与Beautiful Soup对比

   1. lxml  局部遍历

   2. Beautiful Soup : 基于HTML DOM，载入整个文档，解析整个DOM树, 因此时间和内存开销都会大很多，性能要低于lxml

   3. |   抓取工具    | 速度 | 使用难度 |  安装难度  |
      | :-----------: | :--: | :------: | :--------: |
      |     正则      | 最快 |   困难   | 无（内置） |
      |     lxml      |  快  |   简单   |    一般    |
      | BeautifulSoup |  慢  |  最简单  |    简单    |

3. 示例：从 bs4包 导入BeautifulSoup类  

   1. ```
      # beautifulsoup4_test.py
      
      from bs4 import BeautifulSoup
      
      html = """
      <html><head><title>The Dormouse's story</title></head>
      <body>
      <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
      <p class="story">Once upon a time there were three little sisters; and their names were
      <a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
      <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
      <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
      and they lived at the bottom of a well.</p>
      <p class="story">...</p>
      """
      
      #创建 Beautiful Soup 对象
      soup = BeautifulSoup(html)
      
      #type(soup)   #<class 'bs4.BeautifulSoup'>
      #print(soup)  #输出对应的字符串, 与html的内容一样
      
      #打开本地 HTML 文件的方式来创建对象
      #soup = BeautifulSoup(open('index.html'))
      
      #格式化输出 soup 对象的内容
      print (soup.prettify())
      ```
      
      
      
      1. `soup = BeautifulSoup(html,“lxml”)`    #指定解析器是lxml,  默认是 BeautifulSoup
   
4. Beautiful Soup的对象: Beautiful Soup把 HTML文档解析成树形结构, 每个节点 是 Python对象.  共4种 Python对象:   `Tag`, `NavigableString`, `BeautifualSoup`, `Comment`

   1. Tag对象: HTML的一个个标签和标签夹的字符串

      1. BeautifulSoup 获取 Tag语法: `BeautifulSoup对象.标签名`,  如 `soup.title`

         ```
         from bs4 import BeautifulSoup
         
         html = """
         <html><head><title>The Dormouse's story</title></head>
         <body>
         	<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
         	<p class="story">Once upon a time there were three little sisters; and their names were
         		<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
         		<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
         		<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
         		and they lived at the bottom of a well.
         	</p>
         	<p class="story">...</p>
         """
         
         #创建 Beautiful Soup 对象
         soup = BeautifulSoup(html)
         
         #type(soup.title)   #<class 'bs4.element.Tag'>
         print (soup.title)
         # <title>The Dormouse's story</title>
         
         print (soup.head)
         # <head><title>The Dormouse's story</title></head>
         
         print (soup.a)
         # <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
         
         print (soup.p)
         # <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
         
         print (type(soup.p))
         # <class 'bs4.element.Tag'>
         ```
      
         1. 符合要求的第一个 的标签,及标签夹的内容
      
      2. Tag对象，有两个属性，是 name 和 attrs, attrs的值是字典类型
      
         ```
         print(soup.name) # [document], soup 对象比较特殊，它的 name 为 [document]
         
         print(soup.head.name)  # head, 其他内部标签，输出的值是标签的名称
         
         print(soup.p.attrs)  # {'class': ['title'], 'name': 'dromouse'}
         # p 的属性的类型是一个字典。
         
         #获取p的class属性的值
         print (soup.p['class']) #['title'],与 soup.p.get('class') 方法等价
         
         #修改p的class属性的值
         soup.p['class'] = "newClass"  #属性值修改
         
         del soup.p['class'] # 删除 p 的属性
         ```
      
         
      
   2. NavigableString对象

      1. 方法:` .string`  #获取标签p夹的的文字 `<p>abc</p>`,   p.string 输出 `abc`

      2. ```
         print (soup.p.string)  # The Dormouse's story
         
         print (type(soup.p.string))  #<class 'bs4.element.NavigableString'>
         ```

         

   3. BeautifulSoup对象

      1. 整个文档对应的对象,  特殊的 Tag.

      2. ```
         print (type(soup.name))  # <type 'unicode'>
         
         print (soup.name ) # [document]
         
         
         print (soup.attrs) # {} 文档本身的属性为空
         ```
         
         

   4. Comment对象 ,用`.string`获取

      1. 一个特殊类型的 `NavigableString `对象, 注释符号的 内容

      2. ```
         print (soup.a)
         # <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
         
         print (soup.a.string)
         # Elsie 
         
         print (type(soup.a.string))
         # <class 'bs4.element.Comment'>
         ```
      
         1. 原理 a 的文本是`<!-- Elsie -->`,  soup.a.string的值是 Elsie, 去掉注释`<!---->`

5. BeautifulSoup的属性: 指定子节点,后代节点

   1. 直接子节点 ：`.contents`,  `.children` 属性
      1. `.contents` ,  返回: 一个 (子节点)的列表, 元素是tag 子节点和子节点夹的内容
      
         1. `print(soup.head.contents) `   #`[<title>The Dormouse's story</title>]`
      
         2. 列表索引 获取它的某 个元素
      
            `print(soup.head.contents[0])`   #`<title>The Dormouse's story</title>`
      
      2. `.children`, 返回:  一个 遍历 (子节点)的迭代器
      
         1. `print( soup.head.children)`   #`<listiterator object at 0x7f71457f5710>`
      
            ```
            for child in  soup.body.children:
                print (child)
            ```
            
            结果:
      
            ```html
            <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
            
            <p class="story">Once upon a time there were three little sisters; and their names were
            <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>,
            <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a> and
            <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>;
            and they lived at the bottom of a well.</p>
            
            <p class="story">...</p>
            ```
      
   2. 所有后代节点: `.descendants` 属性, 返回:  一个 生成器, 包含所有后代节点，用于循环遍历

      2. ```
         for child in soup.descendants:
         	print(child)
         ```
      
         

   3. 节点内容: `.string` 属性

      1. 一个标签里 没有标签 ，` .string` 返回标签里的内容

      2. 标签里 只有一个标签，` .string` 也返回最里面的内容

      3. ```
         # <html><head><title>The Dormouse's story</title></head>
         print(soup.head.string)  #The Dormouse's story
         print(soup.title.string)  #The Dormouse's story
         ```

         

6. BeautifulSoup对象的方法

   1. `find(name, attrs, recursive, text, **wargs)`,  返回第一个匹配到的对象

      1. 参数

         1. | 参数名 | 作用          | 示例                                                         |
               | ------ | ------------- | ------------------------------------------------------------ |
               | name   | 查找标签      | li = soup.find('li');  li.text, li.attrs, li.string          |
               | text   | 查找文本      | 范围指定的文本, 返回要查找的字符串. soup.find(text="first item"), 返回"first item" |
               | attrs  | 基于attrs参数 | 定位tag,  返回tag和tag夹的内容.  attrs的字典的元素不能是 id  |

               用keyword id定位tag, 如 `li = soup.find(id = 'flask')`  用id='flask'定位li标签`
      
      2. class是python的保留关键字，若要匹配 class的属性，有以下两种：
      
            1. attrs属性用字典的方式参数传递, 
      
            2. BeautifulSoup自带的特别关键字`class_`
      
            3. ```
                  #第一种: attrs属性用字典进行传递参数, 定位tag
                  find_class = soup.find(attrs={'class':'item-1'})
                  print('findclass:',find_class,'\n')
                  
                  # 第二种:BeautifulSoup 的特别关键字参数class_
                  beautifulsoup_class_ = soup.find(class_ = 'item-1')
                  print('BeautifulSoup_class_:',beautifulsoup_class_,'\n')
      
   2. `find_all(name, attrs, recursive, text, **kwargs)`  #返回值类型: <class 'bs4.element.ResultSet'>

      1. 参数

         1. | 参数名 | 作用                    |
            | ------ | ----------------------- |
            | name   | 查找标签                |
            | text   | 查找文本                |
            | attrs  | 基于attrs参数, 查找标签 |

      2. name : 传字符串 , name是字符串, 有引号, 查找所有名字为 name 的tag, 

         1. 值: tag的名字 `soup.find_all('b')`   #`[<b>The Dormouse's story</b>]`,  找名字是'b'的标签

         2. 值: 正则对象, re.compile('正则表达式'),   正则表达式的 match() 匹配内容.

            2. 如:  找出以b开头的 所有 标签

               ```
               import re
               for tag in soup.find_all(re.compile("^b")):
                print(tag.name)  #标签的名字body, b
               ```

         3. 值: tag的列表: `soup.find_all(["a", "b"])`,  返回列表, 元素: 标签和标签夹的内容

      3. 用keyword  id定位tag, 只能是id, 不能是class

         1. ```
            soup.find_all(id='link2')
            # [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]

         

      4. text : 搜HTML的text,  值:字符串, 字符串元素的列表 , 正则表达式 .  返回值: list类型, 元素是字符串

         1. 值: 字符串

            `soup.find_all(text="Elsie")`   # [u'Elsie']

         2. 值: 列表

            `soup.find_all(text=["Tillie", "Elsie", "Lacie"])`  # [u'Elsie', u'Lacie', u'Tillie']

         3. 值: 正则表达式

            `soup.find_all(text=re.compile("Dormouse"))`  #[u"The Dormouse's story", u"The Dormouse's story"]

         

   3. CSS选择器: `soup.select()`，参数值: 字符串类型, 值: 标签名, 类名, id名.  返回值类型:  list类型. 与 find_all() 功能相同

      1. 参数特点: 标签名前没有任何修饰,  类名前加`.`， id名前加`#`

      2. 参数值: `'标签名'`

         `soup.select('title') `  #`[<title>The Dormouse's story</title>]`

         `soup.select('b')`   #`[<b>The Dormouse's story</b>]`

      3. 参数值:   `".类名"`,    `soup.select('.sister')`  

         输出: 

         ```
         #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
         ```

         

      4. 参数值:  `"#id 名"`

         `soup.select('#link1')`

         输出:

         ```
         #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
         ```

      5. 参数值: "标签名 类名 id名 "的组合 , 空格分开.  或"标签名>子标签名"

         2. 查找 p 标签中，id 等于 link1的内容， 空格分开

            `soup.select('p #link1')`

            输出:

            ```
            #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]
            ```

         3. `直接子标签`,   返回子标签定位的列表

            `soup.select("head > title")`   #`[<title>The Dormouse's story</title>]`

      6. 谓语查找: 属性 用中括号括起， 

         2. 属性 标签属 同一节点，不加`空格`，否则匹配不到

            `soup.select('a[class="sister"]')`  

         3. 属性 标签属 不是同一个节点, 空格隔开

            `soup.select('p a[href="http://example.com/elsie"]')`

            1. `soup.select('p a[href="http://example.com/elsie"]')`  与 `soup.select('p [href="http://example.com/elsie"]')`  效果一样

      7. get_text() 获取文本  如`<p>abc</p>`的内容是 abc

         1.  select 方法返回列表,  遍历的每个元素. get_text() 方法 获取 文本

         2. `soup.select('title')[0].get_text()`

         3. ```
            for title in soup.select('title'):
                print title.get_text()
            ```

7. 小结: BeautifulSoup的对象 有 find_all(), css的 select() 选择器 两种方法


#### 2.9 案例：用BeautifulSoup的爬虫

 1. 项目分析: 腾讯社招页面来做演示：http://hr.tencent.com/position.php?&start=10#a

 2. 代码

    ```
    #bs4_tencent.py
    
    from bs4 import BeautifulSoup
    import urllib
    import json    #json格式存储
    
    def tencent():
        url = 'http://hr.tencent.com/'
        request = urllib.request.Request(url + 'position.php?&start=10#a')
        response =urllib.request.urlopen(request)
        resHtml = response.read()
        output =open('tencent.json','w')
        html = BeautifulSoup(resHtml,'lxml')
    
        #创建CSS选择器对象, 用标签[谓语]方式定位
        result = html.select('tr[class="even"]')
        result2 = html.select('tr[class="odd"]')
        result += result2
    
        items = []
        for site in result:
            item = {}
    
            name = site.select('td a')[0].get_text()
            detailLink = site.select('td a')[0].attrs['href']
            catalog = site.select('td')[1].get_text()
            recruitNumber = site.select('td')[2].get_text()
            workLocation = site.select('td')[3].get_text()
            publishTime = site.select('td')[4].get_text()
    
            item['name'] = name
            item['detailLink'] = url + detailLink
            item['catalog'] = catalog
            item['recruitNumber'] = recruitNumber
            item['publishTime'] = publishTime
    
            items.append(item)
    
        #禁用ascii编码，按utf-8编码
        line = json.dumps(items,ensure_ascii=False)
    
        output.write(line.encode('utf-8'))
        output.close()
    
    if __name__ == "__main__":
       tencent()
    ```

1. 小结

   1. ```
      soup.select('tr a')[0].get_text()   #html = """
      <html><head><title>The Dormouse's story</title></head>
      <body>
      <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
      <p class="story">Once upon a time there were three little sisters; and their names were
      <tr><a href="http://example.com/elsie" class="sister" id="link1">aaaaa<!-- Elsie --></a>2222222 </tr>
      <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
      <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
      and they lived at the bottom of a well.</p>
      <p class="story">...</p>
      ... """
      
      soup = BeautifulSoup(html)
      soup.select('tr a')[0].get_text() #输出文本aaaaa, 定位标签,获取文本
      soup.select('tr a')[0].attrs['href']  #输出'http://example.com/elsie',获取属性
      ```

      


##### 2 小结 数据提取

1. 数据提取:  从响应中获取 数据 的过程

2. 数据分类

   1. 非结构化数据 : html,  处理方法 : 正则表达式 re 、xpath
   2. 结构化数据 : json, xmI,  处理方法 : 用BeautifulSoup转化 为python数据类型

3. json数据处理方法: 转化为 python 类型 数据,  用于 数据交互, 如 前台 后台 数据交互

   1. json字符串 -- json.loads() --> python数据类型

   2. json字符串 <-- json.dumps() -- python数据类型
      1. `json.dumps(ret1, ensure_ascii=False, indent=4)`

   3. 包含json的类文件 对象 -- json.load() --> python数据类型

   4. 包含json的类文件 对象 <-- json.dump() -- python数据类型
      1. 有 read() 或 write() 方法的对象是 类文件对象
      2. f = open("a.txt", "r")  # f 是类文件对象

   5. 小结: 为什么要 json.dumps(), 从python类型转json类型. 原因: str 操作不丰富, eval()  处理不了 嵌套

      

4. 寻找返回json的url步骤:
   1. 使用 chrome 切换到 手机页面
   2. 抓包手机app的 软件

5. 正则表达式 : 定义: 用"规则字符串" 过滤 字符串.  规则字符串 是 逻辑

   2. 常用方法:
      1. re.compile ( 编译)
      2. pattern.match (从头找一个 )
      3. pattern.search (找-个)
      4. pattern.findall (找所有)
      5. pattern.sub (替换)
   3. 正则表达式:  ① 字符 ② 预定义字符集, `[]` ③ 数量词, 字符或`(...)`之后 
   4. python 的原始字符串 `r`
      1. `r'\nab'` == `"\\nab"`
      2. `re.match(r'\nab', "\nab").group()`   #输出`\nab`
      3. 正则表达式 尽量用 原始字符串
         1. 原始字符串, 按 字面意思使用
         2. windows 不使用 原始字符串 出问题
   5. 小结
      1. `re.findall(r"a.*bc" ,”"a\nbc" ,re.DOTALL)` 和 `
         re.findall(r"a(.* )bc" ,"a\nbc" ,re.DOTALL)` 的区别?
         不分组时匹配的是全部， 分组后匹配的是组内的内容

6. xpath, lxml

   1. xpath: 在 HTML\XML 文档中查找信息的 语言, 遍历 元素和属性

      1. xpath 节点选择 工具

         1. `Chrome` 插件 XPath Helper
         2. xpath表达式 编辑工具 : XMLQuire (XML格式文件可以)
         3. Firefox插件 XPath Checker

      2. 节点选择 语法

         1. 路径表达式  选取XML文档中的 节点 或者 节点集. 与 电脑文档路径 类似

         2. | 节点或属性 表达式 | 描述                                            |
            | ----------------- | ----------------------------------------------- |
            | `nodename`        | 选取此节点的所有子节点                          |
            | `/`               | 从根节点选取                                    |
            | `//`              | 从 匹配 的当前节点选择文档中的节点， 不考虑位置 |
            | `.`               | 选取当前节点                                    |
            | `..`              | 选取当前节点的父节点                            |
            | `@`               | 选取属性                                        |

            1. chrome插件选择标签, 选中的标签 添加属性class="xh-highlight"

         3. <table>
                <tr><td colspan='2'>选择 某个特定的节点或者包含某个指定的值的节点</td></tr>
                <tr><td>路径表达式 </td><td>结果</td></tr>
                <tr><td> /bookstore/book[1] </td><td>选取bookstore元素的第一个book元素</td></tr>
                <tr><td>/bookstore/book[last()]</td><td>选取bookstore元素的最后一个book元素</td></tr>
                <tr><td>/bookstore/book[last()-1]</td><td>选取bookstore元素的倒数第二个book元素</td></tr>
                <tr><td>/bookstore/book[position()<3]</td><td>选取bookstore 元素的最前面的两个book元素</td></tr>
                <tr><td>//title[@lang]</td><td>选取所有属性名为lang的的title元素</td></tr>
                <tr><td>//title[@lang='eng']</td><td>选取有属性lang且值为eng的所有title 元素</td></tr>
                <tr><td>/bookstore/book[price>35.00]</td><td>选取bookstore 元素的所有book元素，且其中的price元素的值须大于35.00</td></tr>
                <tr><td>/bookstore/book[price>35.00/title</td><td>选取bookstore元素中的book元素中的所有title 元素，且price元素的值 大于35.00</td></tr>
            </table>

         4. <table>
                <tr><td colspan=2>选择 未知 节点</td></tr>
                <tr><td>通配符</td><td>描述</td></tr>
                <tr><td>*</td><td>匹配任何元素节点</td></tr>
                <tr><td>@*</td><td>匹配任何属性节点</td></tr>
                <tr><td>node()</td><td>匹配任何类型的节点</td></tr>
            </table>

         5. <table>
                <tr><td colspan=2>表格中 列出了一些路径表达式，及结果</td></tr>
                <tr><td>路径表达式</td><td>结果</td></tr>
                <tr><td>/bookstore/*</td><td>选取bookstore元素的所有子元素</td></tr>
                <tr><td>//*</td><td>选取文档中的所有元素</td></tr>
                <tr><td>html/node()/meta/@*</td><td>选择html下的任意节点下的meta节点的所有属性</td></tr>
                <tr><td>//title[@*]</td><td>选取所有带有属性的title元素</td></tr>
            </table>

         6. <table>
                <tr><td colspan=2>选取 若干 路径</td></tr>
                <tr><td>路径表达式</td><td>结果</td></tr>
                <tr><td>//book/title | //book/price</td><td>选取book元素的所有title 和 price元素</td></tr>
                <tr><td>//title | //price</td><td>选取文档中的所有title 和 price元素</td></tr>
                <tr><td>/bookstore/book/title | //price</td><td>选取属于bookstore元素的book元素的所有title 元素，以及文档中所有的price元素</td></tr>
            </table>

   2. XML

      1. | 数据格式 | 描述                                        | 设计目标                            |
         | -------- | ------------------------------------------- | ----------------------------------- |
         | XML      | Extensible Markup Language (可扩展标记语言) | 传输和存储 数据，其焦点是数据的内容 |
         | HTML     | HyperText Markup Language (超文本标记语言)  | 显示数据 及如何更好 显示数据        |

      2. XML的节点: XML的每个 标签 叫节点

         1. ```
            <book>
            	<title>Harry Potter</title>
            	<author>J K. Rowling</author>
            	<year>2005</year>
            	<price>29.99</price>
            </book>
            ```

      3. 节点关系:

         1. parent 父节点. book元素是title, author, yea, price元素的父节点
         2. children 子节点. title, author, year, price元素是book元素的子节点
         3. sibling 同胞节点. title, author, year, price 是同胞节点
         4. ancestor  祖先. Year节点的先辈是book节点
         5. descendant  后代. book的后代是title, author, year, price元素

   3. lxml库:  Python HTML/XML解析器,  用 xpath() 定位元素, 获取节点信息

      1. 导入lxml 的 etree 库 `from lxml import etree`

         1. lxml 自动修正 html 代码

      2. 利用 `etree.HTML()`，将 字符串 转化为 Element 对象

      3. Element对象具有xpath的方法

         1. ```
            html = etree.HTML(text)  #字符串 转为 Element 对象
            htmL.xpath()
            ```

   4. 单线程爬虫, 多线程爬虫

         1. 单线程爬虫: url list --> 发送请求, 获取响应 --> 提取数据 --> 保存
         2. 多线程爬虫: url list --> url 队列 --> 发送请求, 获取响应 --> 响应队列 --> 提取数据 --> 数据队列 --> 保存

   5. 爬虫套路

         1. 准备url
               1. 情景: url 地址不明显, 总数不确定. 准备 start_url, 通过下面的方式用代码提取下一页的url
                     1.  ①xpath .  ②寻找 url 地址。部分参数在当前的响应中( 如当前页码数和总的页码数)
               2. 情景: 页码总数 明确, url 地址规律明显. 准备url_list .  
         2. 发送请求, 获取响应
                   1.  反反爬虫: 增加随机的 user-Agent,IP.
                   2.  对方判出我们是爬虫后, 增加更多的headers, 含cookie请求头
                   3.  用 session 处理cookies 
                   4.  准备一堆 cookie, 组成 cookie 池
                       3. 不登录时:准备刚开始成功请求对方网站的 cookie, 用来获取对方网站设置在 response 中的 cookie, 下次请求时, 用之前的列表中的 cookie 请求
                       4. 登录时:准备多个帐号, 用程序获取每个账号的cookie, 访问 登录后才能访同网站, 随机地选择cookie
         3. 提取数据. 
            1. 如数据在当前的 urL中: 
               1. 提取 列表页 数据: 直接请求列表页 url 地址。
               2. 提取 详情页 数据: 确定 url, 发送请求, 提取数据, 返回数据
            2. 数据不在当前 url 中, Network 中寻找数据的位置
               1. 用 chome 中的过滤条件。选出 js, css, img 之外的按钮
               2. 用 chrome 中的seaurch all fle, 搜索数据
            3. 数据的提取方法:
               1. xpath() 方法。 从 html 中提取整块的数据。再分组, 之后提取每一组数据
               2. re 方法, 提取 json 字符串
               3. json方法

          4. 保存在本地. text, json, csv格式.   csv: 逗号分隔的文件,非常简单. 保存在 数据库
             
         7. 队列queue.Queue()

                 1. 队列 是线程安全的。List 不安全. list中, 两个线程同时读取同一个数据, 会发送两次请求或两次响应。尽量不要list。在队列中,一个线程操作时,另一个线程 处于等待, 线程安全. 原因: 用Queue() 后, 线程模块不直接联系, 中间有队列.
                   
                 2. queue.Queue() 的方法, 生成队列对象: que = queue.Queue()
                   
                             1. que.get_nowait()  #queue空时 报错  
                       2. que.get()  #空时 不报错, 等待  
                             3. que.put()  #放入  
                       4. que.put_nowait  #满了放入报错  
                             5. que.qsize()  #获取队列中保存的个数  
                       6. que.join()  #进程线程都会等待.在线程中 join 让主线程阻塞, 等待子线程的完成  
                             7. Oue.get() #须与 que().task_done 配合才使 queue 减1, 否则不减1  
                       8. que.put() 自动加1
             
         8. 多线程

                 1. ```
                    t1 = threading.Thread(target=self.get_url_list) #没有参数
                    thread_list = []
                    thread_list.append(t1)
                    for t in thread_list: 
                    	t.setDaemon(True)  #问题:线程在while True中不结束. 解决方法:子线程对象设为守护线程。该子线程不重要。主线程结束,子线程结束
                    	t.start()
                    	
                    for q in [self.url_queue, self.html_queue, self.content_queue]:
                    	q.join()  #让主线程等待阻塞,等队列中的任务或完成后 再完成主线程
                    for i in range(3):
                    	t_parse = threading.Thread(target=self.parse_url)
                    	thread_list.append(t_parse)  #创建一线程对象,放入list中
                    ```

         9. 爬取动态数据: js生成的数据: 

                     1. 方法1:  network中, Response和 Preview中,ctr+f查找之.  
                     2. 方法2: 数据通过js渲染生成的, 对应的数据重新发送了一次请求, 请求在哪里呢. 选search all files并在其中查找个数据,   不管ccs与 js后缀的地址, 选取其他类型的url地址, 然后再Filter过滤器中输入选取的url地址的部分内容, 过滤出有所查内容的url地址. 选择一个name, 在network的preview中查看内容. 在network的Headers中查看Request URL的url值. 复制在浏览器中黏贴查看. 勾选 preserve log 保存log日志
                     3. 左侧name : 各 url 地址, 右侧 : 相应 url 地址的 响应
                 
         8. lxml的工作原理: 默认编码是utf-8格式, lxml用utf-8格式对传入的字符串解码成unicode

               


### 3 动态HTML处理和机器图像识别

1. 爬虫(Spider)，反爬虫(Anti-Spider)，反反爬虫(Anti-Anti-Spider) 之间恢宏壮阔的斗争...
   1. day1: 
      1. 爬虫, 遍历某站的电影列表页面，电影名字存进自己的数据库
      2. 反爬虫: 某个时间段请求量陡增. 分析日志, 发现都是 IP(xxx.xxx.xxx.xxx)这个用户, user-agent 还是 Python-urllib/2.7 
   2. day2
      1. 爬虫: 进行了修改.user-agent 模仿百度 ("Baiduspider...")， IP 半个小时就换一个IP代理
      2. 反爬虫 : 服务器上设置了一个频率限制, 每分钟超过120次请求的再屏蔽IP
         1. 写了个脚本, 通过 hostname 检查 ip 是不是真的百度家的， ip 设置一个白名单
   3. day3
      1. 爬虫: 修改代码, 随机1-3秒爬一次，爬10次休息10秒. 每天只在8-12，18-20点爬. 隔几天还休息一下
      2. 反爬虫: 看 日志. 当3个小时的总请求超过50次的时候弹出一个验证码弹框, 没有准确正确输入的话就把 IP 记录进黑名单
   4. day4
      1. 爬虫: 学习 图像识别（关键词 PIL，tesseract）, 再对验证码进行了二值化，分词，模式训练之后. 最后识别了小黎的验证码
      2. 反爬虫: 数据 由前端同学异步获取，并且通过 JavaScript 的加密库生成动态的 token. 同时加密库再 混淆
   5. day5
      1. 爬虫: 放弃了基于 HttpClient的爬虫, 选择 内置浏览器引擎的爬虫(关键词：PhantomJS，Selenium)
      2. 反爬虫
2. 爬虫建议
   1. 减少请求次数: 能抓列表页 不抓详情页，减轻服务器压力. 程序员都是混口饭吃不容易。
   2. 不 只看 Web 网站，还看 手机 App 和 H5, 这样的反爬虫措施 少。
   3. 实际上，防守方只做到 `IP 限制频次`. 太高级成本增加
   4. 高性能爬虫，考虑多线程(成熟的框架如 Scrapy都已支持), 分布式...

#### 3.1 动态HTML介绍

1. JavaScript 网页:JavaScript 是客户端脚本语言, 它收集 用户的跟踪数据,  它直接提交 表单不用重载页, 它 在页面嵌入 多媒体文件运行网页游戏。

   2. 网页 源代码 `<scripy>`标签

      ```
      <script type="text/javascript"
      src="https://statics.huxiu.com/w/mini/static_2015/js/sea.js?v=201601150944">
      </script>
      
      ```

      
   
2. jQuery: 常见的js库, 70%  流行的网站(约 200 万), 约 30% 的其他网站(约 2 亿)都在用。

   2. 源代码有` jQuery `入口, 如:

      ```
   <script type="text/javascript" 
      src="https://statics.huxiu.com/w/mini/static_2015/js/jquery-1.11.1.min.js?v=201512181512">
      </script>
      ```
      
      1. `jquery-1.11.1.min.js`  jQuery入口
   
   3. jQuery网站 采集数据需要注意: jQuery创建 HTML内容, 只有执行JavaScript 代码 后才显示。传统采集 , 只能获得 JavaScript 代码执行前页面上的内容。
   
3. Ajax : 一种技术.  Asynchronous JavaScript and XML(异步 JavaScript 和 XML)

   1. ajax 不用单独的 `页面请求`  就可以发送http请求,和网络服务器 交互 (收发信息)

   2. 发出 `HTTP` 请求 是客户端与服务器通信的唯一方式

      

4. DHTML: Dynamic HTML, DHTML, 一系列 解决网络问题的 技术集合, 和 Ajax 一样

   2.  JavaScript 控制 HTML,CSS. 即 客户端语言控制HTML 元素(HTML,CSS)
   
5. Python采集 Ajax/DHTML页面的 两种途径

   1. 途径1: 直接从 `JavaScript` 代码 采集内容（费时费力）
   2. 途径2: 用 Python 的 `第三方库` 运行 JavaScript，采集 浏览器里`看到的`页面 (可以)

#### 3.2 Selenium与PhantomJS

问题:`selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

解决方案: `browser = webdriver.PhantomJS(executable_path=r'C:\Users\lyh\Anaconda2\phantomjs-2.1.1-windows\bin\phantomjs.exe')

Selenium已被弃用PhantomJS，请使用无头版本的Chrome或Firefox代替

----

1. Selenium: 浏览器自动化操作框架, 

   1.  三种工具组成
      1. 第一个工具: SeleniumIDE，是Firefox的扩展插件，支持用户录制和回访测试
      2. 第二个工具: Selenium WebDriver, web应用的自动化测试框架. 提供了各种语言环境的API, 支持更多控制权和编写应用程序
      3. 第三个工具: SeleniumGrid帮助工程师使用Selenium API控制分布在一系列机器上的浏览器实例，并发运行更多测试。在项目内部，分别被称为“IDE”、“WebDriver”和“Grid”
   2. Selenium安装:  
      1. 方法1:  PyPI 网站下载 Selenium库[https://pypi.python.org/simple/selenium](https://pypi.python.org/simple/selenium/)
      2. 方法2:  第三方管理器 pip, 命令：`sudo pip install selenium`

2. PhantomJS: 基于Webkit的“无界面”(headless) `浏览器`.  可编程的无头浏览器.

   无头浏览器：有完整的浏览器内核,包括js解析引擎,渲染引擎,请求处理等,但不包括显示和用户交互页面的浏览器

   1.  它 把网站加载到内存并执行页面上的 `JavaScript`,  不 展示 界面， 运行比完整的浏览器 高效.  Selenium和 PhantomJS 结合, 运行网络爬虫, 处理 JavaScrip、Cookie、headers，任何真实用户做的事
   2. PhantomJS安装:  
      1. Ubuntu16.04 命令安装：`sudo apt-get install phantomjs`
      2. 其他系统无法安装，官方网站[http://phantomjs.org/download.html)](http://phantomjs.org/download.html) 下载
   3. PhantomJS是浏览器, 不是python库, 通过Selenium调用PhantomJS 
   4. PhantomJS官方参考文档: [http://phantomjs.org/documentation](http://phantomjs.org/documentation/)

3. Selenium3.x调用浏览器必须有一个webdriver驱动文件

   1. Chrome驱动文件下载：[点击下载chromedrive](https://chromedriver.storage.googleapis.com/index.html?path=2.35/),   chromedriver.exe

      1. ```
         #创建Chrome浏览器对象，在电脑上在打开一个浏览器窗口
         driver = webdriver.Chrome("F:\Chrome\ChromeDriver\chromedriver")

   2. Firefox驱动文件下载:[点解下载geckodriver](https://github.com/mozilla/geckodriver/releases)

      1. ```
         #创建Firefox浏览器对象，在电脑上在打开一个浏览器窗口
         browser = webdriver.Firefox(executable_path ="F:\GeckoDriver\geckodriver")

##### Selenium已被弃用PhantomJS

 1. WebDriver: WebDriver: Selenium包的 子包。Selenium的第二个工具.  WebDriver是W3C的一个标准，由Selenium主持

      1. 自动测试web应用的框架. 像 加载网站的 `浏览器`, 像 BeautifulSoup 或Selector 对象. 用于查找页面元素, 与页面上的元素 交互 (发送文本、点击等), 运行网络爬虫。WebDriver支持Firefox([FirefoxDriver](http://code.google.com/p/selenium/wiki/FirefoxDriver))、IE ([InternetExplorerDriver](http://code.google.com/p/selenium/wiki/InternetExplorerDriver))、Opera ([OperaDriver](http://www.opera.com/developer/tools/operadriver/))和Chrome ([ChromeDriver](http://code.google.com/p/selenium/wiki/ChromeDriver))。 它还支持[Android](http://lib.csdn.net/base/android) ([AndroidDriver](http://code.google.com/p/selenium/wiki/AndroidDriver))和iPhone ([IPhoneDriver](http://code.google.com/p/selenium/wiki/IPhoneDriver))的移动应用测试

 2. webdriver的使用步骤

    1. 导入 webdriver: `from selenium import webdriver`
    2. 引入的Keys包, 调用键盘按键操作 :    `from selenium.webdriver.common.keys import Keys`
    3. 创建`PhantomJS`浏览器对象:    `driver = webdriver.PhantomJS(executable_path=r'D://phantomjs//bin//phantomjs.exe')`.    #指定PhantomJS的驱动路径
    4. get()方法等到页面完全加载, 才继续程序, 通常 time.sleep(2), 然后    `driver.get("http://www.baidu.com/")`
    5. 获取 id为 wrapper的标签的 文本内容  `data = driver.find_element_by_id("wrapper").text`
       1. 打印数据内容  `print(data)`
       2. 打印 页面标题 "百度一下，你就知道" `print(driver.title)`
    6. 生成 当前页面 快照 并 保存: `driver.save_screenshot("baidu.png")`
    7. 模拟输入框输入字符串: id="kw"定位的百度输入框，输入字符串"长城": `driver.find_element_by_id("kw").send_keys(u"长城")`
    8. 模拟点击: id="su" 定位百度搜索按钮，click()模拟动作: `driver.find_element_by_id("su").click()`
    9. 获取新的页面 快照: `driver.save_screenshot("长城.png")`
    10. 打印 网页渲染后的 源代码: `print(driver.page_source)`
    11. 获取 当前页面 Cookie: `print(driver.get_cookies())`
    12. 模拟`ctrl+a` 全选 输入框内容: `driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'a')`
    13. 模拟`ctrl+x` 剪切 输入框内容: `driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'x')`
    14. 模拟输入框 重新输入 内容: `driver.find_element_by_id("kw").send_keys("itcast")`
    15. 模拟 Enter回车键: `driver.find_element_by_id("su").send_keys(Keys.RETURN)`
    16. 模拟清除 输入框内容: `driver.find_element_by_id("kw").clear()`
    17. 生成新的 页面快照: `driver.save_screenshot("itcast.png")`
    18. 获取 当前url: `print (driver.current_url)`
    19. 模拟关闭 当前页面，如 只有一个页面， 关闭浏览器: `driver.close()`
    20. 关闭浏览器: `driver.quit()`

 3. 页面操作

     1. 表单输入框: `<input type="text" name="user-name" id="passwd-id" />`

     2. webdriver对象的方法: 定位UI元素 (WebElements)

        1. 获取 id标签值: `element = driver.find_element_by_id("passwd-id")`

        2. 获取 name标签值: `element = driver.find_element_by_name("user-name")`

        3. 获取 标签名值: `element = driver.find_elements_by_tag_name("input")`

        4. 通过XPath 匹配: `element = driver.find_element_by_xpath("//input[@id='passwd-id']")`

        5. 选取单个元素方法

           1. ```
              find_elements_by_id
              find_elements_by_name
              find_elements_by_xpath
              find_elements_by_link_text
              find_elements_by_partial_link_text
              find_elements_by_tag_name
              find_elements_by_class_name
              find_elements_by_css_selector
              ```

         6. driver.find_element(by, value)  

            1. 参数: 

               1. by的属性: 'By.ID', 'By.CLASS_NAME', 'By.TAG_NAME', 'By.NAME', 'By.LINK_TEXT', 'By.PARTIAL_LINK_TEXT', 'By.CSS_SELECTOR'
               2. value的值, 是对应的by的值: 

            2. By.ID

               `<div id="coolestWidgetEvah">...</div>`

               实现:

               ```python
               element = driver.find_element_by_id("coolestWidgetEvah")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               element = driver.find_element(by=By.ID, value="coolestWidgetEvah")
               ```

             3. By.CLASS_NAME

                ``<div class="cheese"><span>Cheddar</span></div><div class="cheese"><span>Gouda</span></div>`

                实现

                ```
                cheeses = driver.find_elements_by_class_name("cheese")
                ------------------------ or -------------------------
                from selenium.webdriver.common.by import By
                cheeses = driver.find_elements(By.CLASS_NAME, "cheese")
                By Tag Name
                ```

            4. By.TAG_NAME

               `<iframe src="..."></iframe>`

               实现

               ```
               frame = driver.find_element_by_tag_name("iframe")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               frame = driver.find_element(By.TAG_NAME, "iframe")
               By Name
               ```

            5. By.NAME   #节点名

               `<input name="cheese" type="text"/>`

               实现

               ```
               cheese = driver.find_element_by_name("cheese")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               cheese = driver.find_element(By.NAME, "cheese")
               By Link Text
               ```

            6. By.LINK_TEXT

               `<a href="http://www.google.com/search?q=cheese">cheese</a>`

               实现

               ```
               cheese = driver.find_element_by_link_text("cheese")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               cheese = driver.find_element(By.LINK_TEXT, "cheese")
               By Partial Link Text
               ```

            7. By.PARTIAL_LINK_TEXT

               `<a href="http://www.google.com/search?q=cheese">search for cheese</a>>`

               实现

               ```
               cheese = driver.find_element_by_partial_link_text("cheese")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               cheese = driver.find_element(By.PARTIAL_LINK_TEXT, "cheese")
               ```

            8. By.CSS_SELECTOR

               `<div id="food"><span class="dairy">milk</span><span class="dairy aged">cheese</span></div>`

               实现

               ```
               cheese = driver.find_element_by_css_selector("#food span.dairy.aged")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               cheese = driver.find_element(By.CSS_SELECTOR, "#food span.dairy.aged")
               By XPath
               ```

            9. By.XPATH

               ```
               <input type="text" name="example" />
               <INPUT type="text" name="other" />
               ```

               实现

               ```
               inputs = driver.find_elements_by_xpath("//input")
               ------------------------ or -------------------------
               from selenium.webdriver.common.by import By
               inputs = driver.find_elements(By.XPATH, "//input")
               ```

     3. driver的属性:  

        1. driver.title   #网页标题
        2. driver.page_source   #网页渲染后的 源代码
        3. driver.current_url    #当前url
        4. driver.window_handles

     4. `element=driver.get()`返回对象element的属性

        1. element.text
        2. type(element)   #<class 'selenium.webdriver.remote.webelement.WebElement'>, 或其组成的list类型

 4. webdriver对象的鼠标动作链: 鼠标操作，比如双击、右击、拖拽甚至按住不动

    1. 创建ActionChains 类 实例, 用 perform()  方法

        1. `ActionChains(driver).move_to_element().perform()`
        
    2. 案例步骤:
    
        1. 导入 ActionChains 类: `from selenium.webdriver import ActionChains`
           
        2. 鼠标 移动 到 ac 位置
    
           ```
           ac = driver.find_element_by_xpath('element')
           ActionChains(driver).move_to_element(ac).perform()
           ```

        3. 在 ac 位置单击

           ```
           ac = driver.find_element_by_xpath("elementA")
           ActionChains(driver).move_to_element(ac).click(ac).perform()
           ```
    
        4. 在 ac 位置 双击
    
           ```
           ac = driver.find_element_by_xpath("elementB")
           ActionChains(driver).move_to_element(ac).double_click(ac).perform()
           ```
    
        5. 在 ac 位置右击
    
           ```
           ac = driver.find_element_by_xpath("elementC")
           ActionChains(driver).move_to_element(ac).context_click(ac).perform()
           ```
    
        6. 在 ac 位置 左键单击 hold住
    
           ```
           ac = driver.find_element_by_xpath('elementF')
           ActionChains(driver).move_to_element(ac).click_and_hold(ac).perform()
           ```
    
        7. 将 ac1 拖拽 到 ac2 位置
    
           ```
           ac1 = driver.find_element_by_xpath('elementD')
           ac2 = driver.find_element_by_xpath('elementE')
           ActionChains(driver).drag_and_drop(ac1, ac2).perform()
           ```
    
 6. 填充表单

     1. `<select> </select>`标签的下拉框

        ```
        <select id="status" class="form-control valid" onchange="" name="status">
            <option value=""></option>
            <option value="0">未审核</option>
            <option value="1">初审通过</option>
            <option value="2">复审通过</option>
            <option value="3">审核不通过</option>
        </select>
        ```

     2. Selenium 提供`Select`类 处理下拉框

        ```
        # 导入 Select 类
        from selenium.webdriver.support.ui import Select
        
        # 找到 name 的选项卡
        select = Select(driver.find_element_by_name('status'))
        
        # 三种选择下拉框的方式
        select.select_by_index(1)  #索引 选择
        select.select_by_value("0")  #值 选择
        select.select_by_visible_text(u"未审核")  #文字 选择 
        
        #全部取消选择
        select.deselect_all()
        ```

        1. 三种选择下拉框 方式
           1. 索引 选择: index 索引 从 0 开始
           2. 值 选择: value 是option标签的属性值，并不显示在下拉框中的值
           3. 文字 选择: visible_text 是 option标签的 文本值，显示在下拉框的值

 7. 弹窗处理

     1. 处理弹窗提示 或 获取提示信息方法: `alert = driver.switch_to_alert()`

 8. 页面切换

     1. 方法1: 窗口的切换: `driver.switch_to.window("this is window name")`

     1. 方法2: 获取 窗口的操作对象

        ```
        for handle in driver.window_handles:
            driver.switch_to_window(handle)
        ```
        
        

 9. 页面前进和后退

    `driver.forward()`     #前进

    `driver.back()`        # 后退

    

 10. Cookies

        1. `driver.delete_cookie("CookieName")`  #by name
        2. `driver.delete_all_cookies()`  # 删除所有的cookie

​    

 11. 页面等待 : 网页 采用了 Ajax 技术, 

       2. 问题: NullPointer的异常

           	1. 某个dom元素没出来，但代码使用了这个WebElement，会抛出NullPointer的异常

       3. Selenium方案: 方案1: 显式等待,  某条件成立时 继续执行. 方案2. 隐式等待 ,等待特定的时间

       4. 显式等待: 指定某个条件， 设置最长等待时间。时间内没有找到元素，抛出异常

           2. ```
              from selenium import webdriver
              from selenium.webdriver.common.by import By
              # WebDriverWait 库，负责循环等待
              from selenium.webdriver.support.ui import WebDriverWait
              # expected_conditions 类，负责条件出发
              from selenium.webdriver.support import expected_conditions as EC
              
              driver = webdriver.Chrome()
              driver.get("http://www.xxxxx.com/loading")
              try:
                  # 页面一直循环，直到 id="myDynamicElement" 出现
                  element = WebDriverWait(driver, 10).until(
                      EC.presence_of_element_located((By.ID, "myDynamicElement"))
                  )
              finally:
                  driver.quit()
              ```
           
               1. 问题: Message: 'chromedriver' executable needs to be in PATH

                   1. 下载路径: `Mirror index of http://chromedriver.storage.googleapis.com/`.  注意版本匹配

                   2. ```
                      from selenim import webdriver
                      import time
                      
                      #chrome_driver = 'chromedriver.exe的地址'
                      chrome_driver = 'D:\chromedriver\chromedriver.exe'
                      
                      browser = webdriver.Chrome(executable_path= chrome_driver)
                      time.sleep(5)
                      ```
              
           3. 内置的等待条件
           
               1. ```
                  title_is
                  title_contains
                  presence_of_element_located
                  visibility_of_element_located
                  visibility_of
                  presence_of_all_elements_located
                  text_to_be_present_in_element
                  text_to_be_present_in_element_value
                  frame_to_be_available_and_switch_to_it
                  invisibility_of_element_located
                  element_to_be_clickable – it is Displayed and Enabled.
                  staleness_of
                  element_to_be_selected
                  element_located_to_be_selected
                  element_selection_state_to_be
                  element_located_selection_state_to_be
                  alert_is_present
                  ```
           
                  1. 直接调用这些条件， 不用自己写这些等待条件
           
       5. 隐式等待

           1. 简单地设置一个等待时间，单位为秒

           2. ```
              from selenium import webdriver
              
              driver = webdriver.Chrome()
              driver.implicitly_wait(10) # seconds
              driver.get("http://www.xxxxx.com/loading")
              myDynamicElement = driver.find_element_by_id("myDynamicElement")
              ```

              1. driver.implicitly_wait(10)  #等待时间

#### 3.3 案例1：网站模拟登录

1. ```
   # -*- coding:utf-8 -*-
   
   # douban.py
   #coding=utf-8
   import time
   from selenium import webdriver
   from selenium.webdriver.common.keys import Keys
   
   class Douban():
       def __init__(self):
           self.url = "https://www.douban.com/"
           self.driver = webdriver.PhantomJS()
   
       def log_in(self):
           self.driver.get(self.url)
           time.sleep(3)#睡3分钟，等待页面加载
           self.driver.save_screenshot("0.png")
           #输入账号
           self.driver.find_element_by_xpath('//*[@id="form_email"]').send_keys("xxxxx@qq.com")
           #输入密码
           self.driver.find_element_by_xpath('//*[@id="form_password"]').send_keys("xxxx")
           #点击登陆
           self.driver.find_element_by_class_name("bn-submit").click()
           time.sleep(2)
           self.driver.save_screenshot("douban.png")
           #输出登陆之后的cookies
           print(self.driver.get_cookies())
   
       def __del__(self):
           '''调用内建的稀构方法，在程序退出的时候自动调用
           类似的还可以在文件打开的时候调用close，数据库链接的断开
           '''
           self.driver.quit()
   
   if __name__ == "__main__":
       douban = Douban() #实例化
       douban.log_in() #之后调用登陆方法
   ```

   1.  selenium 加 PhantomJS
      1. `driver = webdriver.PhantomJS()`
      2. selenium的driver的方法是  .`find_element_*_*("")`
      3. 获取属性  `.get_attribute("src")`
      4. 获取文本内容  `.text`

#### 3.4 案例二：动态页面模拟点击

1. 爬取斗鱼平台的 所有房间信息

   ```
   #coding=utf-8
   from selenium import webdriver
   import json
   import time
   class Douyu:
       # 1.发送首页的请求
       def __init__(self):
       	#创建PhantomJS浏览器对象
           self.driver = webdriver.PhantomJS(executable_path=r'D://phantomjs//bin//phantomjs.exe')
           self.driver.get("https://www.douyu.com/directory/all") #请求首页
   
       #获取没页面内容
       def get_content(self):
           time.sleep(3) #每次发送完请求等待三秒，等待页面加载完成
           li_list = self.driver.find_elements_by_xpath('//ul[@id="live-list-contentbox"]/li')
           contents = []
           for i in li_list: #遍历房间列表
               item = {}
               item["img"] = i.find_element_by_xpath("./a//img").get_attribute("src") #获取房间图片
               item["title"] = i.find_element_by_xpath("./a").get_attribute("title") #获取房间名字
               item["category"] = i.find_element_by_xpath("./a/div[@class='mes']/div/span").text #获取房间分类
               item["name"] = i.find_element_by_xpath("./a/div[@class='mes']/p/span[1]").text #获取主播名字
               item["watch_num"] = i.find_element_by_xpath("./a/div[@class='mes']/p/span[2]").text #获取观看人数
               print(item)
               contents.append(item)
           return contents
       #保存本地
       def save_content(self,contents):
           f = open("douyu.txt","a")
           for content in contents:
               json.dump(content,f,ensure_ascii=False,indent=2)
               f.write("\n")
           f.close()
   
       def run(self):
           #1.发送首页的请求
           #2.获取第一页的信息
           contents = self.get_content()
               #保存内容
           self.save_content(contents)
           #3.循环  点击下一页按钮，知道下一页对应的class名字不再是"shark-pager-next"
           while self.driver.find_element_by_class_name("shark-pager-next"): #判断有没有下一页
               #点击下一页的按钮
               self.driver.find_element_by_class_name("shark-pager-next").click() #
               # 4.继续获取下一页的内容
               contents = self.get_content()
               #4.1.保存内容
               self.save_content(contents)
   
   if __name__ == "__main__":
       douyu = Douyu()
       douyu.run()
   ```
   
   

#### 3.5 机器视觉与tesseract介绍

1. 光学文字识别 , OCR, Optical Character Recognition, 机器视觉 一个分支
   1. 图像中文字翻译成文字, 称为 光学文字识别. OCR的底层库 不多

2. ORC库概述
   1. tesseract: OCR 库, Google 赞助.最优秀最精确的开源 OCR 系统, 通过训练识别出任何字体，任何 Unicode 字符
   2. 安装 tesseract
      1. windows: 下载可执行安装文件https://code.google.com/p/tesseract-ocr/downloads/list安装
      2. Linux 系统: 通过 apt-get 安装: `$sudo apt-get tesseract-ocr`
      3. Mac OS X系统: 用 Homebrew(http://brew.sh/)等第三方库可以很方便地安装 `brew install tesseract`
3. pytesseract:  Python 版本的 tesseract库. 安装 :  `pip install pytesseract`
   1. `tesseract` Python 的命令行的命令工具, 不能 import 语句导入库. 安装pytesseract后, 用 tesseract 命令在 Python 的外面运行

#### 3.6 处理一些格式规范的文字

 1. 图片预处理: 1. 图片转换成灰度图, 2. 调整 亮度和对比度, 3. 裁剪和旋转

 2. OCR示例

       1. linux中Terminal中实现OCR

          1. `tesseract test.jpg text`   #转换图片test.jpg的文字到 text.txt中
          2. cat text.txt  #显示结果

       2. Python代码实现OCR

             1. ```
                  import pytesseract
                  from PIL import Image
                  
                  image = Image.open(r'test.jpg')
                  text = pytesseract.image_to_string(image)
                  print (text)
                  ```

                  

       3. 对图片进行阈值过滤和降噪处理（了解即可）

            ```
            from PIL import Image
            import subprocess
            
            def cleanFile(filePath, newFilePath):
                image = Image.open(filePath)
            
                #对图片进行阈值过滤（低于143的置为黑色，否则为白色）
                image = image.point(lambda x: 0 if x < 143 else 255)
                # 重新保存图片
                image.save(newFilePath)
            
                # 调用系统的tesseract命令对图片进行OCR识别     
                subprocess.call(["tesseract", newFilePath, "output"])
            
                # 打开文件读取结果
                with open("output.txt", 'r') as f:
                    print(f.read())
            
            if __name__ == "__main__":
                cleanFile("text2.png", "text2clean.png")
            ```

       4. subprocess模块: 创建一个新子进程，连接到子进程的stdin、stdout、stderr管道并获取它们的返回码。(先导入import subprocess)

            1. `subprocess`模块 替代过时的模块和函数，如：` os.system, os.spawn*, os.popen*, popen2.*`命令

            2. subprocess.call(), subprocess.check_call(), subprocess.check_output()

                 1. 执行由参数提供的命令. 用数组为参数运行命令,也可用字符串为参数运行命令(通过设置参数`shell=True`).  参数`shell`默认为`False`

            3. subprocess.call()

                 1. ```
                      #做一个查看文件的例子:
                      subprocess.call(["ls", "-a"])
                      
                      #上边的例子,把shell设置为True,就可以使用字符串了:
                      subprocess.call("ls -a", shell=True)
                      ```

            4. | 模块的函数                   | 案例                                | 解释                                                         |
                 | ---------------------------- | ----------------------------------- | ------------------------------------------------------------ |
                 | subprocess.run()             | subprocess.run(["df","-h"])         | *python解析则传入命令的每个参数的列表*                       |
                 | subprocess.call()            | res =subprocess.call(["ls","-l"])   | 执行命令，返回命令的结果和执行状态，0或者非0                 |
                 | subprocess.check_call()      | subprocess.check_call(["ls","-l"])  | 执行命令，返回结果和状态，正常为0 ，执行错误则抛出异常       |
                 | subprocess.getstatusoutput() | subprocess.getstatusoutput('pwd')   | 接受字符串形式的命令，返回 一个元组形式的结果，第一个元素是命令执行状态，第二个为执行结果 |
                 | subprocess.getoutput()       | subprocess.getoutput('pwd')         | 接受字符串形式的命令，查看执行结果                           |
                 | subprocess.check_output()    | res =subprocess.check_output("pwd") | 执行命令，返回执行的结果，而不是打印                         |
                 |                              |                                     |                                                              |

            5. subprocess.Popen(), 以上subprocess使用的方法，都是对subprocess.Popen的封装，下面我们就来看看这个Popen方法. 

                 1. class subprocess.Popen( args,bufsize=0,executable=None,stdin=None, stdout=None,stderr=None,preexec_fn=None,close_fds=False,shell=False,
                      cwd=None,env=None,universal_newlines=False,startupinfo=None,creationflags=0)

                      1. args: 是一个字符串，可以是一个包含程序参数的列表。要执行的程序一般就是这个列表的第一项，或者是字符串本身

                           1. ```
                                subprocess.Popen(["cat","test.txt"])
                                subprocess.Popen("cat test.txt")
                                ```

                 | res = subprocess.Popen()的属性方法 | 实例               | 解析                                                         |
                 | ---------------------------------- | ------------------ | ------------------------------------------------------------ |
                 | res.stdout                         | res.stdout.read()  | 标准输出                                                     |
                 | res.stdout                         | res.stdout.close() | 关闭                                                         |
                 | res.stderr                         | res.stderr.read()  | 标准错误中有错误信息                                         |
                 |                                    | res.poll()         | 定时检查命令有没有执行完毕，执行完毕后返回执行结果的状态，没有执行完毕返回None |
                 |                                    | res.wait()         | *中间会一直等待*                                             |
                 |                                    | res.terminate()    | 结束进程                                                     |
                 |                                    | res.pid            | 获取当前执行子shell的程序的进程号                            |

                 

            6. subprocess.call(["tesseract", newFilePath, "output"])   #调用 tesseract命令, 对图片newFilePath处理, 保存的output中

 3. 从网站图片中抓取文字

       1. 亚马逊图书: 图书的预览页是 Ajax 脚本加载的,预览图片隐藏在 div 节点 下面

       2. 亚马逊图书: 托尔斯泰的《战争与和平》

            ```
            import time
            from urllib.request import urlretrieve
            import subprocess
            from selenium import webdriver
            #创建PhantoJS浏览器对象, 参数指定驱动
            driver = webdriver.PhantomJS(executable_path=r'D://phantomjs//bin//phantomjs.exe')
            
            #创建Firefox浏览器对象, 参数指定驱动
            # driver = webdriver.Firefox("Firefox驱动的目录")
            
            driver.get("http://www.amazon.com/War-Peace-Leo-Nikolayevich-Tolstoy/dp/1427030200")
            # 单击图书预览按钮 driver.find_element_by_id("sitbLogoImg").click() imageList = set()
            # 等待页面加载完成
            time.sleep(5)
            # 当向右箭头可以点击时,开始翻页
            while "pointer" in driver.find_element_by_id("sitbReaderRightPageTurner").get_attribute("style"):
                driver.find_element_by_id("sitbReaderRightPageTurner").click()
                time.sleep(2)
                # 获取已加载的新页面(一次可以加载多个页面,但是重复的页面不能加载到集合中)
                pages = driver.find_elements_by_xpath("//div[@class='pageImage']/div/img")
                for page in pages:
                    image = page.get_attribute("src")
                    imageList.add(image)
            driver.quit()
            
            # 用Tesseract处理我们收集的图片URL链接
            for image in sorted(imageList):
                # 保存图片
                urlretrieve(image, "page.jpg")
                p = subprocess.Popen(["tesseract", "page.jpg", "page"], stdout=subprocess.PIPE,stderr=subprocess.PIPE)
                f = open("page.txt", "r")
                p.wait() print(f.read())
            ```

#### 3.7 验证码的机器识别

 1. 知乎验证码 处理

     1. 验证码:  验证码图片的 src 属性 `<img src="WebForm.aspx?id=8AP85CQKE9TJ">`,   验证码的答案 在服务器端的 数据库, 有时间限制

     2. 处理方法:  先把验证码图片下载到硬盘里，清洗干净. 然后用 tesseract 处理 图片. 最后返回 网站要求的识别结果
     
     3. 代码

        ```
        #!/usr/bin/env python
        # -*- coding:utf-8 -*-
        
        import requests
        import time
        import pytesseract
        from PIL import Image
        from bs4 import BeautifulSoup
        
        def captcha(data):
        	#存储数据的二进制格式到图形文件
            with open('captcha.jpg','wb') as fp:
                fp.write(data)
            time.sleep(1)
            #创建图形文件
            image = Image.open("captcha.jpg")
            text = pytesseract.image_to_string(image)
            print("机器识别后的验证码为：" + text)
            command = raw_input("请输入Y表示同意使用，按其他键自行重新输入：")
            if (command == "Y" or command == "y"):
                return text
            else:
                return raw_input('输入验证码：')
        
        def zhihuLogin(username,password):
        
            #创建一个保存Cookie值的session对象
            session_a = requests.Session()
            headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}
        
            # 先获取页面信息，找到需要POST的数据（并且已记录当前页面的Cookie）
            html = session_a.get('https://www.zhihu.com/#signin', headers=headers).content
        
            #找到 name 属性值为 _xsrf 的input标签，取出value里的值
            _xsrf = BeautifulSoup(html ,'lxml').find('input', attrs={'name':'_xsrf'}).get('value')
        
            #获取验证码，r后面的值是Unix时间戳,time.time()
            captcha_url = 'https://www.zhihu.com/captcha.gif?r=%d&type=login' % (time.time() * 1000)
            response = session_a.get(captcha_url, headers = headers)
        
            data = {
                "_xsrf":_xsrf,
                "email":username,
                "password":password,
                "remember_me":True,
                "captcha": captcha(response.content)  #验证码
            }
        	
        	#验证码用 post 传递
            response = session_a.post('https://www.zhihu.com/login/email', data = data, headers=headers)
            print (response.text)
        
            response = sessiona.get('https://www.zhihu.com/people/maozhaojun/activities', headers=headers)
            print (response.text)
        
        
        if __name__ == "__main__":
            zhihuLogin('xxxx@qq.com','ALAxxxxIME')
        ```
        
        1. 验证码 放 form中
        2. form用post() 发送给服务器端
        2. input()与raw_input()的区别只存在[Python](https://so.csdn.net/so/search?from=pc_blog_highlight&q=Python)2.X环境下,因为在Python3中将Python2中的input()删除了,同时将raw_input()改名为input()
     
 2. 尝试处理中文字符

    1. 命令行方式: 

       1. `tesseract --list-langs`   #查看支持的语言，`chi_sim`表示支持简体中文
       2. 指定 语言识别 `tesseract -l chi_sim paixu.png paixu`   #`-l` 指定语言

    2. python中代码

       ```
       #!/usr/bin/env python
       # -*- coding:utf-8 -*-
       
       from PIL import Image
       import subprocess
       
       def cleanFile(filePath)
           image = Image.open(filePath)
       
           # 调用系统的tesseract命令, 对图片进行OCR中文识别
           subprocess.call(["tesseract", "-l", "chi_sim", filePath, "paixu"])
       
           # 打开文件读取结果
           with open("paixu.txt", 'r') as f:
               print(f.read())
       
       if __name__ == "__main__":
           cleanFile("paixu.png")
       ```

       

#### 3.8 执行JavaScript语句

1. `driver.execute_script(js语句)`  #driver执行js脚本

   1. ```
      #隐藏百度图片
      from selenium import webdriver
      
      driver = webdriver.Chrome("D:\\chromedriver\\chromedriver.exe")
      driver.get("https://www.baidu.com/")
      
      # 给 搜索输入框 标红的javascript脚本
      js = "var q=document.getElementById(\"kw\");q.style.border=\"2px solid red\";"
      
      # 调用给搜索输入框标红js脚本
      driver.execute_script(js)
      
      #查看页面快照
      driver.save_screenshot("redbaidu.png")
      
      #js隐藏元素，将获取的图片元素隐藏
      img = driver.find_element_by_xpath("//*[@id='lg']/img")
      driver.execute_script('$(arguments[0]).fadeOut()',img)
      
      # 向下滚动到页面底部
      driver.execute_script("$('.scroll_top').click(function(){$('html,body').animate({scrollTop: '0px'}, 800);});")
      
      #查看页面快照
      driver.save_screenshot("nullbaidu.png")
      
      driver.quit()
      ```
   
   2. ```
      #模拟滚动条滚动到底部
      from selenium import webdriver
      import time
      
      driver = webdriver.Chrome("chromedriver.exe路径")
      driver.get("https://movie.douban.com/typerank?type_name=剧情&type=11&interval_id=100:90&action=")
      
      # 向下滚动10000像素
      js = "document.body.scrollTop=10000"
      #js="var q=document.documentElement.scrollTop=10000"
      time.sleep(3)
      
      #查看页面快照
      driver.save_screenshot("douban.png")
      
      # 执行JS语句
      driver.execute_script(js)
      time.sleep(10)
      
      #查看页面快照
      driver.save_screenshot("newdouban.png")
      
      driver.quit()
   
   

#### 3.9训练Tesseract

1. 设置环境变量 `$TESSDATA_PREFIX`，指定 Tesseract 训练的数据文件目录
   1. Linux 系统和 Mac OS X 系统
      1. `export TESSDATA_PREFIX=/usr/local/share/Tesseract`
   2. Windows 系统
      1. 命令设置环境变量: `#setx TESSDATA_PREFIX C:\Program Files\Tesseract OCR\Tesseract`
   
   1. 然后此目录下 放置一份tessdata数据文件

2. 大多数 验证码 比较简单

   1. PHP 内容管理系统 Drupal  验证码模块 https://www.drupal.org/project/captcha

      `captcha`

   2. 命令行: Tesseract命令识别图片: `tesseract captcha.png output`

3. 矩形定位文件(box file)

   1. 一个验证码图片 生成一个矩形定位文件

   2. 通过 `jTessBoxEditor` 软件 修改矩形的定位

   3. ```
   4  15 26 33 55 0
      M  38 13 67 45 0
      m  79 15 101 26 0
      C  111 33 136 60 0
      3  147 17 176 45 0
      ```
   
      1. 第一列 图片中的每个字符
   2. 后面  4 个数字 这个字符的在最小矩形的坐标(左下角 (x , y)、右上角( x , y ))
      3. 最后一个数字“0”表示图片样本的编号
   
   4. 矩形定位文件 保存为  .box 后缀的文本文件 ，(例如 4MmC3.box)

4. 阅读资料

   1. Tesseract 官方文档：https://github.com/tesseract-ocr/tesseract/wiki
   2. 博客园的一篇训练教程：http://www.cnblogs.com/mjorcen/p/3800739.html?utm_source=tuicool&utm_medium=referral

##### 3 小结

1. 爬虫代码的建议
   1. 尽量减少请求次数: 能抓列表页就不抓详情页, 保存获取到的html页面，供查错和重复请求使用
   2. 关注网站的所有类型的页面: wap页面，触屏版页面. H5页面. APP
   3. 多伪装: 动态的UA, 代理ip, 不使用cookie
   4. 利用多线程, 分布式: 在不被反爬的情况下, 尽可能的提高速度

2. 动态HTML技术(了解)

   1. JS: 常用的脚本语言。收集用户的跟踪数据，直接提交表单不需要重载页面, 在页面嵌入多媒体文件，甚至 网页游戏. JS 内有 ajax技术
   2. jQuery : 是一个快速、简洁的 JavaScript 框架， 封装了JavaScript常用的功能代码
   3. ajax : 实现网页异步更新，即在不重新加载网页的情况下， 对网页的某部分进行更新
   4.  JS, jQuery, Ajax 对 搜索引擎, 爬虫 不友好

3. selenium 和phantomJS

      1. selenium: 1. web 网页的的自动化测试工具. 2. 控制浏览器。加载页面, 点击按钮, 前进后退, 截屏. 3 要对应浏览器的驱动

            1. Selenium操作步骤:

                  1. 加载页面
                              1. from selenium import webdriver
                                  2. diver= webdriver.chrome()  #或webdriver.phantomJS()
                                  3. `driver.get("www.baidu.com") `
                                  4. driver.save_screenshot("长城.png")
                  2. 元素定位和操作
                              1. driver.find_elment_by_id("kw").send_keys("长城")
                              2. driver.find_element_by_id(“su").click()  # 点击
                  3. 查看请求消息
                              1. driver.page_source
                              2. driver.get_cookies()
                              3. driver.current_url
                  4. 退出
                              1. driver.close() #退出当前页面
                              2. driver.quit()  #退出浏览器

            2. selenium 方法定位 页面元素

               1. find_element_by_id()  #返回一个
               2. driver.find_elements_by_tag_name()  #用标签名定位元素
               3. driver.find_elements_by_class_name()  #用类名定位元素
               4. driver.find_element_by_link_text("下一页").get_attribute("href")  #用文本定位元素
               5. driver.find_element_by_partial_link_text("下一页").get_attribute('href')  #用部分文本定位元素
               6. driver.find_elements_by_css_selector()  #用css选择器定位元素
               7. driver.find_elements_by_xpath("./h1/p")   #返回list, 没有时返回 空list
               8. driver.find_element_by_xpath(".//h1/p").text   #用xpath定位到元素, 取文本, 没有时报错
               9. driver.find_element_by_xpath(".//h1/p").get_attribute("href")   #用xpath定位到元素,取href属性,  没有时报错
               10. 注意点
                  1. find_element 和 find_elements 的区别: 返回一个和返回一个列表
                  2. `by_link_text` 和 `by_partial_link_text` 的区别: 全部文本和包含某个文本
                  3. by_css_selector 的用法: #food span.dairy.aged
                  4. by_xpath 中获取属性和文本分布用 `get_ attribute()` 和 `.text`

            3. selenium 案例

                  1. ```
                        #百度查询
                        #coding=utf-8
                        from selenium import webdriver
                        #实例化一个浏览器
                        diver= webdriver.chrome()  #或webdriver.phantomJS()
                        #发送请求
                        driver.get("www.baidu.com")  #执行完。打开的浏览器不退出, 仍在内存
                        driver.quite()  #退出
                        #元素定位法 (在elements中找到的 id="kw"的 input 标签)
                        driver.find_elment_by_id("kw").send_keys("python")
                        #元素定位(通过id="su” 找到按钮)
                        driver.find_element_by_id(“su").click()
                        #截屏
                        driver.save_screenshot("./baidu.png")
                        #设窗口大小
                        driver.set_window_size(1920, 1080)
                        #最大化
                        driver.maximize_size()
                        #登录 获取 cookie
                        cookies = driver.get_cookies()
                        cookies = {i["name"]: i["values"] for i in cookies}
                        #获取页面字等串, 交给 lxml 模块处理,用 xpath 提取
                        #driver 获取 html 字待串
                        driver.page_source  #内容是elements的, 不是当前 url 的响应
                        
                        #element元素, 即 页面上的 标签
                        driver.find_element_by_id("kw").send_keys("长城")
                        driver.current_url  #当前url地址
                        driver.click()  #实现跳转
                        driver.close()  #退出当前页面
                        ```

                  2. ```
                        #豆瓣 登录
                        #coding=utf-8
                        from selenium import webdriver
                        #实例化一个浏览器
                        diver= webdriver.chrome()  #或webdriver.phantomJS()
                        driver.get("www.douban.com") 
                        driver.quite()  #退出
                        #元素定位法 
                        driver.find_elment_by_id(" ").send_keys(" ")
                        #元素定位 
                        driver.find_element_by_class_name(“ln_submit").click()
                        
                        cookies = {i["name"]: i["values"] for i in driver.get_cookies()}
                        
                        class DoubanSpider:
                        def __init__(self):
                        pass
                        
                        def run(self):
                        #逻辑过程:
                        1. start_url
                        2. 发生请求, 获取响应
                        3. 提取数据,提取下一页的元素
                        4. 保存数据
                        5. 点击下一页元素,循环
                        
                        #提取数据
                        1. 提取前先分组
                        2. 提取方法 re, xpath, selenium
                        3. .find_element_by_class_name()  #只能用一个class的值, 多个class的值报错
                        4. .path()   #参数是class的全部的值
                        ```

                  3. ```
                        #打码 云平台
                        #识别验证码
                        captcha_image_url = driver.find_element_by_id("captcha_image").get_attribute("src") #获取属性值
                        
                        import requests
                        captcha_content = requests.get(captcha_image_url).content
                        #云打码
                        from yundama.dama import indentify
                        captcha_code = indentify(captcha_content)
                        #输入验证码
                        driver.find_element_by_id("captcha_field").send_keys(captcha_code)
                        dirver.find_element_by_class_name("bn_submit").click()
                        
                        #验证码识别总结
                        
                        1. url不变, 验证码不 变: 请求验证码地址, 获得响应, 识别
                        2. url不变, 验证码 变:  思路:对方服容器返回验证码的时候,会和用户的信息与验证码进行对应. 之后, 用户发送POST请求时,会对比post清求中的验证码 和 当前用户存储在服务器的验证码是否一致。
                        
                        #流程
                        1. 实例化 session
                        2. 使用session请求登录页面, 获取验证码的地址
                        3. 使用sesion请求验证码,识别之
                        4. 使用session发送请求
                        
                        #使用 selenium 登录。遇到验证码
                        1. url 不变, 验证码不变. 方法同上
                        2. url 不变,  验证码变(步骤): 1. selenium 请求登录页面，同时获取验证码的地址, 2. 用 driver 获取登录页面中的 cookie 交给 requests 模块发送验证码请求, 识别, 3. 输入验证码,点击登录
                        
                        ```

            4.  cookie用法

                  1.  ` {cookie['name']: cookie["value'] for cookie in driver.get_cookies()}`
                  2. `driver.delete_cookie("CookieName")`  #删除一个cookie, 名字定位
                  3. `driver.delete_all_cookies()`  #删除所有的 cookie

            5. 页面为什么需要等待

                  1.  如 网站 用了动态 htm l技术, 页面的部分元素出现需要时间, 设置一个等待时间,  要求在时间内出              现,否则报错.  `time.sleep(10)`
                  2. 显式等待 指定某个条件, 设置最长等待时间。时间内 没有找到元素,, 抛出异常。
                        verWait(driver, 10).unti(EC.presence_ of_ element_ located((By.ID, "myDynamicElement"))`
                  3. 隐式等待(了解): 简单地设置 个最大等待时间,单位为秒。
                        2. `driver.implicitly_ wait(10)`

            6. Selenium总结

                  1. 应用场景: 1 动态html页面 请求, 2 登录获取cookies
                  2. 使用步骤: 1 导包并且实例化 driver. 2 发送请求. 3 定位获取数据. 4 保存. 5 退出driver

            7. Cookies相关方法  `driver.get_cookies()`

            8. 页面等待 : 强制等待

      2. phantomJS 安装指南:

            1. 官网下载. apt-get可能会报错

            2. 解压文件 `tar -xvf phantomjs-1.9.7-linux-x86_64.tar.bz2`

            3. 将程序移到 合适的位置  `sudo mv phantomjs-1.9.7-linux-x86_64 /usr/ocal/src/phantomjs`

            4. 创建软链接 到环境变量中。这样 在shell中 用phantomjs命令 `sudo ln -sf /usr/local/src/phantojs/bin/phantomjs /usr/local/bin/ phantomjs`

            5. 检查是否正常工作  `phantomjs --version`

            6. 下载地址
                      1. chromedirver下载地址 `https://npm.taobao.org/mirrors/chromedriver`
                      2. phantomjs下载地址 `http://phantomjs.org/download.html`

                  


      

4. 应用模块

      1. pillow  #截图模块
      2. selenium使用流程  #自动补全地址功能
            1. 先定位元素，然后用 `.text` 获取文本, 用 `.get_attribute()` 获取属性
            2. selenium 获取的页面数据是浏览器中 elements 的 内容
            3. selenium 请求第一页时会等待页面加载完了之后再获取数据。但点击翻页之后, 直接获取数据, 此时可能会报错，因为数据没有加载完, 用time.sleep(3)
      3. find_element，find_elements 区别
            1. `find_element`  返回一个 `element`, 没有时报错,   
            2. `find_elements` 返回一个 list ,没有时近回空列表,  
            3. 判断是否有下一页时,用 `find_elements` , 根据列表长度判断,  
      4. 页面有 ifame或frame, 先用drivers.switch_to.frame() 方法切换到相应的 frame 中才能定位元素
            1. driver.switch_to.frame("login-frame")  #子 frame 的name值是 `login-frame`

